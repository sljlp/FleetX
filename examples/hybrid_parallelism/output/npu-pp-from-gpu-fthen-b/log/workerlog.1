WARNING: AVX is not support on your machine. Hence, no_avx core will be imported, It has much worse preformance than avx core.
/home/gongwb/.local/lib/python3.7/site-packages/setuptools/depends.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0630 21:35:03.589370 57649 init.cc:88] Before Parse: argc is 2, Init commandline: dummy --tryfromenv=check_nan_inf,benchmark,eager_delete_scope,fraction_of_cpu_memory_to_use,initial_cpu_memory_in_mb,init_allocated_mem,paddle_num_threads,dist_threadpool_size,eager_delete_tensor_gb,fast_eager_deletion_mode,memory_fraction_of_eager_deletion,allocator_strategy,reader_queue_speed_test_mode,print_sub_graph_dir,pe_profile_fname,inner_op_parallelism,enable_parallel_graph,fuse_parameter_groups_size,multiple_of_cupti_buffer_size,fuse_parameter_memory_size,tracer_profile_fname,dygraph_debug,use_system_allocator,enable_unused_var_check,free_idle_chunk,free_when_no_cache_hit,call_stack_level,sort_sum_gradient,max_inplace_grad_add,use_pinned_memory,cpu_deterministic,selected_npus,fraction_of_gpu_memory_to_use,initial_gpu_memory_in_mb,reallocate_gpu_memory_in_mb,gpu_memory_limit_mb 
I0630 21:35:03.589581 57649 init.cc:95] After Parse: argc is 1
/home/liupeng51/github/Paddle/build/build_ubuntu_develop_release_ascend_y_none_3.7.5/python/paddle/hapi/model.py:28: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
  from collections import Iterable
-----------  Configuration Arguments -----------
data_dir: ./data_bak
debug: False
do_eval: True
epoch: 100
ernie_config_file: config/ernie_base_config.json
eval_batch_size: 35
eval_data_path: ./data
eval_steps: -1
global_bsz: 64
global_steps: 0
grad_merge: 0
init_checkpoint: output/step_1
learning_rate: 0.0001
log_steps: 1
max_seq_len: 512
micro_bsz: 8
num_dp: 1
num_mp: 1
num_pp: 2
num_sharding: 1
num_train_steps: 250
output_dir: output/npu-pp-from-gpu-fthen-b
preln: False
save_steps: 500
seed: 2021
use_amp: True
use_hybrid_dp: True
use_lamb: False
use_offload: False
use_recompute: True
use_sharding: True
use_sop: False
vocab_file: ./config/30k-clean.vocab.albert
warmup_steps: 10000
weight_decay: 0.01
------------------------------------------------
[INFO] 2021-06-30 21:35:04,358 [run_pretraining.py:  285]:	pretraining start
[INFO] 2021-06-30 21:35:04,359 [run_pretraining.py:  310]:	using recompute.
[INFO] 2021-06-30 21:35:04,359 [run_pretraining.py:  355]:	using globa_bsz: 64 micro_bsz: 8, acc_steps: 8
[DEBUG] 2021-06-30 21:35:04,422 [run_pretraining.py:  148]:	========= dp_sharding worker: 0 of 1 ==========
[INFO] 2021-06-30 21:35:04,423 [pretraining_ds_mlm.py:  263]:	Apply sharding in distribution env 0/1
[INFO] 2021-06-30 21:35:04,423 [pretraining_ds_mlm.py:  265]:	read from ./data_bak/part-00000.104,./data_bak/part-00000.100,./data_bak/part-00000.107,./data_bak/part-00000.103,./data_bak/part-00000.10,./data_bak/part-00000.105,./data_bak/part-00000.101,./data_bak/part-00000.102,./data_bak/part-00000.106,./data_bak/part-00000.109,./data_bak/part-00000.108
I0630 21:35:04.423588 57649 reader_py.cc:387] init_lod_tensor_blocking_queue
INFO:root:places would be ommited when DataLoader is not iterable
input_mask(-1, 512, 1)
/home/liupeng51/github/Paddle/build/build_ubuntu_develop_release_ascend_y_none_3.7.5/python/paddle/fluid/layers/math_op_patch.py:320: UserWarning: /home/liupeng51/github/FleetX/examples/hybrid_parallelism/model/ernie.py:153
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/home/liupeng51/github/Paddle/build/build_ubuntu_develop_release_ascend_y_none_3.7.5/python/paddle/fluid/layers/math_op_patch.py:320: UserWarning: /home/liupeng51/github/FleetX/examples/hybrid_parallelism/model/ernie.py:154
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/home/liupeng51/github/Paddle/build/build_ubuntu_develop_release_ascend_y_none_3.7.5/python/paddle/fluid/layers/math_op_patch.py:320: UserWarning: /home/liupeng51/github/FleetX/examples/hybrid_parallelism/model/transformer_encoder.py:166
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/home/liupeng51/github/Paddle/build/build_ubuntu_develop_release_ascend_y_none_3.7.5/python/paddle/fluid/layers/math_op_patch.py:320: UserWarning: /home/liupeng51/github/FleetX/examples/hybrid_parallelism/model/transformer_encoder.py:276
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/home/liupeng51/github/Paddle/build/build_ubuntu_develop_release_ascend_y_none_3.7.5/python/paddle/fluid/layers/math_op_patch.py:320: UserWarning: /home/liupeng51/github/FleetX/examples/hybrid_parallelism/model/transformer_encoder.py:39
The behavior of expression A + B has been unified with elementwise_add(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_add(X, Y, axis=0) instead of A + B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
/home/liupeng51/github/Paddle/build/build_ubuntu_develop_release_ascend_y_none_3.7.5/python/paddle/fluid/layers/math_op_patch.py:320: UserWarning: /home/liupeng51/github/FleetX/examples/hybrid_parallelism/model/transformer_encoder.py:40
The behavior of expression A * B has been unified with elementwise_mul(X, Y, axis=-1) from Paddle 2.0. If your code works well in the older versions but crashes in this version, try to use elementwise_mul(X, Y, axis=0) instead of A * B. This transitional warning will be dropped in the future.
  op_type, op_type, EXPRESSION_MAP[method_name]))
[DEBUG] 2021-06-30 21:35:05,158 [run_pretraining.py:  395]:	base lr: 0.0001
/home/liupeng51/github/Paddle/build/build_ubuntu_develop_release_ascend_y_none_3.7.5/python/paddle/distributed/fleet/base/fleet_base.py:813: UserWarning: It is recommended to use DistributedStrategy in fleet.init(). The strategy here is only for compatibility. If the strategy in fleet.distributed_optimizer() is not None, then it will overwrite the DistributedStrategy in fleet.init(), which will take effect in distributed training.
  "It is recommended to use DistributedStrategy "
2021-06-30 21:35:05 INFO     Gradient merge in [pp_gm], acc step = [8]
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:Gradient merge in [pp_gm], acc step = [8]
Wed Jun 30 21:35:05-INFO: recompute segment[0]
Wed Jun 30 21:35:05-INFO: segment start op: [lookup_table_v2]: [['src_ids', 'word_embedding']]
Wed Jun 30 21:35:05-INFO: segment end op: [layer_norm]: [['encoder_layer_0_post_ffn_layer_norm_bias', 'encoder_layer_0_post_ffn_layer_norm_scale', 'tmp_10']]
Wed Jun 30 21:35:05-INFO: recompute segment[0]
Wed Jun 30 21:35:05-INFO: segment start op: [lookup_table_v2]: [['src_ids', 'word_embedding']]
Wed Jun 30 21:35:05-INFO: segment end op: [layer_norm]: [['encoder_layer_0_post_ffn_layer_norm_bias', 'encoder_layer_0_post_ffn_layer_norm_scale', 'tmp_10']]
Wed Jun 30 21:35:05-INFO: found [0] vars which cross recompute segment: [set()], better checkpoints might be set to reduce those vars
pp_rank: 1
2021-06-30 21:35:10 INFO     global word size: 2
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:global word size: 2
2021-06-30 21:35:10 INFO     global rank: 1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:global rank: 1
2021-06-30 21:35:10 INFO     global endpoints: ['192.168.206.27:6170', '192.168.206.27:6171']
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:global endpoints: ['192.168.206.27:6170', '192.168.206.27:6171']
2021-06-30 21:35:10 INFO     global ring id: 3
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:global ring id: 3
2021-06-30 21:35:10 INFO     ##############################
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:##############################
2021-06-30 21:35:10 INFO     mp group size: 1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:mp group size: 1
2021-06-30 21:35:10 INFO     mp rank: -1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:mp rank: -1
2021-06-30 21:35:10 INFO     mp group id: -1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:mp group id: -1
2021-06-30 21:35:10 INFO     mp group endpoints: []
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:mp group endpoints: []
2021-06-30 21:35:10 INFO     mp ring id: -1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:mp ring id: -1
2021-06-30 21:35:10 INFO     ##############################
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:##############################
2021-06-30 21:35:10 INFO     sharding group size: 1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:sharding group size: 1
2021-06-30 21:35:10 INFO     sharding rank: -1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:sharding rank: -1
2021-06-30 21:35:10 INFO     sharding group id: -1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:sharding group id: -1
2021-06-30 21:35:10 INFO     sharding group endpoints: []
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:sharding group endpoints: []
2021-06-30 21:35:10 INFO     sharding ring id: -1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:sharding ring id: -1
2021-06-30 21:35:10 INFO     ##############################
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:##############################
2021-06-30 21:35:10 INFO     pp group size: 2
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:pp group size: 2
2021-06-30 21:35:10 INFO     pp rank: 1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:pp rank: 1
2021-06-30 21:35:10 INFO     pp group id: 0
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:pp group id: 0
2021-06-30 21:35:10 INFO     pp group endpoints: ['192.168.206.27:6170', '192.168.206.27:6171']
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:pp group endpoints: ['192.168.206.27:6170', '192.168.206.27:6171']
2021-06-30 21:35:10 INFO     pp ring id: 20
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:pp ring id: 20
2021-06-30 21:35:10 INFO     ##############################
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:##############################
2021-06-30 21:35:10 INFO     pure dp group size: 1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:pure dp group size: 1
2021-06-30 21:35:10 INFO     pure dp rank: -1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:pure dp rank: -1
2021-06-30 21:35:10 INFO     pure dp group endpoints: []
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:pure dp group endpoints: []
2021-06-30 21:35:10 INFO     pure dp ring id: -1
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:pure dp ring id: -1
2021-06-30 21:35:10 INFO     ##############################
INFO:paddle.distributed.fleet.meta_optimizers.sharding_optimizer:##############################
pp pair:(0, 1), ring_id: 20
pp pair:(1, 0), ring_id: 21
[INFO] 2021-06-30 21:35:15,510 [run_pretraining.py:  427]:	applied_meta_list: ['ShardingOptimizer', 'AMPOptimizer', 'RecomputeOptimizer']
[INFO] 2021-06-30 21:35:15,511 [run_pretraining.py:  428]:	*******go out of gaurd
I0630 21:35:30.303295 57649 gen_hccl_id_op_helper.cc:181] Server listening on: 192.168.206.27:6171 successful.
I0630 21:35:30.464428 57649 collective_helper_npu.cc:83] initialized comm: 0xffffc7783700, nranks: 2, hccl_id: 0x3cf2f034, rank: 1
I0630 21:35:31.897251 57649 collective_helper_npu.cc:88] initialized comm: 0xffffc7783700, nranks: 2, hccl_id: 0x3cf2f034, rank: 1
I0630 21:35:31.897485 57649 collective_helper_npu.cc:93] hccl communicator of rank 1 in ring 3 has been created on device 1, with comm: 0x3ce5dc70
I0630 21:35:32.896797 57649 gen_hccl_id_op_helper.cc:181] Server listening on: 192.168.206.27:6171 successful.
I0630 21:35:32.898247 57649 collective_helper_npu.cc:83] initialized comm: 0xffffc7783700, nranks: 2, hccl_id: 0x3cf09114, rank: 1
I0630 21:35:34.113612 57649 collective_helper_npu.cc:88] initialized comm: 0xffffc7783700, nranks: 2, hccl_id: 0x3cf09114, rank: 1
I0630 21:35:34.116477 57649 collective_helper_npu.cc:93] hccl communicator of rank 1 in ring 20 has been created on device 1, with comm: 0x3ce86190
W0630 21:35:34.456823 57649 gen_hccl_id_op_helper.cc:120] connect addr=192.168.206.27:6170 failed 1 times with reason: Connection refused retry after 0.5 seconds
I0630 21:35:34.958655 57649 collective_helper_npu.cc:83] initialized comm: 0xffffc7783700, nranks: 2, hccl_id: 0x3cf0a294, rank: 0
I0630 21:35:36.179078 57649 collective_helper_npu.cc:88] initialized comm: 0xffffc7783700, nranks: 2, hccl_id: 0x3cf0a294, rank: 0
I0630 21:35:36.179219 57649 collective_helper_npu.cc:93] hccl communicator of rank 0 in ring 21 has been created on device 1, with comm: 0x3ce9dd20
[INFO] 2021-06-30 21:35:36,464 [run_pretraining.py:  483]:	 
[INFO] 2021-06-30 21:35:36,465 [run_pretraining.py:  484]:	############################WARNING############################
[INFO] 2021-06-30 21:35:36,465 [run_pretraining.py:  485]:	####### using ini_checkpoint, not init_pretraining_params ####
[INFO] 2021-06-30 21:35:36,465 [run_pretraining.py:  486]:	## meaning hyper param e.g. lr will inherit from checkpoint ##
[INFO] 2021-06-30 21:35:36,465 [run_pretraining.py:  487]:	###############################################################
Load model from output/step_1
[INFO] 2021-06-30 21:35:36,900 [run_pretraining.py:  489]:	 
data_loader started
run training
I0630 21:35:42.412693 58828 lod_tensor_blocking_queue.h:104] Init queue with size 1
I0630 21:35:42.412820 58828 buffered_reader.cc:41] BufferedReader
I0630 21:38:47.062553 58828 op_call_stack.cc:61] 

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::__future_base::_Async_state_impl<std::thread::_Invoker<std::tuple<void (paddle::framework::DeviceWorker::*)(), paddle::framework::DeviceWorker*> >, void>::_Async_state_impl(std::thread::_Invoker<std::tuple<void (paddle::framework::DeviceWorker::*)(), paddle::framework::DeviceWorker*> >&&)::{lambda()#1}> > >::_M_run()
1   std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
2   std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<void>, std::__future_base::_Result_base::_Deleter>, std::thread::_Invoker<std::tuple<void (paddle::framework::DeviceWorker::*)(), paddle::framework::DeviceWorker*> >, void> >::_M_invoke(std::_Any_data const&)
3   paddle::framework::SectionWorker::TrainFiles()
4   paddle::framework::SectionWorker::RunBackward(int, std::unique_ptr<paddle::framework::GarbageCollector, std::default_delete<paddle::framework::GarbageCollector> >&, std::unordered_map<paddle::framework::OperatorBase const*, std::vector<std::string, std::allocator<std::string> >, std::hash<paddle::framework::OperatorBase const*>, std::equal_to<paddle::framework::OperatorBase const*>, std::allocator<std::pair<paddle::framework::OperatorBase const* const, std::vector<std::string, std::allocator<std::string> > > > >&)
5   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
6   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
7   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
8   paddle::operators::NpuOpRunner::Run(void*) const
9   paddle::platform::EnforceNotMet::EnforceNotMet(paddle::platform::ErrorSummary const&, char const*, int)
10  paddle::platform::GetCurrentTraceBackString()

----------------------
Error Message Summary:
----------------------
ExternalError:  ACL error, the error code is : 500002.  (at /home/liupeng51/github/Paddle/paddle/fluid/operators/npu_op_runner.cc:320)
Traceback (most recent call last):
  File "run_pretraining.py", line 620, in <module>
    train(args)
  File "run_pretraining.py", line 531, in train
    ret = exe.run(train_program, fetch_list=fetch_list, use_program_cache=True) # run one mini-batch(=acc_steps micro-batch)
  File "/home/liupeng51/github/Paddle/build/build_ubuntu_develop_release_ascend_y_none_3.7.5/python/paddle/fluid/executor.py", line 1110, in run
    six.reraise(*sys.exc_info())
  File "/home/gongwb/.local/lib/python3.7/site-packages/six.py", line 703, in reraise
    raise value
  File "/home/liupeng51/github/Paddle/build/build_ubuntu_develop_release_ascend_y_none_3.7.5/python/paddle/fluid/executor.py", line 1108, in run
    return_merged=return_merged)
  File "/home/liupeng51/github/Paddle/build/build_ubuntu_develop_release_ascend_y_none_3.7.5/python/paddle/fluid/executor.py", line 1138, in _run_impl
    return self.train_from_dataset(program, fetch_list=fetch_list)
  File "/home/liupeng51/github/Paddle/build/build_ubuntu_develop_release_ascend_y_none_3.7.5/python/paddle/fluid/executor.py", line 1728, in train_from_dataset
    print_period, fetch_handler)
  File "/home/liupeng51/github/Paddle/build/build_ubuntu_develop_release_ascend_y_none_3.7.5/python/paddle/fluid/executor.py", line 1528, in _run_from_dataset
    self._default_executor.release_trainer(trainer_instance)
OSError: 

  Compile Traceback (most recent call last):
    File "run_pretraining.py", line 620, in <module>
      train(args)
    File "run_pretraining.py", line 380, in train
      graph_vars = create_model(args, 'train', micro_bsz, dp_sharding_rank, dp_sharding_worldsize, topo, acc_steps)
    File "run_pretraining.py", line 196, in create_model
      mask_lm_loss, mean_mask_lm_loss = ernie.get_lm_output(mask_label, mask_pos)
    File "/home/liupeng51/github/FleetX/examples/hybrid_parallelism/model/ernie.py", line 418, in get_lm_output
      logits=fc_out, label=mask_label)
    File "/home/liupeng51/github/Paddle/build/build_ubuntu_develop_release_ascend_y_none_3.7.5/python/paddle/fluid/layers/loss.py", line 1296, in softmax_with_cross_entropy
      attrs=attrs)
    File "/home/liupeng51/github/Paddle/build/build_ubuntu_develop_release_ascend_y_none_3.7.5/python/paddle/fluid/layer_helper.py", line 43, in append_op
      return self.main_program.current_block().append_op(*args, **kwargs)
    File "/home/liupeng51/github/Paddle/build/build_ubuntu_develop_release_ascend_y_none_3.7.5/python/paddle/fluid/framework.py", line 2947, in append_op
      attrs=kwargs.get("attrs", None))
    File "/home/liupeng51/github/Paddle/build/build_ubuntu_develop_release_ascend_y_none_3.7.5/python/paddle/fluid/framework.py", line 2019, in __init__
      for frame in traceback.extract_stack():

--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   std::thread::_State_impl<std::thread::_Invoker<std::tuple<std::__future_base::_Async_state_impl<std::thread::_Invoker<std::tuple<void (paddle::framework::DeviceWorker::*)(), paddle::framework::DeviceWorker*> >, void>::_Async_state_impl(std::thread::_Invoker<std::tuple<void (paddle::framework::DeviceWorker::*)(), paddle::framework::DeviceWorker*> >&&)::{lambda()#1}> > >::_M_run()
1   std::__future_base::_State_baseV2::_M_do_set(std::function<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> ()>*, bool*)
2   std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base, std::__future_base::_Result_base::_Deleter> (), std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<void>, std::__future_base::_Result_base::_Deleter>, std::thread::_Invoker<std::tuple<void (paddle::framework::DeviceWorker::*)(), paddle::framework::DeviceWorker*> >, void> >::_M_invoke(std::_Any_data const&)
3   paddle::framework::SectionWorker::TrainFiles()
4   paddle::framework::SectionWorker::RunBackward(int, std::unique_ptr<paddle::framework::GarbageCollector, std::default_delete<paddle::framework::GarbageCollector> >&, std::unordered_map<paddle::framework::OperatorBase const*, std::vector<std::string, std::allocator<std::string> >, std::hash<paddle::framework::OperatorBase const*>, std::equal_to<paddle::framework::OperatorBase const*>, std::allocator<std::pair<paddle::framework::OperatorBase const* const, std::vector<std::string, std::allocator<std::string> > > > >&)
5   paddle::framework::OperatorBase::Run(paddle::framework::Scope const&, paddle::platform::Place const&)
6   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&) const
7   paddle::framework::OperatorWithKernel::RunImpl(paddle::framework::Scope const&, paddle::platform::Place const&, paddle::framework::RuntimeContext*) const
8   paddle::operators::NpuOpRunner::Run(void*) const
9   paddle::platform::EnforceNotMet::EnforceNotMet(paddle::platform::ErrorSummary const&, char const*, int)
10  paddle::platform::GetCurrentTraceBackString()

----------------------
Error Message Summary:
----------------------
ExternalError:  ACL error, the error code is : 500002.  (at /home/liupeng51/github/Paddle/paddle/fluid/operators/npu_op_runner.cc:320)
  [operator < softmax_with_cross_entropy_grad > error]
I0630 21:38:49.878386 57649 reader.h:164] ~ReaderHolder
I0630 21:38:49.878464 57649 reader.h:164] ~ReaderHolder
I0630 21:38:49.878479 57649 buffered_reader.cc:22] ~BufferedReader
I0630 21:38:49.878487 57649 lod_tensor_blocking_queue.h:59] LoDTensorBlockingQueue close
I0630 21:38:49.878494 57649 blocking_queue.h:132] close queue
I0630 21:38:49.878674 57649 reader.cc:76] ~DecoratedReader
I0630 21:38:49.878682 57649 lod_tensor_blocking_queue.h:59] LoDTensorBlockingQueue close
I0630 21:38:49.878688 57649 blocking_queue.h:132] close queue
I0630 21:38:49.878695 57649 lod_tensor_blocking_queue.h:59] LoDTensorBlockingQueue close
I0630 21:38:49.878700 57649 blocking_queue.h:132] close queue
terminate called without an active exception


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::framework::SignalHandle(char const*, int)

----------------------
Error Message Summary:
----------------------
FatalError: `Process abort signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1625060329 (unix time) try "date -d @1625060329" if you are using GNU date ***]
  [SignalInfo: *** SIGABRT (@0xe131) received by PID 57649 (TID 0xffff3f7fe1e0) from PID 57649 ***]

