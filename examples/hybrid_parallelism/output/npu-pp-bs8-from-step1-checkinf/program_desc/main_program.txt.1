{ // block 0
    persist var @LR_DECAY_COUNTER@ : LOD_TENSOR.shape(1,).dtype(int64).stop_gradient(True)
    persist var create_py_reader_0 : READER)
    persist var double_buffer_0 : READER)
    var src_ids : LOD_TENSOR.shape(-1, 512).dtype(int32).stop_gradient(True)
    var sent_ids : LOD_TENSOR.shape(-1, 512).dtype(int32).stop_gradient(True)
    var pos_ids : LOD_TENSOR.shape(-1, 512).dtype(int32).stop_gradient(True)
    var mask_label : LOD_TENSOR.shape(-1, 1).dtype(int32).stop_gradient(True)
    var mask_pos : LOD_TENSOR.shape(-1, 1).dtype(int32).stop_gradient(True)
    persist var encoder_layer_6_multi_head_att_query_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_query_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var layer_norm_12.tmp_2 : LOD_TENSOR.shape(1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_36.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_query_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_query_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_36.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_key_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_key_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_37.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_key_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_key_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_37.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_value_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_value_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_38.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_value_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_value_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_38.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_24.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_24.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_24.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_25.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_25.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_25.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_26.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_26.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_26.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var scale_7.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_13.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    persist var stack_0.tmp_0 : LOD_TENSOR.shape(1, 12, 512, 512).dtype(float16).stop_gradient(True)
    var tmp_56 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var softmax_6.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_14.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_27.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_27.tmp_1 : LOD_TENSOR.shape(0, -1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var reshape2_27.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_output_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_output_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_39.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_output_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_output_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_39.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_57 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_6_post_att_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_att_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_13.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_13.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_13.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_0.w_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_6_ffn_fc_0.w_0.cast_fp16 : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var fc_40.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_0.b_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_6_ffn_fc_0.b_0.cast_fp16 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var fc_40.tmp_1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_40.tmp_1.cast_fp32 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_6.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_58 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_59 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_60 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_6.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_61 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_62 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_63 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_1.w_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_6_ffn_fc_1.w_0.cast_fp16 : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_63.cast_fp16 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_41.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_1.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_6_ffn_fc_1.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_41.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_64 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_6_post_ffn_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_ffn_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_14.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_14.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_14.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_query_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_query_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_42.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_query_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_query_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_42.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_key_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_key_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_43.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_key_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_key_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_43.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_value_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_value_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_44.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_value_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_value_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_44.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_28.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_28.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_28.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_29.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_29.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_29.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_30.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_30.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_30.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var scale_8.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_15.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var tmp_65 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var softmax_7.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_16.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_31.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_31.tmp_1 : LOD_TENSOR.shape(0, -1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var reshape2_31.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_output_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_output_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_45.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_output_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_output_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_45.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_66 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_7_post_att_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_att_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_15.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_15.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_15.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_0.w_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_7_ffn_fc_0.w_0.cast_fp16 : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var fc_46.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_0.b_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_ffn_fc_0.b_0.cast_fp16 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var fc_46.tmp_1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_46.tmp_1.cast_fp32 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_7.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_67 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_68 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_69 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_7.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_70 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_71 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_72 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_1.w_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_7_ffn_fc_1.w_0.cast_fp16 : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_72.cast_fp16 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_47.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_1.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_ffn_fc_1.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_47.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_73 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_7_post_ffn_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_ffn_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_16.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_16.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_16.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_query_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_query_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_48.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_query_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_query_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_48.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_key_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_key_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_49.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_key_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_key_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_49.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_value_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_value_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_50.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_value_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_value_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_50.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_32.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_32.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_32.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_33.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_33.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_33.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_34.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_34.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_34.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var scale_9.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_17.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var tmp_74 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var softmax_8.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_18.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_35.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_35.tmp_1 : LOD_TENSOR.shape(0, -1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var reshape2_35.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_output_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_output_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_51.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_output_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_output_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_51.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_75 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_8_post_att_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_att_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_17.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_17.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_17.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_0.w_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_8_ffn_fc_0.w_0.cast_fp16 : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var fc_52.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_0.b_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_ffn_fc_0.b_0.cast_fp16 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var fc_52.tmp_1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_52.tmp_1.cast_fp32 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_8.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_76 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_77 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_78 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_8.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_79 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_80 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_81 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_1.w_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_8_ffn_fc_1.w_0.cast_fp16 : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_81.cast_fp16 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_53.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_1.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_ffn_fc_1.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_53.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_82 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_8_post_ffn_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_ffn_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_18.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_18.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_18.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_query_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_query_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_54.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_query_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_query_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_54.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_key_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_key_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_55.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_key_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_key_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_55.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_value_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_value_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_56.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_value_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_value_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_56.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_36.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_36.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_36.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_37.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_37.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_37.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_38.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_38.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_38.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var scale_10.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_19.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var tmp_83 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var softmax_9.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_20.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_39.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_39.tmp_1 : LOD_TENSOR.shape(0, -1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var reshape2_39.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_output_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_output_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_57.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_output_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_output_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_57.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_84 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_9_post_att_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_att_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_19.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_19.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_19.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_0.w_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_9_ffn_fc_0.w_0.cast_fp16 : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var fc_58.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_0.b_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_ffn_fc_0.b_0.cast_fp16 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var fc_58.tmp_1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_58.tmp_1.cast_fp32 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_9.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_85 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_86 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_87 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_9.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_88 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_89 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_90 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_1.w_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_9_ffn_fc_1.w_0.cast_fp16 : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_90.cast_fp16 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_59.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_1.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_ffn_fc_1.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_59.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_91 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_9_post_ffn_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_ffn_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_20.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_20.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_20.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_query_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_query_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_60.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_query_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_query_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_60.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_key_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_key_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_61.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_key_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_key_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_61.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_value_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_value_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_62.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_value_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_value_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_62.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_40.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_40.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_40.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_41.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_41.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_41.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_42.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_42.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_42.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var scale_11.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_21.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var tmp_92 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var softmax_10.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_22.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_43.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_43.tmp_1 : LOD_TENSOR.shape(0, -1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var reshape2_43.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_output_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_output_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_63.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_output_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_output_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_63.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_93 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_10_post_att_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_att_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_21.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_21.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_21.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_0.w_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_10_ffn_fc_0.w_0.cast_fp16 : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var fc_64.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_0.b_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_ffn_fc_0.b_0.cast_fp16 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var fc_64.tmp_1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_64.tmp_1.cast_fp32 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_10.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_94 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_95 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_96 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_10.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_97 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_98 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_99 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_1.w_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_10_ffn_fc_1.w_0.cast_fp16 : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_99.cast_fp16 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_65.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_1.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_ffn_fc_1.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_65.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_100 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_10_post_ffn_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_ffn_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_22.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_22.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_22.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_query_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_query_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_66.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_query_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_query_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_66.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_key_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_key_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_67.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_key_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_key_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_67.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_value_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_value_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_68.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_value_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_value_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_68.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_44.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_44.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_44.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_45.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_45.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_45.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_46.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_46.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_46.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var scale_12.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_23.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var tmp_101 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var softmax_11.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_24.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_47.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_47.tmp_1 : LOD_TENSOR.shape(0, -1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var reshape2_47.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_output_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_output_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_69.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_output_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_output_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_69.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_102 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_11_post_att_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_att_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_23.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_23.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_23.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_0.w_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_11_ffn_fc_0.w_0.cast_fp16 : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var fc_70.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_0.b_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_ffn_fc_0.b_0.cast_fp16 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var fc_70.tmp_1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_70.tmp_1.cast_fp32 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_11.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_103 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_104 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_105 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_11.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_106 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_107 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_108 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_1.w_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_11_ffn_fc_1.w_0.cast_fp16 : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_108.cast_fp16 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_71.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_1.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_ffn_fc_1.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_71.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_109 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_11_post_ffn_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_ffn_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_24.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_24.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_24.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var cast_1.tmp_0 : LOD_TENSOR.shape(-1, 1).dtype(int32).stop_gradient(True)
    var reshape2_48.tmp_0 : LOD_TENSOR.shape(-1, 768).dtype(float16).stop_gradient(False)
    var reshape2_48.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var gather_0.tmp_0 : LOD_TENSOR.shape(-1, 768).dtype(float16).stop_gradient(False)
    persist var mask_lm_trans_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var mask_lm_trans_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_72.tmp_0 : LOD_TENSOR.shape(-1, 768).dtype(float16).stop_gradient(False)
    persist var mask_lm_trans_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var mask_lm_trans_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_72.tmp_1 : LOD_TENSOR.shape(-1, 768).dtype(float16).stop_gradient(False)
    var fc_72.tmp_1.cast_fp32 : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var pow_12.tmp_0 : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var tmp_110 : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var tmp_111 : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var tmp_112 : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var tanh_12.tmp_0 : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var tmp_113 : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var tmp_114 : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var tmp_115 : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var tmp_115.cast_fp16 : LOD_TENSOR.shape(-1, 768).dtype(float16).stop_gradient(False)
    persist var mask_lm_trans_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_25.tmp_0 : LOD_TENSOR.shape(-1,).dtype(float32).stop_gradient(True)
    var layer_norm_25.tmp_1 : LOD_TENSOR.shape(-1,).dtype(float32).stop_gradient(True)
    var layer_norm_25.tmp_2 : LOD_TENSOR.shape(-1, 768).dtype(float16).stop_gradient(False)
    persist var mask_lm_out_fc.w_0 : LOD_TENSOR.shape(768, 30000).dtype(float32).stop_gradient(False)
    var mask_lm_out_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 30000).dtype(float16).stop_gradient(False)
    var fc_73.tmp_0 : LOD_TENSOR.shape(-1, 30000).dtype(float16).stop_gradient(False)
    persist var mask_lm_out_fc.b_0 : LOD_TENSOR.shape(30000,).dtype(float32).stop_gradient(False)
    var mask_lm_out_fc.b_0.cast_fp16 : LOD_TENSOR.shape(30000,).dtype(float16).stop_gradient(False)
    var fc_73.tmp_1 : LOD_TENSOR.shape(-1, 30000).dtype(float16).stop_gradient(False)
    var fc_73.tmp_1.cast_fp32 : LOD_TENSOR.shape(-1, 30000).dtype(float32).stop_gradient(False)
    var softmax_with_cross_entropy_0.tmp_2 : LOD_TENSOR.shape(-1, 30000).dtype(float32).stop_gradient(False)
    persist var softmax_with_cross_entropy_0.tmp_1 : LOD_TENSOR.shape(-1, 1).dtype(float32).stop_gradient(False)
    var softmax_with_cross_entropy_0.tmp_0 : LOD_TENSOR.shape(-1, 30000).dtype(float32).stop_gradient(False)
    persist var mean_mask_lm_loss.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var fill_constant_3.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var tmp_116 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var tmp_117 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var cast_2.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var tmp_118 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var tmp_119 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    var logical_not_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    persist var scheduled_learning_rate : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var _generated_var_0 : STEP_SCOPES)
    var _generated_var_1 : STEP_SCOPES)
    var float_status : LOD_TENSOR.shape(8,).dtype(float32).stop_gradient(True)
    persist var loss_scaling_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var tmp_130 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var tmp_130@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var mean_mask_lm_loss.tmp_0@GRAD : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var softmax_with_cross_entropy_0.tmp_1@GRAD : LOD_TENSOR.shape(-1, 1).dtype(float32).stop_gradient(False)
    var fc_73.tmp_1.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 30000).dtype(float32).stop_gradient(False)
    var fc_73.tmp_1@GRAD : LOD_TENSOR.shape(-1, 30000).dtype(float16).stop_gradient(False)
    var fc_73.tmp_0@GRAD : LOD_TENSOR.shape(-1, 30000).dtype(float16).stop_gradient(False)
    var mask_lm_out_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(30000,).dtype(float16).stop_gradient(False)
    var layer_norm_25.tmp_2@GRAD : LOD_TENSOR.shape(-1, 768).dtype(float16).stop_gradient(False)
    var mask_lm_out_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 30000).dtype(float16).stop_gradient(False)
    var mask_lm_trans_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var mask_lm_trans_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_115.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 768).dtype(float16).stop_gradient(False)
    var tmp_115@GRAD : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var fc_72.tmp_1.cast_fp32@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var tmp_114@GRAD : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var tmp_113@GRAD : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var tanh_12.tmp_0@GRAD : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var tmp_112@GRAD : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var tmp_111@GRAD : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var fc_72.tmp_1.cast_fp32@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var tmp_110@GRAD : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var pow_12.tmp_0@GRAD : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var fc_72.tmp_1.cast_fp32@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var fc_72.tmp_1.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 768).dtype(float32).stop_gradient(False)
    var fc_72.tmp_1@GRAD : LOD_TENSOR.shape(-1, 768).dtype(float16).stop_gradient(False)
    var fc_72.tmp_0@GRAD : LOD_TENSOR.shape(-1, 768).dtype(float16).stop_gradient(False)
    var mask_lm_trans_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var gather_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 768).dtype(float16).stop_gradient(False)
    var mask_lm_trans_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var reshape2_48.tmp_0@GRAD : LOD_TENSOR.shape(-1, 768).dtype(float16).stop_gradient(False)
    var layer_norm_24.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_11_post_ffn_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_post_ffn_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_109@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_71.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_23.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_71.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_11_ffn_fc_1.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var tmp_108.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_11_ffn_fc_1.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_108@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_70.tmp_1.cast_fp32@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_107@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_106@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_105@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_104@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_70.tmp_1.cast_fp32@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_103@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_70.tmp_1.cast_fp32@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_70.tmp_1.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_70.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_70.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_11_ffn_fc_0.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_23.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_11_ffn_fc_0.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_23.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_11_post_att_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_post_att_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_102@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_69.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_22.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_69.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_11_multi_head_att_output_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var transpose_47.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var encoder_layer_11_multi_head_att_output_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var matmul_v2_24.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var softmax_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var transpose_46.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var tmp_101@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_23.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var scale_12.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_45.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_44.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var fc_68.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_67.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_66.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_68.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_11_multi_head_att_value_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_22.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_11_multi_head_att_value_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_67.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_11_multi_head_att_key_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_22.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_11_multi_head_att_key_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_66.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_11_multi_head_att_query_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_22.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_11_multi_head_att_query_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var layer_norm_22.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_10_post_ffn_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_post_ffn_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_100@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_65.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_21.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_65.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_10_ffn_fc_1.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var tmp_99.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_10_ffn_fc_1.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_99@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_64.tmp_1.cast_fp32@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_98@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_97@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_96@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_95@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_64.tmp_1.cast_fp32@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_94@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_64.tmp_1.cast_fp32@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_64.tmp_1.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_64.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_64.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_10_ffn_fc_0.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_21.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_10_ffn_fc_0.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_21.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_10_post_att_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_post_att_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_93@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_63.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_20.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_63.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_10_multi_head_att_output_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var transpose_43.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var encoder_layer_10_multi_head_att_output_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var matmul_v2_22.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var softmax_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var transpose_42.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var tmp_92@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_21.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var scale_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_41.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_40.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var fc_62.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_61.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_60.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_62.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_10_multi_head_att_value_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_20.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_10_multi_head_att_value_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_61.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_10_multi_head_att_key_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_20.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_10_multi_head_att_key_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_60.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_10_multi_head_att_query_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_20.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_10_multi_head_att_query_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var layer_norm_20.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_9_post_ffn_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_post_ffn_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_91@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_59.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_19.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_59.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_9_ffn_fc_1.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var tmp_90.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_9_ffn_fc_1.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_90@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_58.tmp_1.cast_fp32@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_89@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_88@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_87@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_86@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_58.tmp_1.cast_fp32@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_85@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_58.tmp_1.cast_fp32@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_58.tmp_1.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_58.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_58.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_9_ffn_fc_0.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_19.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_9_ffn_fc_0.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_19.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_9_post_att_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_post_att_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_84@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_57.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_18.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_57.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_9_multi_head_att_output_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var transpose_39.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var encoder_layer_9_multi_head_att_output_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var matmul_v2_20.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var softmax_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var transpose_38.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var tmp_83@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_19.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var scale_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_37.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_36.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var fc_56.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_55.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_54.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_56.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_9_multi_head_att_value_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_18.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_9_multi_head_att_value_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_55.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_9_multi_head_att_key_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_18.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_9_multi_head_att_key_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_54.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_9_multi_head_att_query_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_18.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_9_multi_head_att_query_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var layer_norm_18.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_8_post_ffn_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_post_ffn_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_82@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_53.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_17.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_53.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_8_ffn_fc_1.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var tmp_81.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_8_ffn_fc_1.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_81@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_52.tmp_1.cast_fp32@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_80@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_79@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_78@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_77@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_52.tmp_1.cast_fp32@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_76@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_52.tmp_1.cast_fp32@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_52.tmp_1.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_52.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_52.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_8_ffn_fc_0.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_17.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_8_ffn_fc_0.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_17.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_8_post_att_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_post_att_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_75@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_51.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_16.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_51.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_8_multi_head_att_output_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var transpose_35.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var encoder_layer_8_multi_head_att_output_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var matmul_v2_18.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var softmax_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var transpose_34.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var tmp_74@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_17.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var scale_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_33.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_32.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var fc_50.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_49.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_48.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_50.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_8_multi_head_att_value_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_16.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_8_multi_head_att_value_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_49.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_8_multi_head_att_key_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_16.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_8_multi_head_att_key_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_48.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_8_multi_head_att_query_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_16.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_8_multi_head_att_query_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var layer_norm_16.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_7_post_ffn_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_post_ffn_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_73@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_47.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_15.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_47.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_7_ffn_fc_1.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var tmp_72.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_7_ffn_fc_1.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_72@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_46.tmp_1.cast_fp32@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_71@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_70@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_69@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_68@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_46.tmp_1.cast_fp32@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_67@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_46.tmp_1.cast_fp32@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_46.tmp_1.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_46.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_46.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_7_ffn_fc_0.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_15.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_7_ffn_fc_0.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_15.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_7_post_att_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_post_att_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_66@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_45.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_14.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_45.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_7_multi_head_att_output_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var transpose_31.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var encoder_layer_7_multi_head_att_output_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var matmul_v2_16.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var softmax_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var transpose_30.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var tmp_65@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_15.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var scale_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_29.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_28.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var fc_44.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_43.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_42.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_44.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_7_multi_head_att_value_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_14.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_7_multi_head_att_value_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_43.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_7_multi_head_att_key_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_14.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_7_multi_head_att_key_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_42.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_7_multi_head_att_query_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_14.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_7_multi_head_att_query_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var layer_norm_14.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_6_post_ffn_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_6_post_ffn_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_64@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_41.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_13.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_41.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_6_ffn_fc_1.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var tmp_63.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_6_ffn_fc_1.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_63@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_40.tmp_1.cast_fp32@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_62@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_61@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_60@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_59@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_40.tmp_1.cast_fp32@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_58@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_40.tmp_1.cast_fp32@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_40.tmp_1.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_40.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_40.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_6_ffn_fc_0.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_13.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_6_ffn_fc_0.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_13.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_6_post_att_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_6_post_att_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_57@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_39.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_12.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_39.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_6_multi_head_att_output_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var transpose_27.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var encoder_layer_6_multi_head_att_output_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var matmul_v2_14.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var softmax_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var transpose_26.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var tmp_56@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_13.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var scale_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_25.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_24.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var fc_38.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_37.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_36.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_38.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_6_multi_head_att_value_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_12.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_6_multi_head_att_value_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_37.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_6_multi_head_att_key_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_12.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_6_multi_head_att_key_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_36.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_6_multi_head_att_query_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_12.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_6_multi_head_att_query_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var layer_norm_12.tmp_2@GRAD : LOD_TENSOR.shape(1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_6_multi_head_att_query_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_query_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_key_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_key_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_value_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_value_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_output_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_output_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_6_ffn_fc_0.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_6_ffn_fc_0.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_6_ffn_fc_1.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_6_ffn_fc_1.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_query_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_query_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_key_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_key_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_value_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_value_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_output_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_output_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_ffn_fc_0.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_7_ffn_fc_0.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_ffn_fc_1.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_7_ffn_fc_1.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_query_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_query_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_key_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_key_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_value_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_value_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_output_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_output_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_ffn_fc_0.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_8_ffn_fc_0.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_ffn_fc_1.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_8_ffn_fc_1.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_query_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_query_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_key_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_key_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_value_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_value_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_output_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_output_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_ffn_fc_0.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_9_ffn_fc_0.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_ffn_fc_1.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_9_ffn_fc_1.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_query_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_query_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_key_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_key_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_value_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_value_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_output_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_output_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_ffn_fc_0.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_10_ffn_fc_0.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_ffn_fc_1.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_10_ffn_fc_1.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_query_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_query_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_key_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_key_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_value_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_value_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_output_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_output_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_ffn_fc_0.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_11_ffn_fc_0.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_ffn_fc_1.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_11_ffn_fc_1.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var mask_lm_trans_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var mask_lm_trans_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var mask_lm_out_fc.w_0@GRAD : LOD_TENSOR.shape(768, 30000).dtype(float32).stop_gradient(False)
    var mask_lm_out_fc.b_0@GRAD : LOD_TENSOR.shape(30000,).dtype(float32).stop_gradient(False)
    var find_infinite_scale.tmp_0 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    persist var num_bad_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var num_good_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    var square_16.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var reduce_sum_16.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_17.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var reduce_sum_17.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_18.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_18.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_19.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_19.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_20.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_20.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_21.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_21.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_22.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_22.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_23.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_23.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_24.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_24.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_25.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_25.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_26.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_26.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_27.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_27.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_28.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_28.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_29.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_29.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_30.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_30.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_31.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_31.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_32.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var reduce_sum_32.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_33.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var reduce_sum_33.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_34.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_34.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_35.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_35.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_36.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_36.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_37.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_37.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_38.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_38.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_39.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_39.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_40.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_40.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_41.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_41.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_42.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_42.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_43.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_43.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_44.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_44.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_45.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_45.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_46.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_46.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_47.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_47.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_128.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var reduce_sum_128.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_129.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var reduce_sum_129.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_130.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_130.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_131.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_131.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_132.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_132.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_133.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_133.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_134.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_134.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_135.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_135.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_136.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_136.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_137.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_137.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_138.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_138.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_139.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_139.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_140.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_140.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_141.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_141.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_142.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_142.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_143.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_143.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_144.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var reduce_sum_144.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_145.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var reduce_sum_145.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_146.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_146.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_147.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_147.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_148.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_148.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_149.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_149.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_150.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_150.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_151.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_151.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_152.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_152.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_153.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_153.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_154.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_154.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_155.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_155.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_156.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_156.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_157.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_157.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_158.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_158.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_159.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_159.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_160.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var reduce_sum_160.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_161.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var reduce_sum_161.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_162.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_162.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_163.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_163.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_164.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_164.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_165.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_165.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_166.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_166.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_167.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_167.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_168.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_168.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_169.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_169.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_170.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_170.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_171.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_171.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_172.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_172.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_173.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_173.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_174.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_174.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_175.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_175.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_176.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var reduce_sum_176.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_177.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var reduce_sum_177.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_178.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_178.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_179.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_179.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_180.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_180.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_181.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_181.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_182.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_182.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_183.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_183.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_184.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_184.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_185.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_185.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_186.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_186.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_187.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_187.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_188.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_188.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_189.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_189.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_190.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_190.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_191.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_191.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_192.tmp_0 : LOD_TENSOR.shape(30000,).dtype(float32).stop_gradient(False)
    var reduce_sum_192.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_193.tmp_0 : LOD_TENSOR.shape(768, 30000).dtype(float32).stop_gradient(False)
    var reduce_sum_193.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_194.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_194.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_195.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_195.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_196.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_196.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_197.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_197.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var sum_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var sqrt_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var fill_constant_7.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var elementwise_max_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var elementwise_div_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var elementwise_mul_16.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var elementwise_mul_17.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var elementwise_mul_18.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_19.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_20.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_21.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_22.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_23.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_24.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_25.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_26.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_27.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_28.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_29.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_30.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_31.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_32.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var elementwise_mul_33.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var elementwise_mul_34.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_35.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_36.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_37.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_38.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_39.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_40.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_41.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_42.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_43.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_44.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_45.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_46.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_47.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_128.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var elementwise_mul_129.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var elementwise_mul_130.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_131.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_132.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_133.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_134.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_135.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_136.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_137.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_138.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_139.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_140.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_141.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_142.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_143.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_144.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var elementwise_mul_145.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var elementwise_mul_146.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_147.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_148.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_149.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_150.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_151.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_152.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_153.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_154.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_155.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_156.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_157.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_158.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_159.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_160.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var elementwise_mul_161.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var elementwise_mul_162.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_163.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_164.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_165.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_166.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_167.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_168.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_169.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_170.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_171.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_172.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_173.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_174.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_175.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_176.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var elementwise_mul_177.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var elementwise_mul_178.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_179.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_180.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_181.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_182.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_183.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_184.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_185.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_186.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_187.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_188.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_189.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_190.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_191.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_192.tmp_0 : LOD_TENSOR.shape(30000,).dtype(float32).stop_gradient(False)
    var elementwise_mul_193.tmp_0 : LOD_TENSOR.shape(768, 30000).dtype(float32).stop_gradient(False)
    var elementwise_mul_194.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_195.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_196.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_197.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_0.b_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_0.b_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_0.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_0.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_1.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_1.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_1.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_1.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_1.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_1.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_1.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_1.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_key_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_key_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_key_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_key_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_key_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_key_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_key_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_key_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_output_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_output_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_output_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_output_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_output_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_output_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_output_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_output_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_query_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_query_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_query_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_query_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_query_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_query_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_query_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_query_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_value_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_value_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_value_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_value_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_value_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_value_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_value_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_value_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_att_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_att_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_att_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_att_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_att_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_att_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_att_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_att_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_ffn_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_ffn_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_ffn_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_ffn_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_ffn_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_ffn_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_ffn_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_ffn_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_0.b_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_0.b_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_0.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_0.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_1.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_1.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_1.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_1.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_1.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_1.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_1.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_1.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_key_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_key_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_key_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_key_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_key_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_key_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_key_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_key_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_output_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_output_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_output_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_output_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_output_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_output_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_output_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_output_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_query_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_query_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_query_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_query_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_query_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_query_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_query_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_query_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_value_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_value_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_value_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_value_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_value_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_value_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_value_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_value_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_att_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_att_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_att_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_att_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_att_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_att_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_att_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_att_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_ffn_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_ffn_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_ffn_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_ffn_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_ffn_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_ffn_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_ffn_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_ffn_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_0.b_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_0.b_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_0.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_0.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_1.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_1.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_1.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_1.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_1.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_1.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_1.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_1.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_key_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_key_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_key_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_key_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_key_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_key_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_key_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_key_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_output_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_output_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_output_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_output_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_output_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_output_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_output_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_output_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_query_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_query_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_query_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_query_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_query_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_query_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_query_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_query_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_value_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_value_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_value_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_value_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_value_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_value_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_value_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_value_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_att_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_att_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_att_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_att_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_att_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_att_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_att_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_att_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_ffn_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_ffn_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_ffn_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_ffn_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_ffn_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_ffn_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_ffn_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_ffn_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_0.b_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_0.b_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_0.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_0.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_1.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_1.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_1.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_1.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_1.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_1.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_1.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_1.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_key_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_key_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_key_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_key_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_key_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_key_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_key_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_key_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_output_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_output_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_output_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_output_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_output_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_output_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_output_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_output_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_query_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_query_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_query_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_query_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_query_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_query_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_query_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_query_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_value_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_value_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_value_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_value_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_value_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_value_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_value_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_value_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_att_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_att_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_att_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_att_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_att_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_att_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_att_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_att_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_ffn_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_ffn_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_ffn_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_ffn_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_ffn_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_ffn_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_ffn_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_ffn_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_0.b_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_0.b_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_0.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_0.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_1.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_1.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_1.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_1.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_1.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_1.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_1.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_1.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_key_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_key_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_key_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_key_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_key_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_key_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_key_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_key_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_output_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_output_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_output_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_output_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_output_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_output_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_output_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_output_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_query_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_query_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_query_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_query_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_query_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_query_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_query_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_query_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_value_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_value_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_value_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_value_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_value_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_value_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_value_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_value_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_att_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_att_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_att_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_att_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_att_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_att_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_att_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_att_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_ffn_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_ffn_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_ffn_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_ffn_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_ffn_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_ffn_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_ffn_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_ffn_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_0.b_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_0.b_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_0.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_0.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_1.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_1.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_1.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_1.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_1.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_1.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_1.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_1.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_key_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_key_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_key_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_key_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_key_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_key_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_key_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_key_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_output_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_output_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_output_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_output_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_output_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_output_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_output_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_output_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_query_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_query_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_query_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_query_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_query_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_query_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_query_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_query_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_value_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_value_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_value_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_value_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_value_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_value_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_value_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_value_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_att_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_att_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_att_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_att_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_att_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_att_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_att_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_att_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_ffn_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_ffn_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_ffn_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_ffn_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_ffn_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_ffn_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_ffn_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_ffn_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var mask_lm_out_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var mask_lm_out_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var mask_lm_out_fc.b_0_moment1_0 : LOD_TENSOR.shape(30000,).dtype(float32).stop_gradient(False)
    persist var mask_lm_out_fc.b_0_moment2_0 : LOD_TENSOR.shape(30000,).dtype(float32).stop_gradient(False)
    persist var mask_lm_out_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var mask_lm_out_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var mask_lm_out_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 30000).dtype(float32).stop_gradient(False)
    persist var mask_lm_out_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 30000).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_query_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_query_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_query_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_query_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_key_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_key_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_key_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_key_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_value_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_value_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_value_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_value_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_output_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_output_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_multi_head_att_output_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_6_multi_head_att_output_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_att_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_att_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_0.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_6_ffn_fc_0.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_0.b_0@GRAD@MERGED : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_6_ffn_fc_0.b_0@GRAD@TMP : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_1.w_0@GRAD@MERGED : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_6_ffn_fc_1.w_0@GRAD@TMP : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_ffn_fc_1.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_6_ffn_fc_1.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_ffn_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_6_post_ffn_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_query_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_query_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_query_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_query_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_key_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_key_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_key_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_key_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_value_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_value_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_value_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_value_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_output_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_output_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_multi_head_att_output_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_multi_head_att_output_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_att_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_att_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_0.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_7_ffn_fc_0.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_0.b_0@GRAD@MERGED : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_ffn_fc_0.b_0@GRAD@TMP : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_1.w_0@GRAD@MERGED : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_7_ffn_fc_1.w_0@GRAD@TMP : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_ffn_fc_1.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_7_ffn_fc_1.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_ffn_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_7_post_ffn_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_query_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_query_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_query_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_query_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_key_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_key_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_key_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_key_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_value_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_value_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_value_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_value_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_output_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_output_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_multi_head_att_output_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_multi_head_att_output_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_att_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_att_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_0.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_8_ffn_fc_0.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_0.b_0@GRAD@MERGED : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_ffn_fc_0.b_0@GRAD@TMP : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_1.w_0@GRAD@MERGED : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_8_ffn_fc_1.w_0@GRAD@TMP : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_ffn_fc_1.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_8_ffn_fc_1.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_ffn_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_8_post_ffn_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_query_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_query_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_query_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_query_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_key_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_key_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_key_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_key_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_value_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_value_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_value_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_value_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_output_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_output_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_multi_head_att_output_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_multi_head_att_output_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_att_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_att_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_0.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_9_ffn_fc_0.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_0.b_0@GRAD@MERGED : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_ffn_fc_0.b_0@GRAD@TMP : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_1.w_0@GRAD@MERGED : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_9_ffn_fc_1.w_0@GRAD@TMP : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_ffn_fc_1.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_9_ffn_fc_1.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_ffn_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_9_post_ffn_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_query_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_query_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_query_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_query_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_key_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_key_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_key_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_key_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_value_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_value_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_value_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_value_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_output_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_output_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_multi_head_att_output_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_multi_head_att_output_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_att_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_att_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_0.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_10_ffn_fc_0.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_0.b_0@GRAD@MERGED : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_ffn_fc_0.b_0@GRAD@TMP : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_1.w_0@GRAD@MERGED : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_10_ffn_fc_1.w_0@GRAD@TMP : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_ffn_fc_1.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_10_ffn_fc_1.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_ffn_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_10_post_ffn_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_query_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_query_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_query_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_query_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_key_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_key_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_key_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_key_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_value_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_value_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_value_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_value_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_output_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_output_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_multi_head_att_output_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_multi_head_att_output_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_att_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_att_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_0.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_11_ffn_fc_0.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_0.b_0@GRAD@MERGED : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_ffn_fc_0.b_0@GRAD@TMP : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_1.w_0@GRAD@MERGED : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_11_ffn_fc_1.w_0@GRAD@TMP : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_ffn_fc_1.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_11_ffn_fc_1.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_ffn_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_11_post_ffn_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var mask_lm_trans_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var mask_lm_trans_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var mask_lm_trans_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var mask_lm_out_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 30000).dtype(float32).stop_gradient(False)
    var mask_lm_out_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 30000).dtype(float32).stop_gradient(False)
    persist var mask_lm_out_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(30000,).dtype(float32).stop_gradient(False)
    var mask_lm_out_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(30000,).dtype(float32).stop_gradient(False)
    var find_infinite_scale.tmp_0@cast_int32 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(False)
    var find_infinite_scale.tmp_0@GLOBAL_WORLD : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)

    {Out=['@LR_DECAY_COUNTER@']} = increment(inputs={X=['@LR_DECAY_COUNTER@']}, op_device = , op_namescope = /, op_role = 16, op_role_var = [], step = 1.0)
    {Out=['create_py_reader_0']} = create_py_reader(inputs={blocking_queue=['lod_tensor_blocking_queue_0']}, device_count = 1, device_index = 0, dtypes = [2, 2, 2, 2, 2], lod_levels = [0, 0, 0, 0, 0], need_check_feed = [1, 1, 1, 1, 1], op_device = , op_namescope = /, op_role = 0, op_role_var = [], ranks = [2, 2, 2, 2, 2], shape_concat = [-1, 512, -1, 512, -1, 512, -1, 1, -1, 1], use_data_config = True)
    {Out=['double_buffer_0']} = create_double_buffer_reader(inputs={UnderlyingReader=['create_py_reader_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place = AUTO)
    {Out=['src_ids', 'sent_ids', 'pos_ids', 'mask_label', 'mask_pos']} = read(inputs={Reader=['double_buffer_0']}, drop_last = True, infer_out = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], throw_eof_exp = True)
    {Out=['encoder_layer_6_multi_head_att_query_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_6_multi_head_att_query_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['layer_norm_12.tmp_2']} = recv_v2(inputs={}, dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_shape = [1, 512, 768], peer = 0, ring_id = 20, srTag = 0, tag = tag, use_calc_stream = True)
    {Out=['fc_36.tmp_0']} = mul(inputs={X=['layer_norm_12.tmp_2'], Y=['encoder_layer_6_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_6_multi_head_att_query_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_6_multi_head_att_query_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_36.tmp_1']} = elementwise_add(inputs={X=['fc_36.tmp_0'], Y=['encoder_layer_6_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_6_multi_head_att_key_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_6_multi_head_att_key_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_37.tmp_0']} = mul(inputs={X=['layer_norm_12.tmp_2'], Y=['encoder_layer_6_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_6_multi_head_att_key_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_6_multi_head_att_key_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_37.tmp_1']} = elementwise_add(inputs={X=['fc_37.tmp_0'], Y=['encoder_layer_6_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_6_multi_head_att_value_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_6_multi_head_att_value_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_38.tmp_0']} = mul(inputs={X=['layer_norm_12.tmp_2'], Y=['encoder_layer_6_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_6_multi_head_att_value_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_6_multi_head_att_value_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_38.tmp_1']} = elementwise_add(inputs={X=['fc_38.tmp_0'], Y=['encoder_layer_6_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_36.tmp_1'], XShape=['reshape2_24.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_36.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_24.tmp_0'], XShape=['transpose_24.tmp_1']} = transpose2(inputs={X=['fc_36.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_37.tmp_1'], XShape=['reshape2_25.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_37.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_25.tmp_0'], XShape=['transpose_25.tmp_1']} = transpose2(inputs={X=['fc_37.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_38.tmp_1'], XShape=['reshape2_26.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_38.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_26.tmp_0'], XShape=['transpose_26.tmp_1']} = transpose2(inputs={X=['fc_38.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['scale_7.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_24.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.125, use_mkldnn = False)
    {Out=['matmul_v2_13.tmp_0']} = matmul_v2(inputs={X=['scale_7.tmp_0'], Y=['transpose_25.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['stack_0.tmp_0']} = recv_v2(inputs={}, dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_shape = [1, 12, 512, 512], peer = 0, ring_id = 20, srTag = 0, tag = tag, use_calc_stream = True)
    {Out=['tmp_56']} = elementwise_add(inputs={X=['matmul_v2_13.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_6.tmp_0']} = softmax(inputs={X=['tmp_56']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_14.tmp_0']} = matmul_v2(inputs={X=['softmax_6.tmp_0'], Y=['transpose_26.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {Out=['transpose_27.tmp_0'], XShape=['transpose_27.tmp_1']} = transpose2(inputs={X=['matmul_v2_14.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['transpose_27.tmp_0'], XShape=['reshape2_27.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_27.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['encoder_layer_6_multi_head_att_output_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_6_multi_head_att_output_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_39.tmp_0']} = mul(inputs={X=['transpose_27.tmp_0'], Y=['encoder_layer_6_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_6_multi_head_att_output_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_6_multi_head_att_output_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_39.tmp_1']} = elementwise_add(inputs={X=['fc_39.tmp_0'], Y=['encoder_layer_6_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_57']} = elementwise_add(inputs={X=['fc_39.tmp_1'], Y=['layer_norm_12.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_13.tmp_0'], Variance=['layer_norm_13.tmp_1'], Y=['layer_norm_13.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_6_post_att_layer_norm_bias'], Scale=['encoder_layer_6_post_att_layer_norm_scale'], X=['tmp_57']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_6_ffn_fc_0.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_6_ffn_fc_0.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_40.tmp_0']} = mul(inputs={X=['layer_norm_13.tmp_2'], Y=['encoder_layer_6_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_6_ffn_fc_0.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_6_ffn_fc_0.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_40.tmp_1']} = elementwise_add(inputs={X=['fc_40.tmp_0'], Y=['encoder_layer_6_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_40.tmp_1.cast_fp32']} = cast(inputs={X=['fc_40.tmp_1']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['pow_6.tmp_0']} = pow(inputs={FactorTensor=[], X=['fc_40.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['tmp_58']} = scale(inputs={ScaleTensor=[], X=['pow_6.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.044714998453855515, use_mkldnn = False)
    {Out=['tmp_59']} = elementwise_add(inputs={X=['fc_40.tmp_1.cast_fp32'], Y=['tmp_58']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_60']} = scale(inputs={ScaleTensor=[], X=['tmp_59']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.7978845834732056, use_mkldnn = False)
    {Out=['tanh_6.tmp_0']} = tanh(inputs={X=['tmp_60']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_61']} = scale(inputs={ScaleTensor=[], X=['tanh_6.tmp_0']}, bias = 1.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0, use_mkldnn = False)
    {Out=['tmp_62']} = scale(inputs={ScaleTensor=[], X=['tmp_61']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.5, use_mkldnn = False)
    {Out=['tmp_63']} = elementwise_mul(inputs={X=['fc_40.tmp_1.cast_fp32'], Y=['tmp_62']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_6_ffn_fc_1.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_6_ffn_fc_1.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['tmp_63.cast_fp16']} = cast(inputs={X=['tmp_63']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_41.tmp_0']} = mul(inputs={X=['tmp_63.cast_fp16'], Y=['encoder_layer_6_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_6_ffn_fc_1.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_6_ffn_fc_1.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_41.tmp_1']} = elementwise_add(inputs={X=['fc_41.tmp_0'], Y=['encoder_layer_6_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_64']} = elementwise_add(inputs={X=['fc_41.tmp_1'], Y=['layer_norm_13.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_14.tmp_0'], Variance=['layer_norm_14.tmp_1'], Y=['layer_norm_14.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_6_post_ffn_layer_norm_bias'], Scale=['encoder_layer_6_post_ffn_layer_norm_scale'], X=['tmp_64']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_7_multi_head_att_query_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_7_multi_head_att_query_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_42.tmp_0']} = mul(inputs={X=['layer_norm_14.tmp_2'], Y=['encoder_layer_7_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_7_multi_head_att_query_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_7_multi_head_att_query_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_42.tmp_1']} = elementwise_add(inputs={X=['fc_42.tmp_0'], Y=['encoder_layer_7_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_7_multi_head_att_key_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_7_multi_head_att_key_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_43.tmp_0']} = mul(inputs={X=['layer_norm_14.tmp_2'], Y=['encoder_layer_7_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_7_multi_head_att_key_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_7_multi_head_att_key_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_43.tmp_1']} = elementwise_add(inputs={X=['fc_43.tmp_0'], Y=['encoder_layer_7_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_7_multi_head_att_value_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_7_multi_head_att_value_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_44.tmp_0']} = mul(inputs={X=['layer_norm_14.tmp_2'], Y=['encoder_layer_7_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_7_multi_head_att_value_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_7_multi_head_att_value_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_44.tmp_1']} = elementwise_add(inputs={X=['fc_44.tmp_0'], Y=['encoder_layer_7_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_42.tmp_1'], XShape=['reshape2_28.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_42.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_28.tmp_0'], XShape=['transpose_28.tmp_1']} = transpose2(inputs={X=['fc_42.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_43.tmp_1'], XShape=['reshape2_29.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_43.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_29.tmp_0'], XShape=['transpose_29.tmp_1']} = transpose2(inputs={X=['fc_43.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_44.tmp_1'], XShape=['reshape2_30.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_44.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_30.tmp_0'], XShape=['transpose_30.tmp_1']} = transpose2(inputs={X=['fc_44.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['scale_8.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_28.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.125, use_mkldnn = False)
    {Out=['matmul_v2_15.tmp_0']} = matmul_v2(inputs={X=['scale_8.tmp_0'], Y=['transpose_29.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['tmp_65']} = elementwise_add(inputs={X=['matmul_v2_15.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_7.tmp_0']} = softmax(inputs={X=['tmp_65']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_16.tmp_0']} = matmul_v2(inputs={X=['softmax_7.tmp_0'], Y=['transpose_30.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {Out=['transpose_31.tmp_0'], XShape=['transpose_31.tmp_1']} = transpose2(inputs={X=['matmul_v2_16.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['transpose_31.tmp_0'], XShape=['reshape2_31.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_31.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['encoder_layer_7_multi_head_att_output_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_7_multi_head_att_output_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_45.tmp_0']} = mul(inputs={X=['transpose_31.tmp_0'], Y=['encoder_layer_7_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_7_multi_head_att_output_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_7_multi_head_att_output_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_45.tmp_1']} = elementwise_add(inputs={X=['fc_45.tmp_0'], Y=['encoder_layer_7_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_66']} = elementwise_add(inputs={X=['fc_45.tmp_1'], Y=['layer_norm_14.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_15.tmp_0'], Variance=['layer_norm_15.tmp_1'], Y=['layer_norm_15.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_7_post_att_layer_norm_bias'], Scale=['encoder_layer_7_post_att_layer_norm_scale'], X=['tmp_66']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_7_ffn_fc_0.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_7_ffn_fc_0.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_46.tmp_0']} = mul(inputs={X=['layer_norm_15.tmp_2'], Y=['encoder_layer_7_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_7_ffn_fc_0.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_7_ffn_fc_0.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_46.tmp_1']} = elementwise_add(inputs={X=['fc_46.tmp_0'], Y=['encoder_layer_7_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_46.tmp_1.cast_fp32']} = cast(inputs={X=['fc_46.tmp_1']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['pow_7.tmp_0']} = pow(inputs={FactorTensor=[], X=['fc_46.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['tmp_67']} = scale(inputs={ScaleTensor=[], X=['pow_7.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.044714998453855515, use_mkldnn = False)
    {Out=['tmp_68']} = elementwise_add(inputs={X=['fc_46.tmp_1.cast_fp32'], Y=['tmp_67']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_69']} = scale(inputs={ScaleTensor=[], X=['tmp_68']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.7978845834732056, use_mkldnn = False)
    {Out=['tanh_7.tmp_0']} = tanh(inputs={X=['tmp_69']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_70']} = scale(inputs={ScaleTensor=[], X=['tanh_7.tmp_0']}, bias = 1.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0, use_mkldnn = False)
    {Out=['tmp_71']} = scale(inputs={ScaleTensor=[], X=['tmp_70']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.5, use_mkldnn = False)
    {Out=['tmp_72']} = elementwise_mul(inputs={X=['fc_46.tmp_1.cast_fp32'], Y=['tmp_71']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_7_ffn_fc_1.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_7_ffn_fc_1.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['tmp_72.cast_fp16']} = cast(inputs={X=['tmp_72']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_47.tmp_0']} = mul(inputs={X=['tmp_72.cast_fp16'], Y=['encoder_layer_7_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_7_ffn_fc_1.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_7_ffn_fc_1.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_47.tmp_1']} = elementwise_add(inputs={X=['fc_47.tmp_0'], Y=['encoder_layer_7_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_73']} = elementwise_add(inputs={X=['fc_47.tmp_1'], Y=['layer_norm_15.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_16.tmp_0'], Variance=['layer_norm_16.tmp_1'], Y=['layer_norm_16.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_7_post_ffn_layer_norm_bias'], Scale=['encoder_layer_7_post_ffn_layer_norm_scale'], X=['tmp_73']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_8_multi_head_att_query_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_8_multi_head_att_query_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_48.tmp_0']} = mul(inputs={X=['layer_norm_16.tmp_2'], Y=['encoder_layer_8_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_8_multi_head_att_query_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_8_multi_head_att_query_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_48.tmp_1']} = elementwise_add(inputs={X=['fc_48.tmp_0'], Y=['encoder_layer_8_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_8_multi_head_att_key_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_8_multi_head_att_key_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_49.tmp_0']} = mul(inputs={X=['layer_norm_16.tmp_2'], Y=['encoder_layer_8_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_8_multi_head_att_key_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_8_multi_head_att_key_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_49.tmp_1']} = elementwise_add(inputs={X=['fc_49.tmp_0'], Y=['encoder_layer_8_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_8_multi_head_att_value_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_8_multi_head_att_value_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_50.tmp_0']} = mul(inputs={X=['layer_norm_16.tmp_2'], Y=['encoder_layer_8_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_8_multi_head_att_value_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_8_multi_head_att_value_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_50.tmp_1']} = elementwise_add(inputs={X=['fc_50.tmp_0'], Y=['encoder_layer_8_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_48.tmp_1'], XShape=['reshape2_32.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_48.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_32.tmp_0'], XShape=['transpose_32.tmp_1']} = transpose2(inputs={X=['fc_48.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_49.tmp_1'], XShape=['reshape2_33.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_49.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_33.tmp_0'], XShape=['transpose_33.tmp_1']} = transpose2(inputs={X=['fc_49.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_50.tmp_1'], XShape=['reshape2_34.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_50.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_34.tmp_0'], XShape=['transpose_34.tmp_1']} = transpose2(inputs={X=['fc_50.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['scale_9.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_32.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.125, use_mkldnn = False)
    {Out=['matmul_v2_17.tmp_0']} = matmul_v2(inputs={X=['scale_9.tmp_0'], Y=['transpose_33.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['tmp_74']} = elementwise_add(inputs={X=['matmul_v2_17.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_8.tmp_0']} = softmax(inputs={X=['tmp_74']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_18.tmp_0']} = matmul_v2(inputs={X=['softmax_8.tmp_0'], Y=['transpose_34.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {Out=['transpose_35.tmp_0'], XShape=['transpose_35.tmp_1']} = transpose2(inputs={X=['matmul_v2_18.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['transpose_35.tmp_0'], XShape=['reshape2_35.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_35.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['encoder_layer_8_multi_head_att_output_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_8_multi_head_att_output_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_51.tmp_0']} = mul(inputs={X=['transpose_35.tmp_0'], Y=['encoder_layer_8_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_8_multi_head_att_output_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_8_multi_head_att_output_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_51.tmp_1']} = elementwise_add(inputs={X=['fc_51.tmp_0'], Y=['encoder_layer_8_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_75']} = elementwise_add(inputs={X=['fc_51.tmp_1'], Y=['layer_norm_16.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_17.tmp_0'], Variance=['layer_norm_17.tmp_1'], Y=['layer_norm_17.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_8_post_att_layer_norm_bias'], Scale=['encoder_layer_8_post_att_layer_norm_scale'], X=['tmp_75']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_8_ffn_fc_0.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_8_ffn_fc_0.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_52.tmp_0']} = mul(inputs={X=['layer_norm_17.tmp_2'], Y=['encoder_layer_8_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_8_ffn_fc_0.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_8_ffn_fc_0.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_52.tmp_1']} = elementwise_add(inputs={X=['fc_52.tmp_0'], Y=['encoder_layer_8_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_52.tmp_1.cast_fp32']} = cast(inputs={X=['fc_52.tmp_1']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['pow_8.tmp_0']} = pow(inputs={FactorTensor=[], X=['fc_52.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['tmp_76']} = scale(inputs={ScaleTensor=[], X=['pow_8.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.044714998453855515, use_mkldnn = False)
    {Out=['tmp_77']} = elementwise_add(inputs={X=['fc_52.tmp_1.cast_fp32'], Y=['tmp_76']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_78']} = scale(inputs={ScaleTensor=[], X=['tmp_77']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.7978845834732056, use_mkldnn = False)
    {Out=['tanh_8.tmp_0']} = tanh(inputs={X=['tmp_78']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_79']} = scale(inputs={ScaleTensor=[], X=['tanh_8.tmp_0']}, bias = 1.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0, use_mkldnn = False)
    {Out=['tmp_80']} = scale(inputs={ScaleTensor=[], X=['tmp_79']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.5, use_mkldnn = False)
    {Out=['tmp_81']} = elementwise_mul(inputs={X=['fc_52.tmp_1.cast_fp32'], Y=['tmp_80']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_8_ffn_fc_1.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_8_ffn_fc_1.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['tmp_81.cast_fp16']} = cast(inputs={X=['tmp_81']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_53.tmp_0']} = mul(inputs={X=['tmp_81.cast_fp16'], Y=['encoder_layer_8_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_8_ffn_fc_1.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_8_ffn_fc_1.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_53.tmp_1']} = elementwise_add(inputs={X=['fc_53.tmp_0'], Y=['encoder_layer_8_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_82']} = elementwise_add(inputs={X=['fc_53.tmp_1'], Y=['layer_norm_17.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_18.tmp_0'], Variance=['layer_norm_18.tmp_1'], Y=['layer_norm_18.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_8_post_ffn_layer_norm_bias'], Scale=['encoder_layer_8_post_ffn_layer_norm_scale'], X=['tmp_82']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_9_multi_head_att_query_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_9_multi_head_att_query_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_54.tmp_0']} = mul(inputs={X=['layer_norm_18.tmp_2'], Y=['encoder_layer_9_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_9_multi_head_att_query_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_9_multi_head_att_query_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_54.tmp_1']} = elementwise_add(inputs={X=['fc_54.tmp_0'], Y=['encoder_layer_9_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_9_multi_head_att_key_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_9_multi_head_att_key_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_55.tmp_0']} = mul(inputs={X=['layer_norm_18.tmp_2'], Y=['encoder_layer_9_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_9_multi_head_att_key_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_9_multi_head_att_key_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_55.tmp_1']} = elementwise_add(inputs={X=['fc_55.tmp_0'], Y=['encoder_layer_9_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_9_multi_head_att_value_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_9_multi_head_att_value_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_56.tmp_0']} = mul(inputs={X=['layer_norm_18.tmp_2'], Y=['encoder_layer_9_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_9_multi_head_att_value_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_9_multi_head_att_value_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_56.tmp_1']} = elementwise_add(inputs={X=['fc_56.tmp_0'], Y=['encoder_layer_9_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_54.tmp_1'], XShape=['reshape2_36.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_54.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_36.tmp_0'], XShape=['transpose_36.tmp_1']} = transpose2(inputs={X=['fc_54.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_55.tmp_1'], XShape=['reshape2_37.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_55.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_37.tmp_0'], XShape=['transpose_37.tmp_1']} = transpose2(inputs={X=['fc_55.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_56.tmp_1'], XShape=['reshape2_38.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_56.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_38.tmp_0'], XShape=['transpose_38.tmp_1']} = transpose2(inputs={X=['fc_56.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['scale_10.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_36.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.125, use_mkldnn = False)
    {Out=['matmul_v2_19.tmp_0']} = matmul_v2(inputs={X=['scale_10.tmp_0'], Y=['transpose_37.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['tmp_83']} = elementwise_add(inputs={X=['matmul_v2_19.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_9.tmp_0']} = softmax(inputs={X=['tmp_83']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_20.tmp_0']} = matmul_v2(inputs={X=['softmax_9.tmp_0'], Y=['transpose_38.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {Out=['transpose_39.tmp_0'], XShape=['transpose_39.tmp_1']} = transpose2(inputs={X=['matmul_v2_20.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['transpose_39.tmp_0'], XShape=['reshape2_39.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_39.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['encoder_layer_9_multi_head_att_output_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_9_multi_head_att_output_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_57.tmp_0']} = mul(inputs={X=['transpose_39.tmp_0'], Y=['encoder_layer_9_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_9_multi_head_att_output_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_9_multi_head_att_output_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_57.tmp_1']} = elementwise_add(inputs={X=['fc_57.tmp_0'], Y=['encoder_layer_9_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_84']} = elementwise_add(inputs={X=['fc_57.tmp_1'], Y=['layer_norm_18.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_19.tmp_0'], Variance=['layer_norm_19.tmp_1'], Y=['layer_norm_19.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_9_post_att_layer_norm_bias'], Scale=['encoder_layer_9_post_att_layer_norm_scale'], X=['tmp_84']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_9_ffn_fc_0.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_9_ffn_fc_0.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_58.tmp_0']} = mul(inputs={X=['layer_norm_19.tmp_2'], Y=['encoder_layer_9_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_9_ffn_fc_0.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_9_ffn_fc_0.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_58.tmp_1']} = elementwise_add(inputs={X=['fc_58.tmp_0'], Y=['encoder_layer_9_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_58.tmp_1.cast_fp32']} = cast(inputs={X=['fc_58.tmp_1']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['pow_9.tmp_0']} = pow(inputs={FactorTensor=[], X=['fc_58.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['tmp_85']} = scale(inputs={ScaleTensor=[], X=['pow_9.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.044714998453855515, use_mkldnn = False)
    {Out=['tmp_86']} = elementwise_add(inputs={X=['fc_58.tmp_1.cast_fp32'], Y=['tmp_85']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_87']} = scale(inputs={ScaleTensor=[], X=['tmp_86']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.7978845834732056, use_mkldnn = False)
    {Out=['tanh_9.tmp_0']} = tanh(inputs={X=['tmp_87']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_88']} = scale(inputs={ScaleTensor=[], X=['tanh_9.tmp_0']}, bias = 1.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0, use_mkldnn = False)
    {Out=['tmp_89']} = scale(inputs={ScaleTensor=[], X=['tmp_88']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.5, use_mkldnn = False)
    {Out=['tmp_90']} = elementwise_mul(inputs={X=['fc_58.tmp_1.cast_fp32'], Y=['tmp_89']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_9_ffn_fc_1.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_9_ffn_fc_1.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['tmp_90.cast_fp16']} = cast(inputs={X=['tmp_90']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_59.tmp_0']} = mul(inputs={X=['tmp_90.cast_fp16'], Y=['encoder_layer_9_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_9_ffn_fc_1.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_9_ffn_fc_1.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_59.tmp_1']} = elementwise_add(inputs={X=['fc_59.tmp_0'], Y=['encoder_layer_9_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_91']} = elementwise_add(inputs={X=['fc_59.tmp_1'], Y=['layer_norm_19.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_20.tmp_0'], Variance=['layer_norm_20.tmp_1'], Y=['layer_norm_20.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_9_post_ffn_layer_norm_bias'], Scale=['encoder_layer_9_post_ffn_layer_norm_scale'], X=['tmp_91']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_10_multi_head_att_query_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_10_multi_head_att_query_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_60.tmp_0']} = mul(inputs={X=['layer_norm_20.tmp_2'], Y=['encoder_layer_10_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_10_multi_head_att_query_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_10_multi_head_att_query_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_60.tmp_1']} = elementwise_add(inputs={X=['fc_60.tmp_0'], Y=['encoder_layer_10_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_10_multi_head_att_key_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_10_multi_head_att_key_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_61.tmp_0']} = mul(inputs={X=['layer_norm_20.tmp_2'], Y=['encoder_layer_10_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_10_multi_head_att_key_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_10_multi_head_att_key_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_61.tmp_1']} = elementwise_add(inputs={X=['fc_61.tmp_0'], Y=['encoder_layer_10_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_10_multi_head_att_value_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_10_multi_head_att_value_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_62.tmp_0']} = mul(inputs={X=['layer_norm_20.tmp_2'], Y=['encoder_layer_10_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_10_multi_head_att_value_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_10_multi_head_att_value_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_62.tmp_1']} = elementwise_add(inputs={X=['fc_62.tmp_0'], Y=['encoder_layer_10_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_60.tmp_1'], XShape=['reshape2_40.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_60.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_40.tmp_0'], XShape=['transpose_40.tmp_1']} = transpose2(inputs={X=['fc_60.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_61.tmp_1'], XShape=['reshape2_41.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_61.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_41.tmp_0'], XShape=['transpose_41.tmp_1']} = transpose2(inputs={X=['fc_61.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_62.tmp_1'], XShape=['reshape2_42.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_62.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_42.tmp_0'], XShape=['transpose_42.tmp_1']} = transpose2(inputs={X=['fc_62.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['scale_11.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_40.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.125, use_mkldnn = False)
    {Out=['matmul_v2_21.tmp_0']} = matmul_v2(inputs={X=['scale_11.tmp_0'], Y=['transpose_41.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['tmp_92']} = elementwise_add(inputs={X=['matmul_v2_21.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_10.tmp_0']} = softmax(inputs={X=['tmp_92']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_22.tmp_0']} = matmul_v2(inputs={X=['softmax_10.tmp_0'], Y=['transpose_42.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {Out=['transpose_43.tmp_0'], XShape=['transpose_43.tmp_1']} = transpose2(inputs={X=['matmul_v2_22.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['transpose_43.tmp_0'], XShape=['reshape2_43.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_43.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['encoder_layer_10_multi_head_att_output_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_10_multi_head_att_output_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_63.tmp_0']} = mul(inputs={X=['transpose_43.tmp_0'], Y=['encoder_layer_10_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_10_multi_head_att_output_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_10_multi_head_att_output_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_63.tmp_1']} = elementwise_add(inputs={X=['fc_63.tmp_0'], Y=['encoder_layer_10_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_93']} = elementwise_add(inputs={X=['fc_63.tmp_1'], Y=['layer_norm_20.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_21.tmp_0'], Variance=['layer_norm_21.tmp_1'], Y=['layer_norm_21.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_10_post_att_layer_norm_bias'], Scale=['encoder_layer_10_post_att_layer_norm_scale'], X=['tmp_93']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_10_ffn_fc_0.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_10_ffn_fc_0.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_64.tmp_0']} = mul(inputs={X=['layer_norm_21.tmp_2'], Y=['encoder_layer_10_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_10_ffn_fc_0.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_10_ffn_fc_0.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_64.tmp_1']} = elementwise_add(inputs={X=['fc_64.tmp_0'], Y=['encoder_layer_10_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_64.tmp_1.cast_fp32']} = cast(inputs={X=['fc_64.tmp_1']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['pow_10.tmp_0']} = pow(inputs={FactorTensor=[], X=['fc_64.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['tmp_94']} = scale(inputs={ScaleTensor=[], X=['pow_10.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.044714998453855515, use_mkldnn = False)
    {Out=['tmp_95']} = elementwise_add(inputs={X=['fc_64.tmp_1.cast_fp32'], Y=['tmp_94']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_96']} = scale(inputs={ScaleTensor=[], X=['tmp_95']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.7978845834732056, use_mkldnn = False)
    {Out=['tanh_10.tmp_0']} = tanh(inputs={X=['tmp_96']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_97']} = scale(inputs={ScaleTensor=[], X=['tanh_10.tmp_0']}, bias = 1.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0, use_mkldnn = False)
    {Out=['tmp_98']} = scale(inputs={ScaleTensor=[], X=['tmp_97']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.5, use_mkldnn = False)
    {Out=['tmp_99']} = elementwise_mul(inputs={X=['fc_64.tmp_1.cast_fp32'], Y=['tmp_98']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_10_ffn_fc_1.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_10_ffn_fc_1.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['tmp_99.cast_fp16']} = cast(inputs={X=['tmp_99']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_65.tmp_0']} = mul(inputs={X=['tmp_99.cast_fp16'], Y=['encoder_layer_10_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_10_ffn_fc_1.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_10_ffn_fc_1.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_65.tmp_1']} = elementwise_add(inputs={X=['fc_65.tmp_0'], Y=['encoder_layer_10_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_100']} = elementwise_add(inputs={X=['fc_65.tmp_1'], Y=['layer_norm_21.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_22.tmp_0'], Variance=['layer_norm_22.tmp_1'], Y=['layer_norm_22.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_10_post_ffn_layer_norm_bias'], Scale=['encoder_layer_10_post_ffn_layer_norm_scale'], X=['tmp_100']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_11_multi_head_att_query_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_11_multi_head_att_query_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_66.tmp_0']} = mul(inputs={X=['layer_norm_22.tmp_2'], Y=['encoder_layer_11_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_11_multi_head_att_query_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_11_multi_head_att_query_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_66.tmp_1']} = elementwise_add(inputs={X=['fc_66.tmp_0'], Y=['encoder_layer_11_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_11_multi_head_att_key_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_11_multi_head_att_key_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_67.tmp_0']} = mul(inputs={X=['layer_norm_22.tmp_2'], Y=['encoder_layer_11_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_11_multi_head_att_key_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_11_multi_head_att_key_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_67.tmp_1']} = elementwise_add(inputs={X=['fc_67.tmp_0'], Y=['encoder_layer_11_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_11_multi_head_att_value_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_11_multi_head_att_value_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_68.tmp_0']} = mul(inputs={X=['layer_norm_22.tmp_2'], Y=['encoder_layer_11_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_11_multi_head_att_value_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_11_multi_head_att_value_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_68.tmp_1']} = elementwise_add(inputs={X=['fc_68.tmp_0'], Y=['encoder_layer_11_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_66.tmp_1'], XShape=['reshape2_44.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_66.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_44.tmp_0'], XShape=['transpose_44.tmp_1']} = transpose2(inputs={X=['fc_66.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_67.tmp_1'], XShape=['reshape2_45.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_67.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_45.tmp_0'], XShape=['transpose_45.tmp_1']} = transpose2(inputs={X=['fc_67.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_68.tmp_1'], XShape=['reshape2_46.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_68.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_46.tmp_0'], XShape=['transpose_46.tmp_1']} = transpose2(inputs={X=['fc_68.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['scale_12.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_44.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.125, use_mkldnn = False)
    {Out=['matmul_v2_23.tmp_0']} = matmul_v2(inputs={X=['scale_12.tmp_0'], Y=['transpose_45.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['tmp_101']} = elementwise_add(inputs={X=['matmul_v2_23.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_11.tmp_0']} = softmax(inputs={X=['tmp_101']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_24.tmp_0']} = matmul_v2(inputs={X=['softmax_11.tmp_0'], Y=['transpose_46.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {Out=['transpose_47.tmp_0'], XShape=['transpose_47.tmp_1']} = transpose2(inputs={X=['matmul_v2_24.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['transpose_47.tmp_0'], XShape=['reshape2_47.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_47.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['encoder_layer_11_multi_head_att_output_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_11_multi_head_att_output_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_69.tmp_0']} = mul(inputs={X=['transpose_47.tmp_0'], Y=['encoder_layer_11_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_11_multi_head_att_output_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_11_multi_head_att_output_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_69.tmp_1']} = elementwise_add(inputs={X=['fc_69.tmp_0'], Y=['encoder_layer_11_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_102']} = elementwise_add(inputs={X=['fc_69.tmp_1'], Y=['layer_norm_22.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_23.tmp_0'], Variance=['layer_norm_23.tmp_1'], Y=['layer_norm_23.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_11_post_att_layer_norm_bias'], Scale=['encoder_layer_11_post_att_layer_norm_scale'], X=['tmp_102']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_11_ffn_fc_0.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_11_ffn_fc_0.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_70.tmp_0']} = mul(inputs={X=['layer_norm_23.tmp_2'], Y=['encoder_layer_11_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_11_ffn_fc_0.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_11_ffn_fc_0.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_70.tmp_1']} = elementwise_add(inputs={X=['fc_70.tmp_0'], Y=['encoder_layer_11_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_70.tmp_1.cast_fp32']} = cast(inputs={X=['fc_70.tmp_1']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['pow_11.tmp_0']} = pow(inputs={FactorTensor=[], X=['fc_70.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['tmp_103']} = scale(inputs={ScaleTensor=[], X=['pow_11.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.044714998453855515, use_mkldnn = False)
    {Out=['tmp_104']} = elementwise_add(inputs={X=['fc_70.tmp_1.cast_fp32'], Y=['tmp_103']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_105']} = scale(inputs={ScaleTensor=[], X=['tmp_104']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.7978845834732056, use_mkldnn = False)
    {Out=['tanh_11.tmp_0']} = tanh(inputs={X=['tmp_105']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_106']} = scale(inputs={ScaleTensor=[], X=['tanh_11.tmp_0']}, bias = 1.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0, use_mkldnn = False)
    {Out=['tmp_107']} = scale(inputs={ScaleTensor=[], X=['tmp_106']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.5, use_mkldnn = False)
    {Out=['tmp_108']} = elementwise_mul(inputs={X=['fc_70.tmp_1.cast_fp32'], Y=['tmp_107']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_11_ffn_fc_1.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_11_ffn_fc_1.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['tmp_108.cast_fp16']} = cast(inputs={X=['tmp_108']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_71.tmp_0']} = mul(inputs={X=['tmp_108.cast_fp16'], Y=['encoder_layer_11_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_11_ffn_fc_1.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_11_ffn_fc_1.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_71.tmp_1']} = elementwise_add(inputs={X=['fc_71.tmp_0'], Y=['encoder_layer_11_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_109']} = elementwise_add(inputs={X=['fc_71.tmp_1'], Y=['layer_norm_23.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_24.tmp_0'], Variance=['layer_norm_24.tmp_1'], Y=['layer_norm_24.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_11_post_ffn_layer_norm_bias'], Scale=['encoder_layer_11_post_ffn_layer_norm_scale'], X=['tmp_109']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['cast_1.tmp_0']} = cast(inputs={X=['mask_pos']}, in_dtype = 2, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 2, use_mkldnn = False)
    {Out=['reshape2_48.tmp_0'], XShape=['reshape2_48.tmp_1']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['layer_norm_24.tmp_2']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [-1, 768], use_quantizer = False)
    {Out=['gather_0.tmp_0']} = gather(inputs={Axis=[], Index=['cast_1.tmp_0'], X=['reshape2_48.tmp_0']}, axis = 0, op_device = , op_namescope = /, op_role = 0, op_role_var = [], overwrite = True)
    {Out=['mask_lm_trans_fc.w_0.cast_fp16']} = cast(inputs={X=['mask_lm_trans_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_72.tmp_0']} = mul(inputs={X=['gather_0.tmp_0'], Y=['mask_lm_trans_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 1, y_num_col_dims = 1)
    {Out=['mask_lm_trans_fc.b_0.cast_fp16']} = cast(inputs={X=['mask_lm_trans_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_72.tmp_1']} = elementwise_add(inputs={X=['fc_72.tmp_0'], Y=['mask_lm_trans_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_72.tmp_1.cast_fp32']} = cast(inputs={X=['fc_72.tmp_1']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['pow_12.tmp_0']} = pow(inputs={FactorTensor=[], X=['fc_72.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['tmp_110']} = scale(inputs={ScaleTensor=[], X=['pow_12.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.044714998453855515, use_mkldnn = False)
    {Out=['tmp_111']} = elementwise_add(inputs={X=['fc_72.tmp_1.cast_fp32'], Y=['tmp_110']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_112']} = scale(inputs={ScaleTensor=[], X=['tmp_111']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.7978845834732056, use_mkldnn = False)
    {Out=['tanh_12.tmp_0']} = tanh(inputs={X=['tmp_112']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_113']} = scale(inputs={ScaleTensor=[], X=['tanh_12.tmp_0']}, bias = 1.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0, use_mkldnn = False)
    {Out=['tmp_114']} = scale(inputs={ScaleTensor=[], X=['tmp_113']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.5, use_mkldnn = False)
    {Out=['tmp_115']} = elementwise_mul(inputs={X=['fc_72.tmp_1.cast_fp32'], Y=['tmp_114']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_115.cast_fp16']} = cast(inputs={X=['tmp_115']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Mean=['layer_norm_25.tmp_0'], Variance=['layer_norm_25.tmp_1'], Y=['layer_norm_25.tmp_2']} = layer_norm(inputs={Bias=['mask_lm_trans_layer_norm_bias'], Scale=['mask_lm_trans_layer_norm_scale'], X=['tmp_115.cast_fp16']}, begin_norm_axis = 1, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['mask_lm_out_fc.w_0.cast_fp16']} = cast(inputs={X=['mask_lm_out_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_73.tmp_0']} = mul(inputs={X=['layer_norm_25.tmp_2'], Y=['mask_lm_out_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 1, y_num_col_dims = 1)
    {Out=['mask_lm_out_fc.b_0.cast_fp16']} = cast(inputs={X=['mask_lm_out_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_73.tmp_1']} = elementwise_add(inputs={X=['fc_73.tmp_0'], Y=['mask_lm_out_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_73.tmp_1.cast_fp32']} = cast(inputs={X=['fc_73.tmp_1']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Backprop=['softmax_with_cross_entropy_0.tmp_2'], Loss=['softmax_with_cross_entropy_0.tmp_1'], Softmax=['softmax_with_cross_entropy_0.tmp_0']} = softmax_with_cross_entropy(inputs={Label=['mask_label'], Logits=['fc_73.tmp_1.cast_fp32']}, axis = -1, ignore_index = -100, numeric_stable_mode = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], soft_label = False, use_softmax = True)
    {Out=['mean_mask_lm_loss.tmp_0']} = mean(inputs={X=['softmax_with_cross_entropy_0.tmp_1']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['fill_constant_3.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [1], str_value = 0.0, value = 0.0)
    {Out=['tmp_116']} = recv_v2(inputs={}, dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_shape = [1], peer = 0, ring_id = 20, srTag = 0, tag = tag, use_calc_stream = True)
    {Out=['tmp_117']} = elementwise_div(inputs={X=['mean_mask_lm_loss.tmp_0'], Y=['tmp_116']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fill_constant_3.tmp_0']} = elementwise_add(inputs={X=['fill_constant_3.tmp_0'], Y=['tmp_117']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['cast_2.tmp_0']} = cast(inputs={X=['@LR_DECAY_COUNTER@']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 16, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['tmp_118']} = fill_constant(inputs={}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [1], str_value = , value = 10000.0)
    {Out=['tmp_119']} = less_than(inputs={X=['cast_2.tmp_0'], Y=['tmp_118']}, axis = -1, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [])
    {Out=['logical_not_1.tmp_0']} = logical_not(inputs={X=['tmp_119']}, op_device = , op_namescope = /, op_role = 16, op_role_var = [])
    {Out=['scheduled_learning_rate'], Scope=['_generated_var_0']} = conditional_block(inputs={Cond=['tmp_119'], Input=['cast_2.tmp_0']}, is_scalar_condition = True, op_device = , op_namescope = /, op_role = 16, op_role_var = [], skip_eager_deletion_vars = [], sub_block = block[1])
    {Out=['scheduled_learning_rate'], Scope=['_generated_var_1']} = conditional_block(inputs={Cond=['logical_not_1.tmp_0'], Input=['@LR_DECAY_COUNTER@']}, is_scalar_condition = True, op_device = , op_namescope = /, op_role = 16, op_role_var = [], skip_eager_deletion_vars = [], sub_block = block[2])
    {FloatStatus=['float_status']} = alloc_float_status(inputs={}, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['tmp_130']} = elementwise_mul(inputs={X=['mean_mask_lm_loss.tmp_0'], Y=['loss_scaling_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 256, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_130@GRAD']} = fill_constant(inputs={}, dtype = 5, force_cpu = False, op_device = , op_role = 257, shape = [1], value = 1.0)
    {Out=['tmp_130@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_130@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], scale = 0.125, use_mkldnn = False)
    {X@GRAD=['mean_mask_lm_loss.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_130@GRAD'], X=['mean_mask_lm_loss.tmp_0'], Y=['loss_scaling_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['softmax_with_cross_entropy_0.tmp_1@GRAD']} = mean_grad(inputs={Out@GRAD=['mean_mask_lm_loss.tmp_0@GRAD'], X=['softmax_with_cross_entropy_0.tmp_1']}, op_device = , op_role = 1)
    {Logits@GRAD=['fc_73.tmp_1.cast_fp32@GRAD']} = softmax_with_cross_entropy_grad(inputs={Backprop=['softmax_with_cross_entropy_0.tmp_2'], Label=['mask_label'], Loss@GRAD=['softmax_with_cross_entropy_0.tmp_1@GRAD'], Softmax=['softmax_with_cross_entropy_0.tmp_0']}, axis = -1, ignore_index = -100, numeric_stable_mode = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], soft_label = False, use_softmax = True)
    {Out=['fc_73.tmp_1@GRAD']} = cast(inputs={X=['fc_73.tmp_1.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_role = 1, out_dtype = 4, use_mkldnn = False)
    {X@GRAD=['fc_73.tmp_0@GRAD'], Y@GRAD=['mask_lm_out_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_73.tmp_1@GRAD'], X=['fc_73.tmp_0'], Y=['mask_lm_out_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['mask_lm_out_fc.b_0', 'mask_lm_out_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_25.tmp_2@GRAD'], Y@GRAD=['mask_lm_out_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_73.tmp_0@GRAD'], X=['layer_norm_25.tmp_2'], Y=['mask_lm_out_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['mask_lm_out_fc.w_0', 'mask_lm_out_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 1, y_num_col_dims = 1)
    {Bias@GRAD=['mask_lm_trans_layer_norm_bias@GRAD'], Scale@GRAD=['mask_lm_trans_layer_norm_scale@GRAD'], X@GRAD=['tmp_115.cast_fp16@GRAD']} = layer_norm_grad(inputs={Bias=['mask_lm_trans_layer_norm_bias'], Mean=['layer_norm_25.tmp_0'], Scale=['mask_lm_trans_layer_norm_scale'], Variance=['layer_norm_25.tmp_1'], X=['tmp_115.cast_fp16'], Y@GRAD=['layer_norm_25.tmp_2@GRAD']}, begin_norm_axis = 1, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['mask_lm_trans_layer_norm_bias', 'mask_lm_trans_layer_norm_bias@GRAD', 'mask_lm_trans_layer_norm_scale', 'mask_lm_trans_layer_norm_scale@GRAD'], use_mkldnn = False)
    {Out=['tmp_115@GRAD']} = cast(inputs={X=['tmp_115.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_role = 1, out_dtype = 5, use_mkldnn = False)
    {X@GRAD=['fc_72.tmp_1.cast_fp32@GRAD@RENAME@block0@0'], Y@GRAD=['tmp_114@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_115@GRAD'], X=['fc_72.tmp_1.cast_fp32'], Y=['tmp_114']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_113@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_114@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.5)
    {Out=['tanh_12.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_113@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 1.0)
    {X@GRAD=['tmp_112@GRAD']} = tanh_grad(inputs={Out=['tanh_12.tmp_0'], Out@GRAD=['tanh_12.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_111@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_112@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.7978845834732056)
    {X@GRAD=['fc_72.tmp_1.cast_fp32@GRAD@RENAME@block0@1'], Y@GRAD=['tmp_110@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_111@GRAD'], X=['fc_72.tmp_1.cast_fp32'], Y=['tmp_110']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['pow_12.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_110@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.044714998453855515)
    {X@GRAD=['fc_72.tmp_1.cast_fp32@GRAD@RENAME@block0@2']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_12.tmp_0@GRAD'], X=['fc_72.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 1, op_role_var = [])
    {Out=['fc_72.tmp_1.cast_fp32@GRAD']} = sum(inputs={X=['fc_72.tmp_1.cast_fp32@GRAD@RENAME@block0@0', 'fc_72.tmp_1.cast_fp32@GRAD@RENAME@block0@1', 'fc_72.tmp_1.cast_fp32@GRAD@RENAME@block0@2']}, op_device = , op_role = 1, use_mkldnn = False)
    {Out=['fc_72.tmp_1@GRAD']} = cast(inputs={X=['fc_72.tmp_1.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_role = 1, out_dtype = 4, use_mkldnn = False)
    {X@GRAD=['fc_72.tmp_0@GRAD'], Y@GRAD=['mask_lm_trans_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_72.tmp_1@GRAD'], X=['fc_72.tmp_0'], Y=['mask_lm_trans_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['mask_lm_trans_fc.b_0', 'mask_lm_trans_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['gather_0.tmp_0@GRAD'], Y@GRAD=['mask_lm_trans_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_72.tmp_0@GRAD'], X=['gather_0.tmp_0'], Y=['mask_lm_trans_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['mask_lm_trans_fc.w_0', 'mask_lm_trans_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 1, y_num_col_dims = 1)
    {X@GRAD=['reshape2_48.tmp_0@GRAD']} = gather_grad(inputs={Axis=[], Index=['cast_1.tmp_0'], Out@GRAD=['gather_0.tmp_0@GRAD'], X=['reshape2_48.tmp_0']}, axis = 0, op_device = , op_namescope = /, op_role = 1, op_role_var = [], overwrite = True)
    {X@GRAD=['layer_norm_24.tmp_2@GRAD']} = reshape2_grad(inputs={Out@GRAD=['reshape2_48.tmp_0@GRAD'], XShape=['reshape2_48.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [-1, 768], use_quantizer = False)
    {Bias@GRAD=['encoder_layer_11_post_ffn_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_11_post_ffn_layer_norm_scale@GRAD'], X@GRAD=['tmp_109@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_11_post_ffn_layer_norm_bias'], Mean=['layer_norm_24.tmp_0'], Scale=['encoder_layer_11_post_ffn_layer_norm_scale'], Variance=['layer_norm_24.tmp_1'], X=['tmp_109'], Y@GRAD=['layer_norm_24.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_11_post_ffn_layer_norm_bias', 'encoder_layer_11_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_11_post_ffn_layer_norm_scale', 'encoder_layer_11_post_ffn_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_71.tmp_1@GRAD'], Y@GRAD=['layer_norm_23.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_109@GRAD'], X=['fc_71.tmp_1'], Y=['layer_norm_23.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_71.tmp_0@GRAD'], Y@GRAD=['encoder_layer_11_ffn_fc_1.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_71.tmp_1@GRAD'], X=['fc_71.tmp_0'], Y=['encoder_layer_11_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_11_ffn_fc_1.b_0', 'encoder_layer_11_ffn_fc_1.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['tmp_108.cast_fp16@GRAD'], Y@GRAD=['encoder_layer_11_ffn_fc_1.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_71.tmp_0@GRAD'], X=['tmp_108.cast_fp16'], Y=['encoder_layer_11_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_11_ffn_fc_1.w_0', 'encoder_layer_11_ffn_fc_1.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['tmp_108@GRAD']} = cast(inputs={X=['tmp_108.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_role = 1, out_dtype = 5, use_mkldnn = False)
    {X@GRAD=['fc_70.tmp_1.cast_fp32@GRAD@RENAME@block0@0'], Y@GRAD=['tmp_107@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_108@GRAD'], X=['fc_70.tmp_1.cast_fp32'], Y=['tmp_107']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_106@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_107@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.5)
    {Out=['tanh_11.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_106@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 1.0)
    {X@GRAD=['tmp_105@GRAD']} = tanh_grad(inputs={Out=['tanh_11.tmp_0'], Out@GRAD=['tanh_11.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_104@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_105@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.7978845834732056)
    {X@GRAD=['fc_70.tmp_1.cast_fp32@GRAD@RENAME@block0@1'], Y@GRAD=['tmp_103@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_104@GRAD'], X=['fc_70.tmp_1.cast_fp32'], Y=['tmp_103']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['pow_11.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_103@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.044714998453855515)
    {X@GRAD=['fc_70.tmp_1.cast_fp32@GRAD@RENAME@block0@2']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_11.tmp_0@GRAD'], X=['fc_70.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 1, op_role_var = [])
    {Out=['fc_70.tmp_1.cast_fp32@GRAD']} = sum(inputs={X=['fc_70.tmp_1.cast_fp32@GRAD@RENAME@block0@0', 'fc_70.tmp_1.cast_fp32@GRAD@RENAME@block0@1', 'fc_70.tmp_1.cast_fp32@GRAD@RENAME@block0@2']}, op_device = , op_role = 1, use_mkldnn = False)
    {Out=['fc_70.tmp_1@GRAD']} = cast(inputs={X=['fc_70.tmp_1.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_role = 1, out_dtype = 4, use_mkldnn = False)
    {X@GRAD=['fc_70.tmp_0@GRAD'], Y@GRAD=['encoder_layer_11_ffn_fc_0.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_70.tmp_1@GRAD'], X=['fc_70.tmp_0'], Y=['encoder_layer_11_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_11_ffn_fc_0.b_0', 'encoder_layer_11_ffn_fc_0.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_23.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_11_ffn_fc_0.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_70.tmp_0@GRAD'], X=['layer_norm_23.tmp_2'], Y=['encoder_layer_11_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_11_ffn_fc_0.w_0', 'encoder_layer_11_ffn_fc_0.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_23.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_23.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_23.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_11_post_att_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_11_post_att_layer_norm_scale@GRAD'], X@GRAD=['tmp_102@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_11_post_att_layer_norm_bias'], Mean=['layer_norm_23.tmp_0'], Scale=['encoder_layer_11_post_att_layer_norm_scale'], Variance=['layer_norm_23.tmp_1'], X=['tmp_102'], Y@GRAD=['layer_norm_23.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_11_post_att_layer_norm_bias', 'encoder_layer_11_post_att_layer_norm_bias@GRAD', 'encoder_layer_11_post_att_layer_norm_scale', 'encoder_layer_11_post_att_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_69.tmp_1@GRAD'], Y@GRAD=['layer_norm_22.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_102@GRAD'], X=['fc_69.tmp_1'], Y=['layer_norm_22.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_69.tmp_0@GRAD'], Y@GRAD=['encoder_layer_11_multi_head_att_output_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_69.tmp_1@GRAD'], X=['fc_69.tmp_0'], Y=['encoder_layer_11_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_11_multi_head_att_output_fc.b_0', 'encoder_layer_11_multi_head_att_output_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_47.tmp_0@GRAD'], Y@GRAD=['encoder_layer_11_multi_head_att_output_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_69.tmp_0@GRAD'], X=['transpose_47.tmp_0'], Y=['encoder_layer_11_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_11_multi_head_att_output_fc.w_0', 'encoder_layer_11_multi_head_att_output_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['transpose_47.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['transpose_47.tmp_0@GRAD'], XShape=['reshape2_47.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_24.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_47.tmp_0@GRAD'], XShape=['transpose_47.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_11.tmp_0@GRAD'], Y@GRAD=['transpose_46.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_24.tmp_0@GRAD'], X=['softmax_11.tmp_0'], Y=['transpose_46.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {X@GRAD=['tmp_101@GRAD']} = softmax_grad(inputs={Out=['softmax_11.tmp_0'], Out@GRAD=['softmax_11.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_v2_23.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_101@GRAD'], X=['matmul_v2_23.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['scale_12.tmp_0@GRAD'], Y@GRAD=['transpose_45.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_23.tmp_0@GRAD'], X=['scale_12.tmp_0'], Y=['transpose_45.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['transpose_44.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_12.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.125)
    {X@GRAD=['fc_68.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_46.tmp_0@GRAD'], XShape=['transpose_46.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_68.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_68.tmp_1@GRAD'], XShape=['reshape2_46.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_67.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_45.tmp_0@GRAD'], XShape=['transpose_45.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_67.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_67.tmp_1@GRAD'], XShape=['reshape2_45.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_66.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_44.tmp_0@GRAD'], XShape=['transpose_44.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_66.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_66.tmp_1@GRAD'], XShape=['reshape2_44.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_68.tmp_0@GRAD'], Y@GRAD=['encoder_layer_11_multi_head_att_value_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_68.tmp_1@GRAD'], X=['fc_68.tmp_0'], Y=['encoder_layer_11_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_11_multi_head_att_value_fc.b_0', 'encoder_layer_11_multi_head_att_value_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_22.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_11_multi_head_att_value_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_68.tmp_0@GRAD'], X=['layer_norm_22.tmp_2'], Y=['encoder_layer_11_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_11_multi_head_att_value_fc.w_0', 'encoder_layer_11_multi_head_att_value_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_67.tmp_0@GRAD'], Y@GRAD=['encoder_layer_11_multi_head_att_key_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_67.tmp_1@GRAD'], X=['fc_67.tmp_0'], Y=['encoder_layer_11_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_11_multi_head_att_key_fc.b_0', 'encoder_layer_11_multi_head_att_key_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_22.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['encoder_layer_11_multi_head_att_key_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_67.tmp_0@GRAD'], X=['layer_norm_22.tmp_2'], Y=['encoder_layer_11_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_11_multi_head_att_key_fc.w_0', 'encoder_layer_11_multi_head_att_key_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_66.tmp_0@GRAD'], Y@GRAD=['encoder_layer_11_multi_head_att_query_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_66.tmp_1@GRAD'], X=['fc_66.tmp_0'], Y=['encoder_layer_11_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_11_multi_head_att_query_fc.b_0', 'encoder_layer_11_multi_head_att_query_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_22.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['encoder_layer_11_multi_head_att_query_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_66.tmp_0@GRAD'], X=['layer_norm_22.tmp_2'], Y=['encoder_layer_11_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_11_multi_head_att_query_fc.w_0', 'encoder_layer_11_multi_head_att_query_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_22.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_22.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_22.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_22.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_22.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_10_post_ffn_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_10_post_ffn_layer_norm_scale@GRAD'], X@GRAD=['tmp_100@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_10_post_ffn_layer_norm_bias'], Mean=['layer_norm_22.tmp_0'], Scale=['encoder_layer_10_post_ffn_layer_norm_scale'], Variance=['layer_norm_22.tmp_1'], X=['tmp_100'], Y@GRAD=['layer_norm_22.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_10_post_ffn_layer_norm_bias', 'encoder_layer_10_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_10_post_ffn_layer_norm_scale', 'encoder_layer_10_post_ffn_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_65.tmp_1@GRAD'], Y@GRAD=['layer_norm_21.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_100@GRAD'], X=['fc_65.tmp_1'], Y=['layer_norm_21.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_65.tmp_0@GRAD'], Y@GRAD=['encoder_layer_10_ffn_fc_1.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_65.tmp_1@GRAD'], X=['fc_65.tmp_0'], Y=['encoder_layer_10_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_10_ffn_fc_1.b_0', 'encoder_layer_10_ffn_fc_1.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['tmp_99.cast_fp16@GRAD'], Y@GRAD=['encoder_layer_10_ffn_fc_1.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_65.tmp_0@GRAD'], X=['tmp_99.cast_fp16'], Y=['encoder_layer_10_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_10_ffn_fc_1.w_0', 'encoder_layer_10_ffn_fc_1.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['tmp_99@GRAD']} = cast(inputs={X=['tmp_99.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_role = 1, out_dtype = 5, use_mkldnn = False)
    {X@GRAD=['fc_64.tmp_1.cast_fp32@GRAD@RENAME@block0@0'], Y@GRAD=['tmp_98@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_99@GRAD'], X=['fc_64.tmp_1.cast_fp32'], Y=['tmp_98']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_97@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_98@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.5)
    {Out=['tanh_10.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_97@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 1.0)
    {X@GRAD=['tmp_96@GRAD']} = tanh_grad(inputs={Out=['tanh_10.tmp_0'], Out@GRAD=['tanh_10.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_95@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_96@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.7978845834732056)
    {X@GRAD=['fc_64.tmp_1.cast_fp32@GRAD@RENAME@block0@1'], Y@GRAD=['tmp_94@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_95@GRAD'], X=['fc_64.tmp_1.cast_fp32'], Y=['tmp_94']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['pow_10.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_94@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.044714998453855515)
    {X@GRAD=['fc_64.tmp_1.cast_fp32@GRAD@RENAME@block0@2']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_10.tmp_0@GRAD'], X=['fc_64.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 1, op_role_var = [])
    {Out=['fc_64.tmp_1.cast_fp32@GRAD']} = sum(inputs={X=['fc_64.tmp_1.cast_fp32@GRAD@RENAME@block0@0', 'fc_64.tmp_1.cast_fp32@GRAD@RENAME@block0@1', 'fc_64.tmp_1.cast_fp32@GRAD@RENAME@block0@2']}, op_device = , op_role = 1, use_mkldnn = False)
    {Out=['fc_64.tmp_1@GRAD']} = cast(inputs={X=['fc_64.tmp_1.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_role = 1, out_dtype = 4, use_mkldnn = False)
    {X@GRAD=['fc_64.tmp_0@GRAD'], Y@GRAD=['encoder_layer_10_ffn_fc_0.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_64.tmp_1@GRAD'], X=['fc_64.tmp_0'], Y=['encoder_layer_10_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_10_ffn_fc_0.b_0', 'encoder_layer_10_ffn_fc_0.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_21.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_10_ffn_fc_0.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_64.tmp_0@GRAD'], X=['layer_norm_21.tmp_2'], Y=['encoder_layer_10_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_10_ffn_fc_0.w_0', 'encoder_layer_10_ffn_fc_0.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_21.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_21.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_21.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_10_post_att_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_10_post_att_layer_norm_scale@GRAD'], X@GRAD=['tmp_93@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_10_post_att_layer_norm_bias'], Mean=['layer_norm_21.tmp_0'], Scale=['encoder_layer_10_post_att_layer_norm_scale'], Variance=['layer_norm_21.tmp_1'], X=['tmp_93'], Y@GRAD=['layer_norm_21.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_10_post_att_layer_norm_bias', 'encoder_layer_10_post_att_layer_norm_bias@GRAD', 'encoder_layer_10_post_att_layer_norm_scale', 'encoder_layer_10_post_att_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_63.tmp_1@GRAD'], Y@GRAD=['layer_norm_20.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_93@GRAD'], X=['fc_63.tmp_1'], Y=['layer_norm_20.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_63.tmp_0@GRAD'], Y@GRAD=['encoder_layer_10_multi_head_att_output_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_63.tmp_1@GRAD'], X=['fc_63.tmp_0'], Y=['encoder_layer_10_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_10_multi_head_att_output_fc.b_0', 'encoder_layer_10_multi_head_att_output_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_43.tmp_0@GRAD'], Y@GRAD=['encoder_layer_10_multi_head_att_output_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_63.tmp_0@GRAD'], X=['transpose_43.tmp_0'], Y=['encoder_layer_10_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_10_multi_head_att_output_fc.w_0', 'encoder_layer_10_multi_head_att_output_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['transpose_43.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['transpose_43.tmp_0@GRAD'], XShape=['reshape2_43.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_22.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_43.tmp_0@GRAD'], XShape=['transpose_43.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_10.tmp_0@GRAD'], Y@GRAD=['transpose_42.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_22.tmp_0@GRAD'], X=['softmax_10.tmp_0'], Y=['transpose_42.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {X@GRAD=['tmp_92@GRAD']} = softmax_grad(inputs={Out=['softmax_10.tmp_0'], Out@GRAD=['softmax_10.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_v2_21.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_92@GRAD'], X=['matmul_v2_21.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['scale_11.tmp_0@GRAD'], Y@GRAD=['transpose_41.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_21.tmp_0@GRAD'], X=['scale_11.tmp_0'], Y=['transpose_41.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['transpose_40.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_11.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.125)
    {X@GRAD=['fc_62.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_42.tmp_0@GRAD'], XShape=['transpose_42.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_62.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_62.tmp_1@GRAD'], XShape=['reshape2_42.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_61.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_41.tmp_0@GRAD'], XShape=['transpose_41.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_61.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_61.tmp_1@GRAD'], XShape=['reshape2_41.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_60.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_40.tmp_0@GRAD'], XShape=['transpose_40.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_60.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_60.tmp_1@GRAD'], XShape=['reshape2_40.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_62.tmp_0@GRAD'], Y@GRAD=['encoder_layer_10_multi_head_att_value_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_62.tmp_1@GRAD'], X=['fc_62.tmp_0'], Y=['encoder_layer_10_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_10_multi_head_att_value_fc.b_0', 'encoder_layer_10_multi_head_att_value_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_20.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_10_multi_head_att_value_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_62.tmp_0@GRAD'], X=['layer_norm_20.tmp_2'], Y=['encoder_layer_10_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_10_multi_head_att_value_fc.w_0', 'encoder_layer_10_multi_head_att_value_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_61.tmp_0@GRAD'], Y@GRAD=['encoder_layer_10_multi_head_att_key_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_61.tmp_1@GRAD'], X=['fc_61.tmp_0'], Y=['encoder_layer_10_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_10_multi_head_att_key_fc.b_0', 'encoder_layer_10_multi_head_att_key_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_20.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['encoder_layer_10_multi_head_att_key_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_61.tmp_0@GRAD'], X=['layer_norm_20.tmp_2'], Y=['encoder_layer_10_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_10_multi_head_att_key_fc.w_0', 'encoder_layer_10_multi_head_att_key_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_60.tmp_0@GRAD'], Y@GRAD=['encoder_layer_10_multi_head_att_query_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_60.tmp_1@GRAD'], X=['fc_60.tmp_0'], Y=['encoder_layer_10_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_10_multi_head_att_query_fc.b_0', 'encoder_layer_10_multi_head_att_query_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_20.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['encoder_layer_10_multi_head_att_query_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_60.tmp_0@GRAD'], X=['layer_norm_20.tmp_2'], Y=['encoder_layer_10_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_10_multi_head_att_query_fc.w_0', 'encoder_layer_10_multi_head_att_query_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_20.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_20.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_20.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_20.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_20.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_9_post_ffn_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_9_post_ffn_layer_norm_scale@GRAD'], X@GRAD=['tmp_91@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_9_post_ffn_layer_norm_bias'], Mean=['layer_norm_20.tmp_0'], Scale=['encoder_layer_9_post_ffn_layer_norm_scale'], Variance=['layer_norm_20.tmp_1'], X=['tmp_91'], Y@GRAD=['layer_norm_20.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_9_post_ffn_layer_norm_bias', 'encoder_layer_9_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_9_post_ffn_layer_norm_scale', 'encoder_layer_9_post_ffn_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_59.tmp_1@GRAD'], Y@GRAD=['layer_norm_19.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_91@GRAD'], X=['fc_59.tmp_1'], Y=['layer_norm_19.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_59.tmp_0@GRAD'], Y@GRAD=['encoder_layer_9_ffn_fc_1.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_59.tmp_1@GRAD'], X=['fc_59.tmp_0'], Y=['encoder_layer_9_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_9_ffn_fc_1.b_0', 'encoder_layer_9_ffn_fc_1.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['tmp_90.cast_fp16@GRAD'], Y@GRAD=['encoder_layer_9_ffn_fc_1.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_59.tmp_0@GRAD'], X=['tmp_90.cast_fp16'], Y=['encoder_layer_9_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_9_ffn_fc_1.w_0', 'encoder_layer_9_ffn_fc_1.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['tmp_90@GRAD']} = cast(inputs={X=['tmp_90.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_role = 1, out_dtype = 5, use_mkldnn = False)
    {X@GRAD=['fc_58.tmp_1.cast_fp32@GRAD@RENAME@block0@0'], Y@GRAD=['tmp_89@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_90@GRAD'], X=['fc_58.tmp_1.cast_fp32'], Y=['tmp_89']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_88@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_89@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.5)
    {Out=['tanh_9.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_88@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 1.0)
    {X@GRAD=['tmp_87@GRAD']} = tanh_grad(inputs={Out=['tanh_9.tmp_0'], Out@GRAD=['tanh_9.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_86@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_87@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.7978845834732056)
    {X@GRAD=['fc_58.tmp_1.cast_fp32@GRAD@RENAME@block0@1'], Y@GRAD=['tmp_85@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_86@GRAD'], X=['fc_58.tmp_1.cast_fp32'], Y=['tmp_85']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['pow_9.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_85@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.044714998453855515)
    {X@GRAD=['fc_58.tmp_1.cast_fp32@GRAD@RENAME@block0@2']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_9.tmp_0@GRAD'], X=['fc_58.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 1, op_role_var = [])
    {Out=['fc_58.tmp_1.cast_fp32@GRAD']} = sum(inputs={X=['fc_58.tmp_1.cast_fp32@GRAD@RENAME@block0@0', 'fc_58.tmp_1.cast_fp32@GRAD@RENAME@block0@1', 'fc_58.tmp_1.cast_fp32@GRAD@RENAME@block0@2']}, op_device = , op_role = 1, use_mkldnn = False)
    {Out=['fc_58.tmp_1@GRAD']} = cast(inputs={X=['fc_58.tmp_1.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_role = 1, out_dtype = 4, use_mkldnn = False)
    {X@GRAD=['fc_58.tmp_0@GRAD'], Y@GRAD=['encoder_layer_9_ffn_fc_0.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_58.tmp_1@GRAD'], X=['fc_58.tmp_0'], Y=['encoder_layer_9_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_9_ffn_fc_0.b_0', 'encoder_layer_9_ffn_fc_0.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_19.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_9_ffn_fc_0.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_58.tmp_0@GRAD'], X=['layer_norm_19.tmp_2'], Y=['encoder_layer_9_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_9_ffn_fc_0.w_0', 'encoder_layer_9_ffn_fc_0.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_19.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_19.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_19.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_9_post_att_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_9_post_att_layer_norm_scale@GRAD'], X@GRAD=['tmp_84@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_9_post_att_layer_norm_bias'], Mean=['layer_norm_19.tmp_0'], Scale=['encoder_layer_9_post_att_layer_norm_scale'], Variance=['layer_norm_19.tmp_1'], X=['tmp_84'], Y@GRAD=['layer_norm_19.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_9_post_att_layer_norm_bias', 'encoder_layer_9_post_att_layer_norm_bias@GRAD', 'encoder_layer_9_post_att_layer_norm_scale', 'encoder_layer_9_post_att_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_57.tmp_1@GRAD'], Y@GRAD=['layer_norm_18.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_84@GRAD'], X=['fc_57.tmp_1'], Y=['layer_norm_18.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_57.tmp_0@GRAD'], Y@GRAD=['encoder_layer_9_multi_head_att_output_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_57.tmp_1@GRAD'], X=['fc_57.tmp_0'], Y=['encoder_layer_9_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_9_multi_head_att_output_fc.b_0', 'encoder_layer_9_multi_head_att_output_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_39.tmp_0@GRAD'], Y@GRAD=['encoder_layer_9_multi_head_att_output_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_57.tmp_0@GRAD'], X=['transpose_39.tmp_0'], Y=['encoder_layer_9_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_9_multi_head_att_output_fc.w_0', 'encoder_layer_9_multi_head_att_output_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['transpose_39.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['transpose_39.tmp_0@GRAD'], XShape=['reshape2_39.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_20.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_39.tmp_0@GRAD'], XShape=['transpose_39.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_9.tmp_0@GRAD'], Y@GRAD=['transpose_38.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_20.tmp_0@GRAD'], X=['softmax_9.tmp_0'], Y=['transpose_38.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {X@GRAD=['tmp_83@GRAD']} = softmax_grad(inputs={Out=['softmax_9.tmp_0'], Out@GRAD=['softmax_9.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_v2_19.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_83@GRAD'], X=['matmul_v2_19.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['scale_10.tmp_0@GRAD'], Y@GRAD=['transpose_37.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_19.tmp_0@GRAD'], X=['scale_10.tmp_0'], Y=['transpose_37.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['transpose_36.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_10.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.125)
    {X@GRAD=['fc_56.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_38.tmp_0@GRAD'], XShape=['transpose_38.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_56.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_56.tmp_1@GRAD'], XShape=['reshape2_38.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_55.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_37.tmp_0@GRAD'], XShape=['transpose_37.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_55.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_55.tmp_1@GRAD'], XShape=['reshape2_37.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_54.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_36.tmp_0@GRAD'], XShape=['transpose_36.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_54.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_54.tmp_1@GRAD'], XShape=['reshape2_36.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_56.tmp_0@GRAD'], Y@GRAD=['encoder_layer_9_multi_head_att_value_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_56.tmp_1@GRAD'], X=['fc_56.tmp_0'], Y=['encoder_layer_9_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_9_multi_head_att_value_fc.b_0', 'encoder_layer_9_multi_head_att_value_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_18.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_9_multi_head_att_value_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_56.tmp_0@GRAD'], X=['layer_norm_18.tmp_2'], Y=['encoder_layer_9_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_9_multi_head_att_value_fc.w_0', 'encoder_layer_9_multi_head_att_value_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_55.tmp_0@GRAD'], Y@GRAD=['encoder_layer_9_multi_head_att_key_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_55.tmp_1@GRAD'], X=['fc_55.tmp_0'], Y=['encoder_layer_9_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_9_multi_head_att_key_fc.b_0', 'encoder_layer_9_multi_head_att_key_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_18.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['encoder_layer_9_multi_head_att_key_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_55.tmp_0@GRAD'], X=['layer_norm_18.tmp_2'], Y=['encoder_layer_9_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_9_multi_head_att_key_fc.w_0', 'encoder_layer_9_multi_head_att_key_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_54.tmp_0@GRAD'], Y@GRAD=['encoder_layer_9_multi_head_att_query_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_54.tmp_1@GRAD'], X=['fc_54.tmp_0'], Y=['encoder_layer_9_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_9_multi_head_att_query_fc.b_0', 'encoder_layer_9_multi_head_att_query_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_18.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['encoder_layer_9_multi_head_att_query_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_54.tmp_0@GRAD'], X=['layer_norm_18.tmp_2'], Y=['encoder_layer_9_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_9_multi_head_att_query_fc.w_0', 'encoder_layer_9_multi_head_att_query_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_18.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_18.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_18.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_18.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_18.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_8_post_ffn_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_8_post_ffn_layer_norm_scale@GRAD'], X@GRAD=['tmp_82@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_8_post_ffn_layer_norm_bias'], Mean=['layer_norm_18.tmp_0'], Scale=['encoder_layer_8_post_ffn_layer_norm_scale'], Variance=['layer_norm_18.tmp_1'], X=['tmp_82'], Y@GRAD=['layer_norm_18.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_8_post_ffn_layer_norm_bias', 'encoder_layer_8_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_8_post_ffn_layer_norm_scale', 'encoder_layer_8_post_ffn_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_53.tmp_1@GRAD'], Y@GRAD=['layer_norm_17.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_82@GRAD'], X=['fc_53.tmp_1'], Y=['layer_norm_17.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_53.tmp_0@GRAD'], Y@GRAD=['encoder_layer_8_ffn_fc_1.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_53.tmp_1@GRAD'], X=['fc_53.tmp_0'], Y=['encoder_layer_8_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_8_ffn_fc_1.b_0', 'encoder_layer_8_ffn_fc_1.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['tmp_81.cast_fp16@GRAD'], Y@GRAD=['encoder_layer_8_ffn_fc_1.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_53.tmp_0@GRAD'], X=['tmp_81.cast_fp16'], Y=['encoder_layer_8_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_8_ffn_fc_1.w_0', 'encoder_layer_8_ffn_fc_1.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['tmp_81@GRAD']} = cast(inputs={X=['tmp_81.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_role = 1, out_dtype = 5, use_mkldnn = False)
    {X@GRAD=['fc_52.tmp_1.cast_fp32@GRAD@RENAME@block0@0'], Y@GRAD=['tmp_80@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_81@GRAD'], X=['fc_52.tmp_1.cast_fp32'], Y=['tmp_80']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_79@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_80@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.5)
    {Out=['tanh_8.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_79@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 1.0)
    {X@GRAD=['tmp_78@GRAD']} = tanh_grad(inputs={Out=['tanh_8.tmp_0'], Out@GRAD=['tanh_8.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_77@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_78@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.7978845834732056)
    {X@GRAD=['fc_52.tmp_1.cast_fp32@GRAD@RENAME@block0@1'], Y@GRAD=['tmp_76@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_77@GRAD'], X=['fc_52.tmp_1.cast_fp32'], Y=['tmp_76']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['pow_8.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_76@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.044714998453855515)
    {X@GRAD=['fc_52.tmp_1.cast_fp32@GRAD@RENAME@block0@2']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_8.tmp_0@GRAD'], X=['fc_52.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 1, op_role_var = [])
    {Out=['fc_52.tmp_1.cast_fp32@GRAD']} = sum(inputs={X=['fc_52.tmp_1.cast_fp32@GRAD@RENAME@block0@0', 'fc_52.tmp_1.cast_fp32@GRAD@RENAME@block0@1', 'fc_52.tmp_1.cast_fp32@GRAD@RENAME@block0@2']}, op_device = , op_role = 1, use_mkldnn = False)
    {Out=['fc_52.tmp_1@GRAD']} = cast(inputs={X=['fc_52.tmp_1.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_role = 1, out_dtype = 4, use_mkldnn = False)
    {X@GRAD=['fc_52.tmp_0@GRAD'], Y@GRAD=['encoder_layer_8_ffn_fc_0.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_52.tmp_1@GRAD'], X=['fc_52.tmp_0'], Y=['encoder_layer_8_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_8_ffn_fc_0.b_0', 'encoder_layer_8_ffn_fc_0.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_17.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_8_ffn_fc_0.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_52.tmp_0@GRAD'], X=['layer_norm_17.tmp_2'], Y=['encoder_layer_8_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_8_ffn_fc_0.w_0', 'encoder_layer_8_ffn_fc_0.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_17.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_17.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_17.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_8_post_att_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_8_post_att_layer_norm_scale@GRAD'], X@GRAD=['tmp_75@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_8_post_att_layer_norm_bias'], Mean=['layer_norm_17.tmp_0'], Scale=['encoder_layer_8_post_att_layer_norm_scale'], Variance=['layer_norm_17.tmp_1'], X=['tmp_75'], Y@GRAD=['layer_norm_17.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_8_post_att_layer_norm_bias', 'encoder_layer_8_post_att_layer_norm_bias@GRAD', 'encoder_layer_8_post_att_layer_norm_scale', 'encoder_layer_8_post_att_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_51.tmp_1@GRAD'], Y@GRAD=['layer_norm_16.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_75@GRAD'], X=['fc_51.tmp_1'], Y=['layer_norm_16.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_51.tmp_0@GRAD'], Y@GRAD=['encoder_layer_8_multi_head_att_output_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_51.tmp_1@GRAD'], X=['fc_51.tmp_0'], Y=['encoder_layer_8_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_8_multi_head_att_output_fc.b_0', 'encoder_layer_8_multi_head_att_output_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_35.tmp_0@GRAD'], Y@GRAD=['encoder_layer_8_multi_head_att_output_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_51.tmp_0@GRAD'], X=['transpose_35.tmp_0'], Y=['encoder_layer_8_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_8_multi_head_att_output_fc.w_0', 'encoder_layer_8_multi_head_att_output_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['transpose_35.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['transpose_35.tmp_0@GRAD'], XShape=['reshape2_35.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_18.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_35.tmp_0@GRAD'], XShape=['transpose_35.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_8.tmp_0@GRAD'], Y@GRAD=['transpose_34.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_18.tmp_0@GRAD'], X=['softmax_8.tmp_0'], Y=['transpose_34.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {X@GRAD=['tmp_74@GRAD']} = softmax_grad(inputs={Out=['softmax_8.tmp_0'], Out@GRAD=['softmax_8.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_v2_17.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_74@GRAD'], X=['matmul_v2_17.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['scale_9.tmp_0@GRAD'], Y@GRAD=['transpose_33.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_17.tmp_0@GRAD'], X=['scale_9.tmp_0'], Y=['transpose_33.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['transpose_32.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_9.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.125)
    {X@GRAD=['fc_50.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_34.tmp_0@GRAD'], XShape=['transpose_34.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_50.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_50.tmp_1@GRAD'], XShape=['reshape2_34.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_49.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_33.tmp_0@GRAD'], XShape=['transpose_33.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_49.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_49.tmp_1@GRAD'], XShape=['reshape2_33.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_48.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_32.tmp_0@GRAD'], XShape=['transpose_32.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_48.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_48.tmp_1@GRAD'], XShape=['reshape2_32.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_50.tmp_0@GRAD'], Y@GRAD=['encoder_layer_8_multi_head_att_value_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_50.tmp_1@GRAD'], X=['fc_50.tmp_0'], Y=['encoder_layer_8_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_8_multi_head_att_value_fc.b_0', 'encoder_layer_8_multi_head_att_value_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_16.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_8_multi_head_att_value_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_50.tmp_0@GRAD'], X=['layer_norm_16.tmp_2'], Y=['encoder_layer_8_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_8_multi_head_att_value_fc.w_0', 'encoder_layer_8_multi_head_att_value_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_49.tmp_0@GRAD'], Y@GRAD=['encoder_layer_8_multi_head_att_key_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_49.tmp_1@GRAD'], X=['fc_49.tmp_0'], Y=['encoder_layer_8_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_8_multi_head_att_key_fc.b_0', 'encoder_layer_8_multi_head_att_key_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_16.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['encoder_layer_8_multi_head_att_key_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_49.tmp_0@GRAD'], X=['layer_norm_16.tmp_2'], Y=['encoder_layer_8_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_8_multi_head_att_key_fc.w_0', 'encoder_layer_8_multi_head_att_key_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_48.tmp_0@GRAD'], Y@GRAD=['encoder_layer_8_multi_head_att_query_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_48.tmp_1@GRAD'], X=['fc_48.tmp_0'], Y=['encoder_layer_8_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_8_multi_head_att_query_fc.b_0', 'encoder_layer_8_multi_head_att_query_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_16.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['encoder_layer_8_multi_head_att_query_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_48.tmp_0@GRAD'], X=['layer_norm_16.tmp_2'], Y=['encoder_layer_8_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_8_multi_head_att_query_fc.w_0', 'encoder_layer_8_multi_head_att_query_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_16.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_16.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_16.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_16.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_16.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_7_post_ffn_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_7_post_ffn_layer_norm_scale@GRAD'], X@GRAD=['tmp_73@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_7_post_ffn_layer_norm_bias'], Mean=['layer_norm_16.tmp_0'], Scale=['encoder_layer_7_post_ffn_layer_norm_scale'], Variance=['layer_norm_16.tmp_1'], X=['tmp_73'], Y@GRAD=['layer_norm_16.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_7_post_ffn_layer_norm_bias', 'encoder_layer_7_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_7_post_ffn_layer_norm_scale', 'encoder_layer_7_post_ffn_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_47.tmp_1@GRAD'], Y@GRAD=['layer_norm_15.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_73@GRAD'], X=['fc_47.tmp_1'], Y=['layer_norm_15.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_47.tmp_0@GRAD'], Y@GRAD=['encoder_layer_7_ffn_fc_1.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_47.tmp_1@GRAD'], X=['fc_47.tmp_0'], Y=['encoder_layer_7_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_7_ffn_fc_1.b_0', 'encoder_layer_7_ffn_fc_1.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['tmp_72.cast_fp16@GRAD'], Y@GRAD=['encoder_layer_7_ffn_fc_1.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_47.tmp_0@GRAD'], X=['tmp_72.cast_fp16'], Y=['encoder_layer_7_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_7_ffn_fc_1.w_0', 'encoder_layer_7_ffn_fc_1.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['tmp_72@GRAD']} = cast(inputs={X=['tmp_72.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_role = 1, out_dtype = 5, use_mkldnn = False)
    {X@GRAD=['fc_46.tmp_1.cast_fp32@GRAD@RENAME@block0@0'], Y@GRAD=['tmp_71@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_72@GRAD'], X=['fc_46.tmp_1.cast_fp32'], Y=['tmp_71']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_70@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_71@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.5)
    {Out=['tanh_7.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_70@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 1.0)
    {X@GRAD=['tmp_69@GRAD']} = tanh_grad(inputs={Out=['tanh_7.tmp_0'], Out@GRAD=['tanh_7.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_68@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_69@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.7978845834732056)
    {X@GRAD=['fc_46.tmp_1.cast_fp32@GRAD@RENAME@block0@1'], Y@GRAD=['tmp_67@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_68@GRAD'], X=['fc_46.tmp_1.cast_fp32'], Y=['tmp_67']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['pow_7.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_67@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.044714998453855515)
    {X@GRAD=['fc_46.tmp_1.cast_fp32@GRAD@RENAME@block0@2']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_7.tmp_0@GRAD'], X=['fc_46.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 1, op_role_var = [])
    {Out=['fc_46.tmp_1.cast_fp32@GRAD']} = sum(inputs={X=['fc_46.tmp_1.cast_fp32@GRAD@RENAME@block0@0', 'fc_46.tmp_1.cast_fp32@GRAD@RENAME@block0@1', 'fc_46.tmp_1.cast_fp32@GRAD@RENAME@block0@2']}, op_device = , op_role = 1, use_mkldnn = False)
    {Out=['fc_46.tmp_1@GRAD']} = cast(inputs={X=['fc_46.tmp_1.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_role = 1, out_dtype = 4, use_mkldnn = False)
    {X@GRAD=['fc_46.tmp_0@GRAD'], Y@GRAD=['encoder_layer_7_ffn_fc_0.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_46.tmp_1@GRAD'], X=['fc_46.tmp_0'], Y=['encoder_layer_7_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_7_ffn_fc_0.b_0', 'encoder_layer_7_ffn_fc_0.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_15.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_7_ffn_fc_0.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_46.tmp_0@GRAD'], X=['layer_norm_15.tmp_2'], Y=['encoder_layer_7_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_7_ffn_fc_0.w_0', 'encoder_layer_7_ffn_fc_0.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_15.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_15.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_15.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_7_post_att_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_7_post_att_layer_norm_scale@GRAD'], X@GRAD=['tmp_66@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_7_post_att_layer_norm_bias'], Mean=['layer_norm_15.tmp_0'], Scale=['encoder_layer_7_post_att_layer_norm_scale'], Variance=['layer_norm_15.tmp_1'], X=['tmp_66'], Y@GRAD=['layer_norm_15.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_7_post_att_layer_norm_bias', 'encoder_layer_7_post_att_layer_norm_bias@GRAD', 'encoder_layer_7_post_att_layer_norm_scale', 'encoder_layer_7_post_att_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_45.tmp_1@GRAD'], Y@GRAD=['layer_norm_14.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_66@GRAD'], X=['fc_45.tmp_1'], Y=['layer_norm_14.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_45.tmp_0@GRAD'], Y@GRAD=['encoder_layer_7_multi_head_att_output_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_45.tmp_1@GRAD'], X=['fc_45.tmp_0'], Y=['encoder_layer_7_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_7_multi_head_att_output_fc.b_0', 'encoder_layer_7_multi_head_att_output_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_31.tmp_0@GRAD'], Y@GRAD=['encoder_layer_7_multi_head_att_output_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_45.tmp_0@GRAD'], X=['transpose_31.tmp_0'], Y=['encoder_layer_7_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_7_multi_head_att_output_fc.w_0', 'encoder_layer_7_multi_head_att_output_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['transpose_31.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['transpose_31.tmp_0@GRAD'], XShape=['reshape2_31.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_16.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_31.tmp_0@GRAD'], XShape=['transpose_31.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_7.tmp_0@GRAD'], Y@GRAD=['transpose_30.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_16.tmp_0@GRAD'], X=['softmax_7.tmp_0'], Y=['transpose_30.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {X@GRAD=['tmp_65@GRAD']} = softmax_grad(inputs={Out=['softmax_7.tmp_0'], Out@GRAD=['softmax_7.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_v2_15.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_65@GRAD'], X=['matmul_v2_15.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['scale_8.tmp_0@GRAD'], Y@GRAD=['transpose_29.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_15.tmp_0@GRAD'], X=['scale_8.tmp_0'], Y=['transpose_29.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['transpose_28.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_8.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.125)
    {X@GRAD=['fc_44.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_30.tmp_0@GRAD'], XShape=['transpose_30.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_44.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_44.tmp_1@GRAD'], XShape=['reshape2_30.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_43.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_29.tmp_0@GRAD'], XShape=['transpose_29.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_43.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_43.tmp_1@GRAD'], XShape=['reshape2_29.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_42.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_28.tmp_0@GRAD'], XShape=['transpose_28.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_42.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_42.tmp_1@GRAD'], XShape=['reshape2_28.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_44.tmp_0@GRAD'], Y@GRAD=['encoder_layer_7_multi_head_att_value_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_44.tmp_1@GRAD'], X=['fc_44.tmp_0'], Y=['encoder_layer_7_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_7_multi_head_att_value_fc.b_0', 'encoder_layer_7_multi_head_att_value_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_14.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_7_multi_head_att_value_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_44.tmp_0@GRAD'], X=['layer_norm_14.tmp_2'], Y=['encoder_layer_7_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_7_multi_head_att_value_fc.w_0', 'encoder_layer_7_multi_head_att_value_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_43.tmp_0@GRAD'], Y@GRAD=['encoder_layer_7_multi_head_att_key_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_43.tmp_1@GRAD'], X=['fc_43.tmp_0'], Y=['encoder_layer_7_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_7_multi_head_att_key_fc.b_0', 'encoder_layer_7_multi_head_att_key_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_14.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['encoder_layer_7_multi_head_att_key_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_43.tmp_0@GRAD'], X=['layer_norm_14.tmp_2'], Y=['encoder_layer_7_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_7_multi_head_att_key_fc.w_0', 'encoder_layer_7_multi_head_att_key_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_42.tmp_0@GRAD'], Y@GRAD=['encoder_layer_7_multi_head_att_query_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_42.tmp_1@GRAD'], X=['fc_42.tmp_0'], Y=['encoder_layer_7_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_7_multi_head_att_query_fc.b_0', 'encoder_layer_7_multi_head_att_query_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_14.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['encoder_layer_7_multi_head_att_query_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_42.tmp_0@GRAD'], X=['layer_norm_14.tmp_2'], Y=['encoder_layer_7_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_7_multi_head_att_query_fc.w_0', 'encoder_layer_7_multi_head_att_query_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_14.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_14.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_14.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_14.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_14.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_6_post_ffn_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_6_post_ffn_layer_norm_scale@GRAD'], X@GRAD=['tmp_64@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_6_post_ffn_layer_norm_bias'], Mean=['layer_norm_14.tmp_0'], Scale=['encoder_layer_6_post_ffn_layer_norm_scale'], Variance=['layer_norm_14.tmp_1'], X=['tmp_64'], Y@GRAD=['layer_norm_14.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_6_post_ffn_layer_norm_bias', 'encoder_layer_6_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_6_post_ffn_layer_norm_scale', 'encoder_layer_6_post_ffn_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_41.tmp_1@GRAD'], Y@GRAD=['layer_norm_13.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_64@GRAD'], X=['fc_41.tmp_1'], Y=['layer_norm_13.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_41.tmp_0@GRAD'], Y@GRAD=['encoder_layer_6_ffn_fc_1.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_41.tmp_1@GRAD'], X=['fc_41.tmp_0'], Y=['encoder_layer_6_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_6_ffn_fc_1.b_0', 'encoder_layer_6_ffn_fc_1.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['tmp_63.cast_fp16@GRAD'], Y@GRAD=['encoder_layer_6_ffn_fc_1.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_41.tmp_0@GRAD'], X=['tmp_63.cast_fp16'], Y=['encoder_layer_6_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_6_ffn_fc_1.w_0', 'encoder_layer_6_ffn_fc_1.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['tmp_63@GRAD']} = cast(inputs={X=['tmp_63.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_role = 1, out_dtype = 5, use_mkldnn = False)
    {X@GRAD=['fc_40.tmp_1.cast_fp32@GRAD@RENAME@block0@0'], Y@GRAD=['tmp_62@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_63@GRAD'], X=['fc_40.tmp_1.cast_fp32'], Y=['tmp_62']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_61@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_62@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.5)
    {Out=['tanh_6.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_61@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 1.0)
    {X@GRAD=['tmp_60@GRAD']} = tanh_grad(inputs={Out=['tanh_6.tmp_0'], Out@GRAD=['tanh_6.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_59@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_60@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.7978845834732056)
    {X@GRAD=['fc_40.tmp_1.cast_fp32@GRAD@RENAME@block0@1'], Y@GRAD=['tmp_58@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_59@GRAD'], X=['fc_40.tmp_1.cast_fp32'], Y=['tmp_58']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['pow_6.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_58@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.044714998453855515)
    {X@GRAD=['fc_40.tmp_1.cast_fp32@GRAD@RENAME@block0@2']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_6.tmp_0@GRAD'], X=['fc_40.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 1, op_role_var = [])
    {Out=['fc_40.tmp_1.cast_fp32@GRAD']} = sum(inputs={X=['fc_40.tmp_1.cast_fp32@GRAD@RENAME@block0@0', 'fc_40.tmp_1.cast_fp32@GRAD@RENAME@block0@1', 'fc_40.tmp_1.cast_fp32@GRAD@RENAME@block0@2']}, op_device = , op_role = 1, use_mkldnn = False)
    {Out=['fc_40.tmp_1@GRAD']} = cast(inputs={X=['fc_40.tmp_1.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_role = 1, out_dtype = 4, use_mkldnn = False)
    {X@GRAD=['fc_40.tmp_0@GRAD'], Y@GRAD=['encoder_layer_6_ffn_fc_0.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_40.tmp_1@GRAD'], X=['fc_40.tmp_0'], Y=['encoder_layer_6_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_6_ffn_fc_0.b_0', 'encoder_layer_6_ffn_fc_0.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_13.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_6_ffn_fc_0.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_40.tmp_0@GRAD'], X=['layer_norm_13.tmp_2'], Y=['encoder_layer_6_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_6_ffn_fc_0.w_0', 'encoder_layer_6_ffn_fc_0.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_13.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_13.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_13.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_6_post_att_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_6_post_att_layer_norm_scale@GRAD'], X@GRAD=['tmp_57@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_6_post_att_layer_norm_bias'], Mean=['layer_norm_13.tmp_0'], Scale=['encoder_layer_6_post_att_layer_norm_scale'], Variance=['layer_norm_13.tmp_1'], X=['tmp_57'], Y@GRAD=['layer_norm_13.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_6_post_att_layer_norm_bias', 'encoder_layer_6_post_att_layer_norm_bias@GRAD', 'encoder_layer_6_post_att_layer_norm_scale', 'encoder_layer_6_post_att_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_39.tmp_1@GRAD'], Y@GRAD=['layer_norm_12.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_57@GRAD'], X=['fc_39.tmp_1'], Y=['layer_norm_12.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_39.tmp_0@GRAD'], Y@GRAD=['encoder_layer_6_multi_head_att_output_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_39.tmp_1@GRAD'], X=['fc_39.tmp_0'], Y=['encoder_layer_6_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_6_multi_head_att_output_fc.b_0', 'encoder_layer_6_multi_head_att_output_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_27.tmp_0@GRAD'], Y@GRAD=['encoder_layer_6_multi_head_att_output_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_39.tmp_0@GRAD'], X=['transpose_27.tmp_0'], Y=['encoder_layer_6_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_6_multi_head_att_output_fc.w_0', 'encoder_layer_6_multi_head_att_output_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['transpose_27.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['transpose_27.tmp_0@GRAD'], XShape=['reshape2_27.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_14.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_27.tmp_0@GRAD'], XShape=['transpose_27.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_6.tmp_0@GRAD'], Y@GRAD=['transpose_26.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_14.tmp_0@GRAD'], X=['softmax_6.tmp_0'], Y=['transpose_26.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {X@GRAD=['tmp_56@GRAD']} = softmax_grad(inputs={Out=['softmax_6.tmp_0'], Out@GRAD=['softmax_6.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_v2_13.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_56@GRAD'], X=['matmul_v2_13.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['scale_7.tmp_0@GRAD'], Y@GRAD=['transpose_25.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_13.tmp_0@GRAD'], X=['scale_7.tmp_0'], Y=['transpose_25.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['transpose_24.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_7.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.125)
    {X@GRAD=['fc_38.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_26.tmp_0@GRAD'], XShape=['transpose_26.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_38.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_38.tmp_1@GRAD'], XShape=['reshape2_26.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_37.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_25.tmp_0@GRAD'], XShape=['transpose_25.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_37.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_37.tmp_1@GRAD'], XShape=['reshape2_25.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_36.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_24.tmp_0@GRAD'], XShape=['transpose_24.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_36.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_36.tmp_1@GRAD'], XShape=['reshape2_24.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_38.tmp_0@GRAD'], Y@GRAD=['encoder_layer_6_multi_head_att_value_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_38.tmp_1@GRAD'], X=['fc_38.tmp_0'], Y=['encoder_layer_6_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_6_multi_head_att_value_fc.b_0', 'encoder_layer_6_multi_head_att_value_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_12.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_6_multi_head_att_value_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_38.tmp_0@GRAD'], X=['layer_norm_12.tmp_2'], Y=['encoder_layer_6_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_6_multi_head_att_value_fc.w_0', 'encoder_layer_6_multi_head_att_value_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_37.tmp_0@GRAD'], Y@GRAD=['encoder_layer_6_multi_head_att_key_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_37.tmp_1@GRAD'], X=['fc_37.tmp_0'], Y=['encoder_layer_6_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_6_multi_head_att_key_fc.b_0', 'encoder_layer_6_multi_head_att_key_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_12.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['encoder_layer_6_multi_head_att_key_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_37.tmp_0@GRAD'], X=['layer_norm_12.tmp_2'], Y=['encoder_layer_6_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_6_multi_head_att_key_fc.w_0', 'encoder_layer_6_multi_head_att_key_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_36.tmp_0@GRAD'], Y@GRAD=['encoder_layer_6_multi_head_att_query_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_36.tmp_1@GRAD'], X=['fc_36.tmp_0'], Y=['encoder_layer_6_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_6_multi_head_att_query_fc.b_0', 'encoder_layer_6_multi_head_att_query_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_12.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['encoder_layer_6_multi_head_att_query_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_36.tmp_0@GRAD'], X=['layer_norm_12.tmp_2'], Y=['encoder_layer_6_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_6_multi_head_att_query_fc.w_0', 'encoder_layer_6_multi_head_att_query_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_12.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_12.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_12.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_12.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_12.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Out=['layer_norm_12.tmp_2@GRAD']} = c_sync_calc_stream(inputs={X=['layer_norm_12.tmp_2@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [])
    send_v2(inputs={X=['layer_norm_12.tmp_2@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], peer = 1, ring_id = 21, srTag = 0, tag = tag, use_calc_stream = False)
    {Out=['layer_norm_12.tmp_2@GRAD']} = c_sync_comm_stream(inputs={X=['layer_norm_12.tmp_2@GRAD']}, op_device = , op_namescope = /, op_role = 2, op_role_var = [], ring_id = 21)
    {Out=['mask_lm_out_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [30000], str_value = , value = 0.0)
    {Out=['mask_lm_out_fc.b_0@GRAD@TMP']} = cast(inputs={X=['mask_lm_out_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['mask_lm_out_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['mask_lm_out_fc.b_0@GRAD@MERGED', 'mask_lm_out_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['mask_lm_out_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 30000], str_value = , value = 0.0)
    {Out=['mask_lm_out_fc.w_0@GRAD@TMP']} = cast(inputs={X=['mask_lm_out_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['mask_lm_out_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['mask_lm_out_fc.w_0@GRAD@MERGED', 'mask_lm_out_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['mask_lm_trans_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['mask_lm_trans_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['mask_lm_trans_layer_norm_scale@GRAD', 'mask_lm_trans_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['mask_lm_trans_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['mask_lm_trans_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['mask_lm_trans_layer_norm_bias@GRAD', 'mask_lm_trans_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['mask_lm_trans_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['mask_lm_trans_fc.b_0@GRAD@TMP']} = cast(inputs={X=['mask_lm_trans_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['mask_lm_trans_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['mask_lm_trans_fc.b_0@GRAD@MERGED', 'mask_lm_trans_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['mask_lm_trans_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['mask_lm_trans_fc.w_0@GRAD@TMP']} = cast(inputs={X=['mask_lm_trans_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['mask_lm_trans_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['mask_lm_trans_fc.w_0@GRAD@MERGED', 'mask_lm_trans_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_11_post_ffn_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_11_post_ffn_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_11_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_11_post_ffn_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_11_post_ffn_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_11_post_ffn_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_11_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_11_post_ffn_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_11_ffn_fc_1.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_11_ffn_fc_1.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_11_ffn_fc_1.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_11_ffn_fc_1.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_11_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_11_ffn_fc_1.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_11_ffn_fc_1.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_11_ffn_fc_1.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_11_ffn_fc_1.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_11_ffn_fc_1.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_11_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_11_ffn_fc_1.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_11_ffn_fc_0.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072], str_value = , value = 0.0)
    {Out=['encoder_layer_11_ffn_fc_0.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_11_ffn_fc_0.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_11_ffn_fc_0.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_11_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_11_ffn_fc_0.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_11_ffn_fc_0.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 3072], str_value = , value = 0.0)
    {Out=['encoder_layer_11_ffn_fc_0.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_11_ffn_fc_0.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_11_ffn_fc_0.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_11_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_11_ffn_fc_0.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_11_post_att_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_11_post_att_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_11_post_att_layer_norm_scale@GRAD', 'encoder_layer_11_post_att_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_11_post_att_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_11_post_att_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_11_post_att_layer_norm_bias@GRAD', 'encoder_layer_11_post_att_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_11_multi_head_att_output_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_11_multi_head_att_output_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_11_multi_head_att_output_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_11_multi_head_att_output_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_11_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_output_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_11_multi_head_att_output_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_11_multi_head_att_output_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_11_multi_head_att_output_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_11_multi_head_att_output_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_11_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_output_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_11_multi_head_att_value_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_11_multi_head_att_value_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_11_multi_head_att_value_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_11_multi_head_att_value_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_11_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_value_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_11_multi_head_att_value_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_11_multi_head_att_value_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_11_multi_head_att_value_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_11_multi_head_att_value_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_11_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_value_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_11_multi_head_att_key_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_11_multi_head_att_key_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_11_multi_head_att_key_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_11_multi_head_att_key_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_11_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_key_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_11_multi_head_att_key_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_11_multi_head_att_key_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_11_multi_head_att_key_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_11_multi_head_att_key_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_11_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_key_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_11_multi_head_att_query_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_11_multi_head_att_query_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_11_multi_head_att_query_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_11_multi_head_att_query_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_11_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_query_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_11_multi_head_att_query_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_11_multi_head_att_query_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_11_multi_head_att_query_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_11_multi_head_att_query_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_11_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_query_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_10_post_ffn_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_10_post_ffn_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_10_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_10_post_ffn_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_10_post_ffn_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_10_post_ffn_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_10_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_10_post_ffn_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_10_ffn_fc_1.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_10_ffn_fc_1.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_10_ffn_fc_1.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_10_ffn_fc_1.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_10_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_10_ffn_fc_1.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_10_ffn_fc_1.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_10_ffn_fc_1.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_10_ffn_fc_1.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_10_ffn_fc_1.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_10_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_10_ffn_fc_1.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_10_ffn_fc_0.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072], str_value = , value = 0.0)
    {Out=['encoder_layer_10_ffn_fc_0.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_10_ffn_fc_0.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_10_ffn_fc_0.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_10_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_10_ffn_fc_0.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_10_ffn_fc_0.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 3072], str_value = , value = 0.0)
    {Out=['encoder_layer_10_ffn_fc_0.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_10_ffn_fc_0.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_10_ffn_fc_0.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_10_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_10_ffn_fc_0.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_10_post_att_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_10_post_att_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_10_post_att_layer_norm_scale@GRAD', 'encoder_layer_10_post_att_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_10_post_att_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_10_post_att_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_10_post_att_layer_norm_bias@GRAD', 'encoder_layer_10_post_att_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_10_multi_head_att_output_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_10_multi_head_att_output_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_10_multi_head_att_output_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_10_multi_head_att_output_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_10_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_output_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_10_multi_head_att_output_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_10_multi_head_att_output_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_10_multi_head_att_output_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_10_multi_head_att_output_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_10_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_output_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_10_multi_head_att_value_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_10_multi_head_att_value_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_10_multi_head_att_value_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_10_multi_head_att_value_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_10_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_value_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_10_multi_head_att_value_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_10_multi_head_att_value_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_10_multi_head_att_value_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_10_multi_head_att_value_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_10_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_value_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_10_multi_head_att_key_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_10_multi_head_att_key_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_10_multi_head_att_key_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_10_multi_head_att_key_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_10_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_key_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_10_multi_head_att_key_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_10_multi_head_att_key_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_10_multi_head_att_key_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_10_multi_head_att_key_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_10_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_key_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_10_multi_head_att_query_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_10_multi_head_att_query_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_10_multi_head_att_query_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_10_multi_head_att_query_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_10_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_query_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_10_multi_head_att_query_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_10_multi_head_att_query_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_10_multi_head_att_query_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_10_multi_head_att_query_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_10_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_query_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_9_post_ffn_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_9_post_ffn_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_9_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_9_post_ffn_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_9_post_ffn_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_9_post_ffn_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_9_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_9_post_ffn_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_9_ffn_fc_1.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_9_ffn_fc_1.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_9_ffn_fc_1.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_9_ffn_fc_1.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_9_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_9_ffn_fc_1.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_9_ffn_fc_1.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_9_ffn_fc_1.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_9_ffn_fc_1.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_9_ffn_fc_1.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_9_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_9_ffn_fc_1.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_9_ffn_fc_0.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072], str_value = , value = 0.0)
    {Out=['encoder_layer_9_ffn_fc_0.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_9_ffn_fc_0.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_9_ffn_fc_0.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_9_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_9_ffn_fc_0.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_9_ffn_fc_0.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 3072], str_value = , value = 0.0)
    {Out=['encoder_layer_9_ffn_fc_0.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_9_ffn_fc_0.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_9_ffn_fc_0.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_9_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_9_ffn_fc_0.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_9_post_att_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_9_post_att_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_9_post_att_layer_norm_scale@GRAD', 'encoder_layer_9_post_att_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_9_post_att_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_9_post_att_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_9_post_att_layer_norm_bias@GRAD', 'encoder_layer_9_post_att_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_9_multi_head_att_output_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_9_multi_head_att_output_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_9_multi_head_att_output_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_9_multi_head_att_output_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_9_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_output_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_9_multi_head_att_output_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_9_multi_head_att_output_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_9_multi_head_att_output_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_9_multi_head_att_output_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_9_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_output_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_9_multi_head_att_value_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_9_multi_head_att_value_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_9_multi_head_att_value_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_9_multi_head_att_value_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_9_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_value_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_9_multi_head_att_value_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_9_multi_head_att_value_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_9_multi_head_att_value_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_9_multi_head_att_value_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_9_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_value_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_9_multi_head_att_key_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_9_multi_head_att_key_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_9_multi_head_att_key_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_9_multi_head_att_key_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_9_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_key_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_9_multi_head_att_key_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_9_multi_head_att_key_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_9_multi_head_att_key_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_9_multi_head_att_key_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_9_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_key_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_9_multi_head_att_query_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_9_multi_head_att_query_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_9_multi_head_att_query_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_9_multi_head_att_query_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_9_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_query_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_9_multi_head_att_query_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_9_multi_head_att_query_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_9_multi_head_att_query_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_9_multi_head_att_query_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_9_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_query_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_8_post_ffn_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_8_post_ffn_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_8_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_8_post_ffn_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_8_post_ffn_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_8_post_ffn_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_8_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_8_post_ffn_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_8_ffn_fc_1.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_8_ffn_fc_1.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_8_ffn_fc_1.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_8_ffn_fc_1.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_8_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_8_ffn_fc_1.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_8_ffn_fc_1.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_8_ffn_fc_1.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_8_ffn_fc_1.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_8_ffn_fc_1.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_8_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_8_ffn_fc_1.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_8_ffn_fc_0.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072], str_value = , value = 0.0)
    {Out=['encoder_layer_8_ffn_fc_0.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_8_ffn_fc_0.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_8_ffn_fc_0.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_8_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_8_ffn_fc_0.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_8_ffn_fc_0.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 3072], str_value = , value = 0.0)
    {Out=['encoder_layer_8_ffn_fc_0.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_8_ffn_fc_0.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_8_ffn_fc_0.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_8_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_8_ffn_fc_0.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_8_post_att_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_8_post_att_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_8_post_att_layer_norm_scale@GRAD', 'encoder_layer_8_post_att_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_8_post_att_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_8_post_att_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_8_post_att_layer_norm_bias@GRAD', 'encoder_layer_8_post_att_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_8_multi_head_att_output_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_8_multi_head_att_output_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_8_multi_head_att_output_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_8_multi_head_att_output_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_8_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_output_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_8_multi_head_att_output_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_8_multi_head_att_output_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_8_multi_head_att_output_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_8_multi_head_att_output_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_8_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_output_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_8_multi_head_att_value_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_8_multi_head_att_value_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_8_multi_head_att_value_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_8_multi_head_att_value_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_8_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_value_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_8_multi_head_att_value_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_8_multi_head_att_value_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_8_multi_head_att_value_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_8_multi_head_att_value_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_8_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_value_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_8_multi_head_att_key_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_8_multi_head_att_key_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_8_multi_head_att_key_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_8_multi_head_att_key_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_8_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_key_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_8_multi_head_att_key_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_8_multi_head_att_key_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_8_multi_head_att_key_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_8_multi_head_att_key_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_8_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_key_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_8_multi_head_att_query_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_8_multi_head_att_query_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_8_multi_head_att_query_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_8_multi_head_att_query_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_8_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_query_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_8_multi_head_att_query_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_8_multi_head_att_query_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_8_multi_head_att_query_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_8_multi_head_att_query_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_8_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_query_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_7_post_ffn_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_7_post_ffn_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_7_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_7_post_ffn_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_7_post_ffn_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_7_post_ffn_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_7_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_7_post_ffn_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_7_ffn_fc_1.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_7_ffn_fc_1.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_7_ffn_fc_1.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_7_ffn_fc_1.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_7_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_7_ffn_fc_1.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_7_ffn_fc_1.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_7_ffn_fc_1.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_7_ffn_fc_1.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_7_ffn_fc_1.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_7_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_7_ffn_fc_1.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_7_ffn_fc_0.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072], str_value = , value = 0.0)
    {Out=['encoder_layer_7_ffn_fc_0.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_7_ffn_fc_0.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_7_ffn_fc_0.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_7_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_7_ffn_fc_0.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_7_ffn_fc_0.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 3072], str_value = , value = 0.0)
    {Out=['encoder_layer_7_ffn_fc_0.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_7_ffn_fc_0.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_7_ffn_fc_0.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_7_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_7_ffn_fc_0.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_7_post_att_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_7_post_att_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_7_post_att_layer_norm_scale@GRAD', 'encoder_layer_7_post_att_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_7_post_att_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_7_post_att_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_7_post_att_layer_norm_bias@GRAD', 'encoder_layer_7_post_att_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_7_multi_head_att_output_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_7_multi_head_att_output_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_7_multi_head_att_output_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_7_multi_head_att_output_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_7_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_output_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_7_multi_head_att_output_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_7_multi_head_att_output_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_7_multi_head_att_output_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_7_multi_head_att_output_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_7_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_output_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_7_multi_head_att_value_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_7_multi_head_att_value_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_7_multi_head_att_value_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_7_multi_head_att_value_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_7_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_value_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_7_multi_head_att_value_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_7_multi_head_att_value_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_7_multi_head_att_value_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_7_multi_head_att_value_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_7_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_value_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_7_multi_head_att_key_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_7_multi_head_att_key_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_7_multi_head_att_key_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_7_multi_head_att_key_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_7_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_key_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_7_multi_head_att_key_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_7_multi_head_att_key_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_7_multi_head_att_key_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_7_multi_head_att_key_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_7_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_key_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_7_multi_head_att_query_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_7_multi_head_att_query_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_7_multi_head_att_query_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_7_multi_head_att_query_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_7_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_query_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_7_multi_head_att_query_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_7_multi_head_att_query_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_7_multi_head_att_query_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_7_multi_head_att_query_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_7_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_query_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_6_post_ffn_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_6_post_ffn_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_6_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_6_post_ffn_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_6_post_ffn_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_6_post_ffn_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_6_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_6_post_ffn_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_6_ffn_fc_1.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_6_ffn_fc_1.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_6_ffn_fc_1.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_6_ffn_fc_1.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_6_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_6_ffn_fc_1.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_6_ffn_fc_1.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_6_ffn_fc_1.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_6_ffn_fc_1.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_6_ffn_fc_1.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_6_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_6_ffn_fc_1.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_6_ffn_fc_0.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072], str_value = , value = 0.0)
    {Out=['encoder_layer_6_ffn_fc_0.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_6_ffn_fc_0.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_6_ffn_fc_0.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_6_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_6_ffn_fc_0.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_6_ffn_fc_0.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 3072], str_value = , value = 0.0)
    {Out=['encoder_layer_6_ffn_fc_0.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_6_ffn_fc_0.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_6_ffn_fc_0.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_6_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_6_ffn_fc_0.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_6_post_att_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_6_post_att_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_6_post_att_layer_norm_scale@GRAD', 'encoder_layer_6_post_att_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_6_post_att_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_6_post_att_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_6_post_att_layer_norm_bias@GRAD', 'encoder_layer_6_post_att_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_6_multi_head_att_output_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_6_multi_head_att_output_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_6_multi_head_att_output_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_6_multi_head_att_output_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_6_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_output_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_6_multi_head_att_output_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_6_multi_head_att_output_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_6_multi_head_att_output_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_6_multi_head_att_output_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_6_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_output_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_6_multi_head_att_value_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_6_multi_head_att_value_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_6_multi_head_att_value_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_6_multi_head_att_value_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_6_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_value_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_6_multi_head_att_value_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_6_multi_head_att_value_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_6_multi_head_att_value_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_6_multi_head_att_value_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_6_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_value_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_6_multi_head_att_key_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_6_multi_head_att_key_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_6_multi_head_att_key_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_6_multi_head_att_key_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_6_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_key_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_6_multi_head_att_key_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_6_multi_head_att_key_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_6_multi_head_att_key_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_6_multi_head_att_key_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_6_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_key_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_6_multi_head_att_query_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_6_multi_head_att_query_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_6_multi_head_att_query_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_6_multi_head_att_query_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_6_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_query_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_6_multi_head_att_query_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_6_multi_head_att_query_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_6_multi_head_att_query_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_6_multi_head_att_query_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_6_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_query_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {FoundInfinite=['find_infinite_scale.tmp_0'], Out=['encoder_layer_6_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_6_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_6_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_6_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_6_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_6_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_6_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_6_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_6_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_7_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_7_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_7_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_7_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_7_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_7_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_7_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_7_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_7_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_8_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_8_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_8_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_8_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_8_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_8_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_8_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_8_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_8_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_9_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_9_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_9_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_9_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_9_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_9_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_9_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_9_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_9_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_10_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_10_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_10_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_10_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_10_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_10_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_10_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_10_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_10_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_11_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_11_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_11_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_11_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_11_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_11_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_11_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_11_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_11_post_ffn_layer_norm_bias@GRAD@MERGED', 'mask_lm_trans_fc.w_0@GRAD@MERGED', 'mask_lm_trans_fc.b_0@GRAD@MERGED', 'mask_lm_trans_layer_norm_scale@GRAD@MERGED', 'mask_lm_trans_layer_norm_bias@GRAD@MERGED', 'mask_lm_out_fc.w_0@GRAD@MERGED', 'mask_lm_out_fc.b_0@GRAD@MERGED']} = check_finite_and_unscale(inputs={FloatStatus=['float_status'], Scale=['loss_scaling_0'], X=['encoder_layer_6_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_6_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_6_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_6_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_6_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_6_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_6_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_6_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_6_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_7_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_7_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_7_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_7_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_7_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_7_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_7_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_7_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_7_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_8_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_8_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_8_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_8_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_8_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_8_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_8_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_8_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_8_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_9_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_9_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_9_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_9_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_9_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_9_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_9_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_9_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_9_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_10_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_10_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_10_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_10_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_10_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_10_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_10_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_10_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_10_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_11_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_11_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_11_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_11_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_11_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_11_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_11_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_11_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_11_post_ffn_layer_norm_bias@GRAD@MERGED', 'mask_lm_trans_fc.w_0@GRAD@MERGED', 'mask_lm_trans_fc.b_0@GRAD@MERGED', 'mask_lm_trans_layer_norm_scale@GRAD@MERGED', 'mask_lm_trans_layer_norm_bias@GRAD@MERGED', 'mask_lm_out_fc.w_0@GRAD@MERGED', 'mask_lm_out_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /, op_role = 2, op_role_var = ['word_embedding@GRAD', 'pos_embedding@GRAD', 'sent_embedding@GRAD', 'pre_encoder_layer_norm_scale@GRAD', 'pre_encoder_layer_norm_bias@GRAD', 'encoder_layer_0_multi_head_att_query_fc.w_0@GRAD', 'encoder_layer_0_multi_head_att_query_fc.b_0@GRAD', 'encoder_layer_0_multi_head_att_key_fc.w_0@GRAD', 'encoder_layer_0_multi_head_att_key_fc.b_0@GRAD', 'encoder_layer_0_multi_head_att_value_fc.w_0@GRAD', 'encoder_layer_0_multi_head_att_value_fc.b_0@GRAD', 'encoder_layer_0_multi_head_att_output_fc.w_0@GRAD', 'encoder_layer_0_multi_head_att_output_fc.b_0@GRAD', 'encoder_layer_0_post_att_layer_norm_scale@GRAD', 'encoder_layer_0_post_att_layer_norm_bias@GRAD', 'encoder_layer_0_ffn_fc_0.w_0@GRAD', 'encoder_layer_0_ffn_fc_0.b_0@GRAD', 'encoder_layer_0_ffn_fc_1.w_0@GRAD', 'encoder_layer_0_ffn_fc_1.b_0@GRAD', 'encoder_layer_0_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_0_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_1_multi_head_att_query_fc.w_0@GRAD', 'encoder_layer_1_multi_head_att_query_fc.b_0@GRAD', 'encoder_layer_1_multi_head_att_key_fc.w_0@GRAD', 'encoder_layer_1_multi_head_att_key_fc.b_0@GRAD', 'encoder_layer_1_multi_head_att_value_fc.w_0@GRAD', 'encoder_layer_1_multi_head_att_value_fc.b_0@GRAD', 'encoder_layer_1_multi_head_att_output_fc.w_0@GRAD', 'encoder_layer_1_multi_head_att_output_fc.b_0@GRAD', 'encoder_layer_1_post_att_layer_norm_scale@GRAD', 'encoder_layer_1_post_att_layer_norm_bias@GRAD', 'encoder_layer_1_ffn_fc_0.w_0@GRAD', 'encoder_layer_1_ffn_fc_0.b_0@GRAD', 'encoder_layer_1_ffn_fc_1.w_0@GRAD', 'encoder_layer_1_ffn_fc_1.b_0@GRAD', 'encoder_layer_1_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_1_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_2_multi_head_att_query_fc.w_0@GRAD', 'encoder_layer_2_multi_head_att_query_fc.b_0@GRAD', 'encoder_layer_2_multi_head_att_key_fc.w_0@GRAD', 'encoder_layer_2_multi_head_att_key_fc.b_0@GRAD', 'encoder_layer_2_multi_head_att_value_fc.w_0@GRAD', 'encoder_layer_2_multi_head_att_value_fc.b_0@GRAD', 'encoder_layer_2_multi_head_att_output_fc.w_0@GRAD', 'encoder_layer_2_multi_head_att_output_fc.b_0@GRAD', 'encoder_layer_2_post_att_layer_norm_scale@GRAD', 'encoder_layer_2_post_att_layer_norm_bias@GRAD', 'encoder_layer_2_ffn_fc_0.w_0@GRAD', 'encoder_layer_2_ffn_fc_0.b_0@GRAD', 'encoder_layer_2_ffn_fc_1.w_0@GRAD', 'encoder_layer_2_ffn_fc_1.b_0@GRAD', 'encoder_layer_2_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_2_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_3_multi_head_att_query_fc.w_0@GRAD', 'encoder_layer_3_multi_head_att_query_fc.b_0@GRAD', 'encoder_layer_3_multi_head_att_key_fc.w_0@GRAD', 'encoder_layer_3_multi_head_att_key_fc.b_0@GRAD', 'encoder_layer_3_multi_head_att_value_fc.w_0@GRAD', 'encoder_layer_3_multi_head_att_value_fc.b_0@GRAD', 'encoder_layer_3_multi_head_att_output_fc.w_0@GRAD', 'encoder_layer_3_multi_head_att_output_fc.b_0@GRAD', 'encoder_layer_3_post_att_layer_norm_scale@GRAD', 'encoder_layer_3_post_att_layer_norm_bias@GRAD', 'encoder_layer_3_ffn_fc_0.w_0@GRAD', 'encoder_layer_3_ffn_fc_0.b_0@GRAD', 'encoder_layer_3_ffn_fc_1.w_0@GRAD', 'encoder_layer_3_ffn_fc_1.b_0@GRAD', 'encoder_layer_3_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_3_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_4_multi_head_att_query_fc.w_0@GRAD', 'encoder_layer_4_multi_head_att_query_fc.b_0@GRAD', 'encoder_layer_4_multi_head_att_key_fc.w_0@GRAD', 'encoder_layer_4_multi_head_att_key_fc.b_0@GRAD', 'encoder_layer_4_multi_head_att_value_fc.w_0@GRAD', 'encoder_layer_4_multi_head_att_value_fc.b_0@GRAD', 'encoder_layer_4_multi_head_att_output_fc.w_0@GRAD', 'encoder_layer_4_multi_head_att_output_fc.b_0@GRAD', 'encoder_layer_4_post_att_layer_norm_scale@GRAD', 'encoder_layer_4_post_att_layer_norm_bias@GRAD', 'encoder_layer_4_ffn_fc_0.w_0@GRAD', 'encoder_layer_4_ffn_fc_0.b_0@GRAD', 'encoder_layer_4_ffn_fc_1.w_0@GRAD', 'encoder_layer_4_ffn_fc_1.b_0@GRAD', 'encoder_layer_4_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_4_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_5_multi_head_att_query_fc.w_0@GRAD', 'encoder_layer_5_multi_head_att_query_fc.b_0@GRAD', 'encoder_layer_5_multi_head_att_key_fc.w_0@GRAD', 'encoder_layer_5_multi_head_att_key_fc.b_0@GRAD', 'encoder_layer_5_multi_head_att_value_fc.w_0@GRAD', 'encoder_layer_5_multi_head_att_value_fc.b_0@GRAD', 'encoder_layer_5_multi_head_att_output_fc.w_0@GRAD', 'encoder_layer_5_multi_head_att_output_fc.b_0@GRAD', 'encoder_layer_5_post_att_layer_norm_scale@GRAD', 'encoder_layer_5_post_att_layer_norm_bias@GRAD', 'encoder_layer_5_ffn_fc_0.w_0@GRAD', 'encoder_layer_5_ffn_fc_0.b_0@GRAD', 'encoder_layer_5_ffn_fc_1.w_0@GRAD', 'encoder_layer_5_ffn_fc_1.b_0@GRAD', 'encoder_layer_5_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_5_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_6_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_6_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_6_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_6_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_6_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_6_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_6_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_6_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_6_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_7_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_7_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_7_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_7_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_7_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_7_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_7_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_7_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_7_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_8_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_8_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_8_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_8_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_8_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_8_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_8_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_8_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_8_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_9_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_9_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_9_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_9_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_9_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_9_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_9_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_9_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_9_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_10_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_10_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_10_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_10_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_10_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_10_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_10_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_10_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_10_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_11_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_11_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_11_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_11_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_11_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_11_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_11_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_11_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_11_post_ffn_layer_norm_bias@GRAD@MERGED', 'mask_lm_trans_fc.w_0@GRAD@MERGED', 'mask_lm_trans_fc.b_0@GRAD@MERGED', 'mask_lm_trans_layer_norm_scale@GRAD@MERGED', 'mask_lm_trans_layer_norm_bias@GRAD@MERGED', 'mask_lm_out_fc.w_0@GRAD@MERGED', 'mask_lm_out_fc.b_0@GRAD@MERGED'])
    {Out=['find_infinite_scale.tmp_0@cast_int32']} = cast(inputs={X=['find_infinite_scale.tmp_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 2, op_role_var = [], out_dtype = 2, use_mkldnn = False)
    {Out=['find_infinite_scale.tmp_0@cast_int32']} = c_allreduce_max(inputs={X=['find_infinite_scale.tmp_0@cast_int32']}, op_device = , op_namescope = /, op_role = 2, op_role_var = [], ring_id = 3, tag = tag, use_calc_stream = True, use_model_parallel = False)
    {Out=['find_infinite_scale.tmp_0@GLOBAL_WORLD']} = cast(inputs={X=['find_infinite_scale.tmp_0@cast_int32']}, in_dtype = 2, op_device = , op_namescope = /, op_role = 2, op_role_var = [], out_dtype = 0, use_mkldnn = False)
    {LossScaling=['loss_scaling_0'], Out=['encoder_layer_6_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_6_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_6_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_6_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_6_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_6_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_6_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_6_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_6_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_7_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_7_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_7_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_7_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_7_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_7_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_7_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_7_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_7_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_8_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_8_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_8_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_8_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_8_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_8_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_8_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_8_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_8_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_9_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_9_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_9_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_9_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_9_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_9_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_9_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_9_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_9_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_10_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_10_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_10_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_10_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_10_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_10_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_10_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_10_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_10_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_11_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_11_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_11_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_11_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_11_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_11_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_11_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_11_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_11_post_ffn_layer_norm_bias@GRAD@MERGED', 'mask_lm_trans_fc.w_0@GRAD@MERGED', 'mask_lm_trans_fc.b_0@GRAD@MERGED', 'mask_lm_trans_layer_norm_scale@GRAD@MERGED', 'mask_lm_trans_layer_norm_bias@GRAD@MERGED', 'mask_lm_out_fc.w_0@GRAD@MERGED', 'mask_lm_out_fc.b_0@GRAD@MERGED'], OutBadSteps=['num_bad_steps_0'], OutGoodSteps=['num_good_steps_0']} = update_loss_scaling(inputs={FoundInfinite=['find_infinite_scale.tmp_0@GLOBAL_WORLD'], InBadSteps=['num_bad_steps_0'], InGoodSteps=['num_good_steps_0'], PrevLossScaling=['loss_scaling_0'], X=['encoder_layer_6_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_6_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_6_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_6_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_6_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_6_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_6_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_6_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_6_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_6_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_7_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_7_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_7_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_7_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_7_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_7_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_7_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_7_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_7_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_7_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_8_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_8_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_8_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_8_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_8_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_8_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_8_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_8_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_8_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_8_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_9_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_9_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_9_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_9_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_9_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_9_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_9_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_9_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_9_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_9_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_10_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_10_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_10_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_10_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_10_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_10_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_10_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_10_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_10_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_10_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_11_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_11_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_11_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_11_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_11_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_11_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_11_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_11_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_11_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_11_post_ffn_layer_norm_bias@GRAD@MERGED', 'mask_lm_trans_fc.w_0@GRAD@MERGED', 'mask_lm_trans_fc.b_0@GRAD@MERGED', 'mask_lm_trans_layer_norm_scale@GRAD@MERGED', 'mask_lm_trans_layer_norm_bias@GRAD@MERGED', 'mask_lm_out_fc.w_0@GRAD@MERGED', 'mask_lm_out_fc.b_0@GRAD@MERGED']}, decr_every_n_nan_or_inf = 2, decr_ratio = 0.5, incr_every_n_steps = 1000, incr_ratio = 2.0, op_device = , op_namescope = /, op_role = 2, op_role_var = [], stop_update = False)
    {Out=['square_16.tmp_0']} = square(inputs={X=['encoder_layer_10_ffn_fc_0.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_ffn_fc_0.b_0', 'elementwise_mul_16.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_16.tmp_0']} = reduce_sum(inputs={X=['square_16.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_ffn_fc_0.b_0', 'elementwise_mul_16.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_17.tmp_0']} = square(inputs={X=['encoder_layer_10_ffn_fc_0.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_ffn_fc_0.w_0', 'elementwise_mul_17.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_17.tmp_0']} = reduce_sum(inputs={X=['square_17.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_ffn_fc_0.w_0', 'elementwise_mul_17.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_18.tmp_0']} = square(inputs={X=['encoder_layer_10_ffn_fc_1.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_ffn_fc_1.b_0', 'elementwise_mul_18.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_18.tmp_0']} = reduce_sum(inputs={X=['square_18.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_ffn_fc_1.b_0', 'elementwise_mul_18.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_19.tmp_0']} = square(inputs={X=['encoder_layer_10_ffn_fc_1.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_ffn_fc_1.w_0', 'elementwise_mul_19.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_19.tmp_0']} = reduce_sum(inputs={X=['square_19.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_ffn_fc_1.w_0', 'elementwise_mul_19.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_20.tmp_0']} = square(inputs={X=['encoder_layer_10_multi_head_att_key_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_key_fc.b_0', 'elementwise_mul_20.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_20.tmp_0']} = reduce_sum(inputs={X=['square_20.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_key_fc.b_0', 'elementwise_mul_20.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_21.tmp_0']} = square(inputs={X=['encoder_layer_10_multi_head_att_key_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_key_fc.w_0', 'elementwise_mul_21.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_21.tmp_0']} = reduce_sum(inputs={X=['square_21.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_key_fc.w_0', 'elementwise_mul_21.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_22.tmp_0']} = square(inputs={X=['encoder_layer_10_multi_head_att_output_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_output_fc.b_0', 'elementwise_mul_22.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_22.tmp_0']} = reduce_sum(inputs={X=['square_22.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_output_fc.b_0', 'elementwise_mul_22.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_23.tmp_0']} = square(inputs={X=['encoder_layer_10_multi_head_att_output_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_output_fc.w_0', 'elementwise_mul_23.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_23.tmp_0']} = reduce_sum(inputs={X=['square_23.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_output_fc.w_0', 'elementwise_mul_23.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_24.tmp_0']} = square(inputs={X=['encoder_layer_10_multi_head_att_query_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_query_fc.b_0', 'elementwise_mul_24.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_24.tmp_0']} = reduce_sum(inputs={X=['square_24.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_query_fc.b_0', 'elementwise_mul_24.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_25.tmp_0']} = square(inputs={X=['encoder_layer_10_multi_head_att_query_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_query_fc.w_0', 'elementwise_mul_25.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_25.tmp_0']} = reduce_sum(inputs={X=['square_25.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_query_fc.w_0', 'elementwise_mul_25.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_26.tmp_0']} = square(inputs={X=['encoder_layer_10_multi_head_att_value_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_value_fc.b_0', 'elementwise_mul_26.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_26.tmp_0']} = reduce_sum(inputs={X=['square_26.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_value_fc.b_0', 'elementwise_mul_26.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_27.tmp_0']} = square(inputs={X=['encoder_layer_10_multi_head_att_value_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_value_fc.w_0', 'elementwise_mul_27.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_27.tmp_0']} = reduce_sum(inputs={X=['square_27.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_value_fc.w_0', 'elementwise_mul_27.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_28.tmp_0']} = square(inputs={X=['encoder_layer_10_post_att_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_post_att_layer_norm_bias', 'elementwise_mul_28.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_28.tmp_0']} = reduce_sum(inputs={X=['square_28.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_post_att_layer_norm_bias', 'elementwise_mul_28.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_29.tmp_0']} = square(inputs={X=['encoder_layer_10_post_att_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_post_att_layer_norm_scale', 'elementwise_mul_29.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_29.tmp_0']} = reduce_sum(inputs={X=['square_29.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_post_att_layer_norm_scale', 'elementwise_mul_29.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_30.tmp_0']} = square(inputs={X=['encoder_layer_10_post_ffn_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_post_ffn_layer_norm_bias', 'elementwise_mul_30.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_30.tmp_0']} = reduce_sum(inputs={X=['square_30.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_post_ffn_layer_norm_bias', 'elementwise_mul_30.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_31.tmp_0']} = square(inputs={X=['encoder_layer_10_post_ffn_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_post_ffn_layer_norm_scale', 'elementwise_mul_31.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_31.tmp_0']} = reduce_sum(inputs={X=['square_31.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_post_ffn_layer_norm_scale', 'elementwise_mul_31.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_32.tmp_0']} = square(inputs={X=['encoder_layer_11_ffn_fc_0.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_ffn_fc_0.b_0', 'elementwise_mul_32.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_32.tmp_0']} = reduce_sum(inputs={X=['square_32.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_ffn_fc_0.b_0', 'elementwise_mul_32.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_33.tmp_0']} = square(inputs={X=['encoder_layer_11_ffn_fc_0.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_ffn_fc_0.w_0', 'elementwise_mul_33.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_33.tmp_0']} = reduce_sum(inputs={X=['square_33.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_ffn_fc_0.w_0', 'elementwise_mul_33.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_34.tmp_0']} = square(inputs={X=['encoder_layer_11_ffn_fc_1.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_ffn_fc_1.b_0', 'elementwise_mul_34.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_34.tmp_0']} = reduce_sum(inputs={X=['square_34.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_ffn_fc_1.b_0', 'elementwise_mul_34.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_35.tmp_0']} = square(inputs={X=['encoder_layer_11_ffn_fc_1.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_ffn_fc_1.w_0', 'elementwise_mul_35.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_35.tmp_0']} = reduce_sum(inputs={X=['square_35.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_ffn_fc_1.w_0', 'elementwise_mul_35.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_36.tmp_0']} = square(inputs={X=['encoder_layer_11_multi_head_att_key_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_key_fc.b_0', 'elementwise_mul_36.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_36.tmp_0']} = reduce_sum(inputs={X=['square_36.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_key_fc.b_0', 'elementwise_mul_36.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_37.tmp_0']} = square(inputs={X=['encoder_layer_11_multi_head_att_key_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_key_fc.w_0', 'elementwise_mul_37.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_37.tmp_0']} = reduce_sum(inputs={X=['square_37.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_key_fc.w_0', 'elementwise_mul_37.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_38.tmp_0']} = square(inputs={X=['encoder_layer_11_multi_head_att_output_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_output_fc.b_0', 'elementwise_mul_38.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_38.tmp_0']} = reduce_sum(inputs={X=['square_38.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_output_fc.b_0', 'elementwise_mul_38.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_39.tmp_0']} = square(inputs={X=['encoder_layer_11_multi_head_att_output_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_output_fc.w_0', 'elementwise_mul_39.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_39.tmp_0']} = reduce_sum(inputs={X=['square_39.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_output_fc.w_0', 'elementwise_mul_39.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_40.tmp_0']} = square(inputs={X=['encoder_layer_11_multi_head_att_query_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_query_fc.b_0', 'elementwise_mul_40.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_40.tmp_0']} = reduce_sum(inputs={X=['square_40.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_query_fc.b_0', 'elementwise_mul_40.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_41.tmp_0']} = square(inputs={X=['encoder_layer_11_multi_head_att_query_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_query_fc.w_0', 'elementwise_mul_41.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_41.tmp_0']} = reduce_sum(inputs={X=['square_41.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_query_fc.w_0', 'elementwise_mul_41.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_42.tmp_0']} = square(inputs={X=['encoder_layer_11_multi_head_att_value_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_value_fc.b_0', 'elementwise_mul_42.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_42.tmp_0']} = reduce_sum(inputs={X=['square_42.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_value_fc.b_0', 'elementwise_mul_42.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_43.tmp_0']} = square(inputs={X=['encoder_layer_11_multi_head_att_value_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_value_fc.w_0', 'elementwise_mul_43.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_43.tmp_0']} = reduce_sum(inputs={X=['square_43.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_value_fc.w_0', 'elementwise_mul_43.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_44.tmp_0']} = square(inputs={X=['encoder_layer_11_post_att_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_post_att_layer_norm_bias', 'elementwise_mul_44.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_44.tmp_0']} = reduce_sum(inputs={X=['square_44.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_post_att_layer_norm_bias', 'elementwise_mul_44.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_45.tmp_0']} = square(inputs={X=['encoder_layer_11_post_att_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_post_att_layer_norm_scale', 'elementwise_mul_45.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_45.tmp_0']} = reduce_sum(inputs={X=['square_45.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_post_att_layer_norm_scale', 'elementwise_mul_45.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_46.tmp_0']} = square(inputs={X=['encoder_layer_11_post_ffn_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_post_ffn_layer_norm_bias', 'elementwise_mul_46.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_46.tmp_0']} = reduce_sum(inputs={X=['square_46.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_post_ffn_layer_norm_bias', 'elementwise_mul_46.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_47.tmp_0']} = square(inputs={X=['encoder_layer_11_post_ffn_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_post_ffn_layer_norm_scale', 'elementwise_mul_47.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_47.tmp_0']} = reduce_sum(inputs={X=['square_47.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_post_ffn_layer_norm_scale', 'elementwise_mul_47.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_128.tmp_0']} = square(inputs={X=['encoder_layer_6_ffn_fc_0.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_ffn_fc_0.b_0', 'elementwise_mul_128.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_128.tmp_0']} = reduce_sum(inputs={X=['square_128.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_ffn_fc_0.b_0', 'elementwise_mul_128.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_129.tmp_0']} = square(inputs={X=['encoder_layer_6_ffn_fc_0.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_ffn_fc_0.w_0', 'elementwise_mul_129.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_129.tmp_0']} = reduce_sum(inputs={X=['square_129.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_ffn_fc_0.w_0', 'elementwise_mul_129.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_130.tmp_0']} = square(inputs={X=['encoder_layer_6_ffn_fc_1.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_ffn_fc_1.b_0', 'elementwise_mul_130.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_130.tmp_0']} = reduce_sum(inputs={X=['square_130.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_ffn_fc_1.b_0', 'elementwise_mul_130.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_131.tmp_0']} = square(inputs={X=['encoder_layer_6_ffn_fc_1.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_ffn_fc_1.w_0', 'elementwise_mul_131.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_131.tmp_0']} = reduce_sum(inputs={X=['square_131.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_ffn_fc_1.w_0', 'elementwise_mul_131.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_132.tmp_0']} = square(inputs={X=['encoder_layer_6_multi_head_att_key_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_key_fc.b_0', 'elementwise_mul_132.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_132.tmp_0']} = reduce_sum(inputs={X=['square_132.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_key_fc.b_0', 'elementwise_mul_132.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_133.tmp_0']} = square(inputs={X=['encoder_layer_6_multi_head_att_key_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_key_fc.w_0', 'elementwise_mul_133.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_133.tmp_0']} = reduce_sum(inputs={X=['square_133.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_key_fc.w_0', 'elementwise_mul_133.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_134.tmp_0']} = square(inputs={X=['encoder_layer_6_multi_head_att_output_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_output_fc.b_0', 'elementwise_mul_134.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_134.tmp_0']} = reduce_sum(inputs={X=['square_134.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_output_fc.b_0', 'elementwise_mul_134.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_135.tmp_0']} = square(inputs={X=['encoder_layer_6_multi_head_att_output_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_output_fc.w_0', 'elementwise_mul_135.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_135.tmp_0']} = reduce_sum(inputs={X=['square_135.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_output_fc.w_0', 'elementwise_mul_135.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_136.tmp_0']} = square(inputs={X=['encoder_layer_6_multi_head_att_query_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_query_fc.b_0', 'elementwise_mul_136.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_136.tmp_0']} = reduce_sum(inputs={X=['square_136.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_query_fc.b_0', 'elementwise_mul_136.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_137.tmp_0']} = square(inputs={X=['encoder_layer_6_multi_head_att_query_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_query_fc.w_0', 'elementwise_mul_137.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_137.tmp_0']} = reduce_sum(inputs={X=['square_137.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_query_fc.w_0', 'elementwise_mul_137.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_138.tmp_0']} = square(inputs={X=['encoder_layer_6_multi_head_att_value_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_value_fc.b_0', 'elementwise_mul_138.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_138.tmp_0']} = reduce_sum(inputs={X=['square_138.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_value_fc.b_0', 'elementwise_mul_138.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_139.tmp_0']} = square(inputs={X=['encoder_layer_6_multi_head_att_value_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_value_fc.w_0', 'elementwise_mul_139.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_139.tmp_0']} = reduce_sum(inputs={X=['square_139.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_value_fc.w_0', 'elementwise_mul_139.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_140.tmp_0']} = square(inputs={X=['encoder_layer_6_post_att_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_post_att_layer_norm_bias', 'elementwise_mul_140.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_140.tmp_0']} = reduce_sum(inputs={X=['square_140.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_post_att_layer_norm_bias', 'elementwise_mul_140.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_141.tmp_0']} = square(inputs={X=['encoder_layer_6_post_att_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_post_att_layer_norm_scale', 'elementwise_mul_141.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_141.tmp_0']} = reduce_sum(inputs={X=['square_141.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_post_att_layer_norm_scale', 'elementwise_mul_141.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_142.tmp_0']} = square(inputs={X=['encoder_layer_6_post_ffn_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_post_ffn_layer_norm_bias', 'elementwise_mul_142.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_142.tmp_0']} = reduce_sum(inputs={X=['square_142.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_post_ffn_layer_norm_bias', 'elementwise_mul_142.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_143.tmp_0']} = square(inputs={X=['encoder_layer_6_post_ffn_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_post_ffn_layer_norm_scale', 'elementwise_mul_143.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_143.tmp_0']} = reduce_sum(inputs={X=['square_143.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_post_ffn_layer_norm_scale', 'elementwise_mul_143.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_144.tmp_0']} = square(inputs={X=['encoder_layer_7_ffn_fc_0.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_ffn_fc_0.b_0', 'elementwise_mul_144.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_144.tmp_0']} = reduce_sum(inputs={X=['square_144.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_ffn_fc_0.b_0', 'elementwise_mul_144.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_145.tmp_0']} = square(inputs={X=['encoder_layer_7_ffn_fc_0.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_ffn_fc_0.w_0', 'elementwise_mul_145.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_145.tmp_0']} = reduce_sum(inputs={X=['square_145.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_ffn_fc_0.w_0', 'elementwise_mul_145.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_146.tmp_0']} = square(inputs={X=['encoder_layer_7_ffn_fc_1.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_ffn_fc_1.b_0', 'elementwise_mul_146.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_146.tmp_0']} = reduce_sum(inputs={X=['square_146.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_ffn_fc_1.b_0', 'elementwise_mul_146.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_147.tmp_0']} = square(inputs={X=['encoder_layer_7_ffn_fc_1.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_ffn_fc_1.w_0', 'elementwise_mul_147.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_147.tmp_0']} = reduce_sum(inputs={X=['square_147.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_ffn_fc_1.w_0', 'elementwise_mul_147.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_148.tmp_0']} = square(inputs={X=['encoder_layer_7_multi_head_att_key_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_key_fc.b_0', 'elementwise_mul_148.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_148.tmp_0']} = reduce_sum(inputs={X=['square_148.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_key_fc.b_0', 'elementwise_mul_148.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_149.tmp_0']} = square(inputs={X=['encoder_layer_7_multi_head_att_key_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_key_fc.w_0', 'elementwise_mul_149.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_149.tmp_0']} = reduce_sum(inputs={X=['square_149.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_key_fc.w_0', 'elementwise_mul_149.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_150.tmp_0']} = square(inputs={X=['encoder_layer_7_multi_head_att_output_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_output_fc.b_0', 'elementwise_mul_150.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_150.tmp_0']} = reduce_sum(inputs={X=['square_150.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_output_fc.b_0', 'elementwise_mul_150.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_151.tmp_0']} = square(inputs={X=['encoder_layer_7_multi_head_att_output_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_output_fc.w_0', 'elementwise_mul_151.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_151.tmp_0']} = reduce_sum(inputs={X=['square_151.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_output_fc.w_0', 'elementwise_mul_151.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_152.tmp_0']} = square(inputs={X=['encoder_layer_7_multi_head_att_query_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_query_fc.b_0', 'elementwise_mul_152.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_152.tmp_0']} = reduce_sum(inputs={X=['square_152.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_query_fc.b_0', 'elementwise_mul_152.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_153.tmp_0']} = square(inputs={X=['encoder_layer_7_multi_head_att_query_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_query_fc.w_0', 'elementwise_mul_153.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_153.tmp_0']} = reduce_sum(inputs={X=['square_153.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_query_fc.w_0', 'elementwise_mul_153.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_154.tmp_0']} = square(inputs={X=['encoder_layer_7_multi_head_att_value_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_value_fc.b_0', 'elementwise_mul_154.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_154.tmp_0']} = reduce_sum(inputs={X=['square_154.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_value_fc.b_0', 'elementwise_mul_154.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_155.tmp_0']} = square(inputs={X=['encoder_layer_7_multi_head_att_value_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_value_fc.w_0', 'elementwise_mul_155.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_155.tmp_0']} = reduce_sum(inputs={X=['square_155.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_value_fc.w_0', 'elementwise_mul_155.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_156.tmp_0']} = square(inputs={X=['encoder_layer_7_post_att_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_post_att_layer_norm_bias', 'elementwise_mul_156.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_156.tmp_0']} = reduce_sum(inputs={X=['square_156.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_post_att_layer_norm_bias', 'elementwise_mul_156.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_157.tmp_0']} = square(inputs={X=['encoder_layer_7_post_att_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_post_att_layer_norm_scale', 'elementwise_mul_157.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_157.tmp_0']} = reduce_sum(inputs={X=['square_157.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_post_att_layer_norm_scale', 'elementwise_mul_157.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_158.tmp_0']} = square(inputs={X=['encoder_layer_7_post_ffn_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_post_ffn_layer_norm_bias', 'elementwise_mul_158.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_158.tmp_0']} = reduce_sum(inputs={X=['square_158.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_post_ffn_layer_norm_bias', 'elementwise_mul_158.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_159.tmp_0']} = square(inputs={X=['encoder_layer_7_post_ffn_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_post_ffn_layer_norm_scale', 'elementwise_mul_159.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_159.tmp_0']} = reduce_sum(inputs={X=['square_159.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_post_ffn_layer_norm_scale', 'elementwise_mul_159.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_160.tmp_0']} = square(inputs={X=['encoder_layer_8_ffn_fc_0.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_ffn_fc_0.b_0', 'elementwise_mul_160.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_160.tmp_0']} = reduce_sum(inputs={X=['square_160.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_ffn_fc_0.b_0', 'elementwise_mul_160.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_161.tmp_0']} = square(inputs={X=['encoder_layer_8_ffn_fc_0.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_ffn_fc_0.w_0', 'elementwise_mul_161.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_161.tmp_0']} = reduce_sum(inputs={X=['square_161.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_ffn_fc_0.w_0', 'elementwise_mul_161.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_162.tmp_0']} = square(inputs={X=['encoder_layer_8_ffn_fc_1.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_ffn_fc_1.b_0', 'elementwise_mul_162.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_162.tmp_0']} = reduce_sum(inputs={X=['square_162.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_ffn_fc_1.b_0', 'elementwise_mul_162.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_163.tmp_0']} = square(inputs={X=['encoder_layer_8_ffn_fc_1.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_ffn_fc_1.w_0', 'elementwise_mul_163.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_163.tmp_0']} = reduce_sum(inputs={X=['square_163.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_ffn_fc_1.w_0', 'elementwise_mul_163.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_164.tmp_0']} = square(inputs={X=['encoder_layer_8_multi_head_att_key_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_key_fc.b_0', 'elementwise_mul_164.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_164.tmp_0']} = reduce_sum(inputs={X=['square_164.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_key_fc.b_0', 'elementwise_mul_164.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_165.tmp_0']} = square(inputs={X=['encoder_layer_8_multi_head_att_key_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_key_fc.w_0', 'elementwise_mul_165.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_165.tmp_0']} = reduce_sum(inputs={X=['square_165.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_key_fc.w_0', 'elementwise_mul_165.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_166.tmp_0']} = square(inputs={X=['encoder_layer_8_multi_head_att_output_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_output_fc.b_0', 'elementwise_mul_166.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_166.tmp_0']} = reduce_sum(inputs={X=['square_166.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_output_fc.b_0', 'elementwise_mul_166.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_167.tmp_0']} = square(inputs={X=['encoder_layer_8_multi_head_att_output_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_output_fc.w_0', 'elementwise_mul_167.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_167.tmp_0']} = reduce_sum(inputs={X=['square_167.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_output_fc.w_0', 'elementwise_mul_167.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_168.tmp_0']} = square(inputs={X=['encoder_layer_8_multi_head_att_query_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_query_fc.b_0', 'elementwise_mul_168.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_168.tmp_0']} = reduce_sum(inputs={X=['square_168.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_query_fc.b_0', 'elementwise_mul_168.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_169.tmp_0']} = square(inputs={X=['encoder_layer_8_multi_head_att_query_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_query_fc.w_0', 'elementwise_mul_169.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_169.tmp_0']} = reduce_sum(inputs={X=['square_169.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_query_fc.w_0', 'elementwise_mul_169.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_170.tmp_0']} = square(inputs={X=['encoder_layer_8_multi_head_att_value_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_value_fc.b_0', 'elementwise_mul_170.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_170.tmp_0']} = reduce_sum(inputs={X=['square_170.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_value_fc.b_0', 'elementwise_mul_170.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_171.tmp_0']} = square(inputs={X=['encoder_layer_8_multi_head_att_value_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_value_fc.w_0', 'elementwise_mul_171.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_171.tmp_0']} = reduce_sum(inputs={X=['square_171.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_value_fc.w_0', 'elementwise_mul_171.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_172.tmp_0']} = square(inputs={X=['encoder_layer_8_post_att_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_post_att_layer_norm_bias', 'elementwise_mul_172.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_172.tmp_0']} = reduce_sum(inputs={X=['square_172.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_post_att_layer_norm_bias', 'elementwise_mul_172.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_173.tmp_0']} = square(inputs={X=['encoder_layer_8_post_att_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_post_att_layer_norm_scale', 'elementwise_mul_173.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_173.tmp_0']} = reduce_sum(inputs={X=['square_173.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_post_att_layer_norm_scale', 'elementwise_mul_173.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_174.tmp_0']} = square(inputs={X=['encoder_layer_8_post_ffn_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_post_ffn_layer_norm_bias', 'elementwise_mul_174.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_174.tmp_0']} = reduce_sum(inputs={X=['square_174.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_post_ffn_layer_norm_bias', 'elementwise_mul_174.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_175.tmp_0']} = square(inputs={X=['encoder_layer_8_post_ffn_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_post_ffn_layer_norm_scale', 'elementwise_mul_175.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_175.tmp_0']} = reduce_sum(inputs={X=['square_175.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_post_ffn_layer_norm_scale', 'elementwise_mul_175.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_176.tmp_0']} = square(inputs={X=['encoder_layer_9_ffn_fc_0.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_ffn_fc_0.b_0', 'elementwise_mul_176.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_176.tmp_0']} = reduce_sum(inputs={X=['square_176.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_ffn_fc_0.b_0', 'elementwise_mul_176.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_177.tmp_0']} = square(inputs={X=['encoder_layer_9_ffn_fc_0.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_ffn_fc_0.w_0', 'elementwise_mul_177.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_177.tmp_0']} = reduce_sum(inputs={X=['square_177.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_ffn_fc_0.w_0', 'elementwise_mul_177.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_178.tmp_0']} = square(inputs={X=['encoder_layer_9_ffn_fc_1.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_ffn_fc_1.b_0', 'elementwise_mul_178.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_178.tmp_0']} = reduce_sum(inputs={X=['square_178.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_ffn_fc_1.b_0', 'elementwise_mul_178.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_179.tmp_0']} = square(inputs={X=['encoder_layer_9_ffn_fc_1.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_ffn_fc_1.w_0', 'elementwise_mul_179.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_179.tmp_0']} = reduce_sum(inputs={X=['square_179.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_ffn_fc_1.w_0', 'elementwise_mul_179.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_180.tmp_0']} = square(inputs={X=['encoder_layer_9_multi_head_att_key_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_key_fc.b_0', 'elementwise_mul_180.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_180.tmp_0']} = reduce_sum(inputs={X=['square_180.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_key_fc.b_0', 'elementwise_mul_180.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_181.tmp_0']} = square(inputs={X=['encoder_layer_9_multi_head_att_key_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_key_fc.w_0', 'elementwise_mul_181.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_181.tmp_0']} = reduce_sum(inputs={X=['square_181.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_key_fc.w_0', 'elementwise_mul_181.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_182.tmp_0']} = square(inputs={X=['encoder_layer_9_multi_head_att_output_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_output_fc.b_0', 'elementwise_mul_182.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_182.tmp_0']} = reduce_sum(inputs={X=['square_182.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_output_fc.b_0', 'elementwise_mul_182.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_183.tmp_0']} = square(inputs={X=['encoder_layer_9_multi_head_att_output_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_output_fc.w_0', 'elementwise_mul_183.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_183.tmp_0']} = reduce_sum(inputs={X=['square_183.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_output_fc.w_0', 'elementwise_mul_183.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_184.tmp_0']} = square(inputs={X=['encoder_layer_9_multi_head_att_query_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_query_fc.b_0', 'elementwise_mul_184.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_184.tmp_0']} = reduce_sum(inputs={X=['square_184.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_query_fc.b_0', 'elementwise_mul_184.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_185.tmp_0']} = square(inputs={X=['encoder_layer_9_multi_head_att_query_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_query_fc.w_0', 'elementwise_mul_185.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_185.tmp_0']} = reduce_sum(inputs={X=['square_185.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_query_fc.w_0', 'elementwise_mul_185.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_186.tmp_0']} = square(inputs={X=['encoder_layer_9_multi_head_att_value_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_value_fc.b_0', 'elementwise_mul_186.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_186.tmp_0']} = reduce_sum(inputs={X=['square_186.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_value_fc.b_0', 'elementwise_mul_186.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_187.tmp_0']} = square(inputs={X=['encoder_layer_9_multi_head_att_value_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_value_fc.w_0', 'elementwise_mul_187.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_187.tmp_0']} = reduce_sum(inputs={X=['square_187.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_value_fc.w_0', 'elementwise_mul_187.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_188.tmp_0']} = square(inputs={X=['encoder_layer_9_post_att_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_post_att_layer_norm_bias', 'elementwise_mul_188.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_188.tmp_0']} = reduce_sum(inputs={X=['square_188.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_post_att_layer_norm_bias', 'elementwise_mul_188.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_189.tmp_0']} = square(inputs={X=['encoder_layer_9_post_att_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_post_att_layer_norm_scale', 'elementwise_mul_189.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_189.tmp_0']} = reduce_sum(inputs={X=['square_189.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_post_att_layer_norm_scale', 'elementwise_mul_189.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_190.tmp_0']} = square(inputs={X=['encoder_layer_9_post_ffn_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_post_ffn_layer_norm_bias', 'elementwise_mul_190.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_190.tmp_0']} = reduce_sum(inputs={X=['square_190.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_post_ffn_layer_norm_bias', 'elementwise_mul_190.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_191.tmp_0']} = square(inputs={X=['encoder_layer_9_post_ffn_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_post_ffn_layer_norm_scale', 'elementwise_mul_191.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_191.tmp_0']} = reduce_sum(inputs={X=['square_191.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_post_ffn_layer_norm_scale', 'elementwise_mul_191.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_192.tmp_0']} = square(inputs={X=['mask_lm_out_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['mask_lm_out_fc.b_0', 'elementwise_mul_192.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_192.tmp_0']} = reduce_sum(inputs={X=['square_192.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['mask_lm_out_fc.b_0', 'elementwise_mul_192.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_193.tmp_0']} = square(inputs={X=['mask_lm_out_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['mask_lm_out_fc.w_0', 'elementwise_mul_193.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_193.tmp_0']} = reduce_sum(inputs={X=['square_193.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['mask_lm_out_fc.w_0', 'elementwise_mul_193.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_194.tmp_0']} = square(inputs={X=['mask_lm_trans_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['mask_lm_trans_fc.b_0', 'elementwise_mul_194.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_194.tmp_0']} = reduce_sum(inputs={X=['square_194.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['mask_lm_trans_fc.b_0', 'elementwise_mul_194.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_195.tmp_0']} = square(inputs={X=['mask_lm_trans_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['mask_lm_trans_fc.w_0', 'elementwise_mul_195.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_195.tmp_0']} = reduce_sum(inputs={X=['square_195.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['mask_lm_trans_fc.w_0', 'elementwise_mul_195.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_196.tmp_0']} = square(inputs={X=['mask_lm_trans_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['mask_lm_trans_layer_norm_bias', 'elementwise_mul_196.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_196.tmp_0']} = reduce_sum(inputs={X=['square_196.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['mask_lm_trans_layer_norm_bias', 'elementwise_mul_196.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_197.tmp_0']} = square(inputs={X=['mask_lm_trans_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['mask_lm_trans_layer_norm_scale', 'elementwise_mul_197.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_197.tmp_0']} = reduce_sum(inputs={X=['square_197.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['mask_lm_trans_layer_norm_scale', 'elementwise_mul_197.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['sum_0.tmp_0']} = sum(inputs={X=['reduce_sum_16.tmp_0', 'reduce_sum_17.tmp_0', 'reduce_sum_18.tmp_0', 'reduce_sum_19.tmp_0', 'reduce_sum_20.tmp_0', 'reduce_sum_21.tmp_0', 'reduce_sum_22.tmp_0', 'reduce_sum_23.tmp_0', 'reduce_sum_24.tmp_0', 'reduce_sum_25.tmp_0', 'reduce_sum_26.tmp_0', 'reduce_sum_27.tmp_0', 'reduce_sum_28.tmp_0', 'reduce_sum_29.tmp_0', 'reduce_sum_30.tmp_0', 'reduce_sum_31.tmp_0', 'reduce_sum_32.tmp_0', 'reduce_sum_33.tmp_0', 'reduce_sum_34.tmp_0', 'reduce_sum_35.tmp_0', 'reduce_sum_36.tmp_0', 'reduce_sum_37.tmp_0', 'reduce_sum_38.tmp_0', 'reduce_sum_39.tmp_0', 'reduce_sum_40.tmp_0', 'reduce_sum_41.tmp_0', 'reduce_sum_42.tmp_0', 'reduce_sum_43.tmp_0', 'reduce_sum_44.tmp_0', 'reduce_sum_45.tmp_0', 'reduce_sum_46.tmp_0', 'reduce_sum_47.tmp_0', 'reduce_sum_128.tmp_0', 'reduce_sum_129.tmp_0', 'reduce_sum_130.tmp_0', 'reduce_sum_131.tmp_0', 'reduce_sum_132.tmp_0', 'reduce_sum_133.tmp_0', 'reduce_sum_134.tmp_0', 'reduce_sum_135.tmp_0', 'reduce_sum_136.tmp_0', 'reduce_sum_137.tmp_0', 'reduce_sum_138.tmp_0', 'reduce_sum_139.tmp_0', 'reduce_sum_140.tmp_0', 'reduce_sum_141.tmp_0', 'reduce_sum_142.tmp_0', 'reduce_sum_143.tmp_0', 'reduce_sum_144.tmp_0', 'reduce_sum_145.tmp_0', 'reduce_sum_146.tmp_0', 'reduce_sum_147.tmp_0', 'reduce_sum_148.tmp_0', 'reduce_sum_149.tmp_0', 'reduce_sum_150.tmp_0', 'reduce_sum_151.tmp_0', 'reduce_sum_152.tmp_0', 'reduce_sum_153.tmp_0', 'reduce_sum_154.tmp_0', 'reduce_sum_155.tmp_0', 'reduce_sum_156.tmp_0', 'reduce_sum_157.tmp_0', 'reduce_sum_158.tmp_0', 'reduce_sum_159.tmp_0', 'reduce_sum_160.tmp_0', 'reduce_sum_161.tmp_0', 'reduce_sum_162.tmp_0', 'reduce_sum_163.tmp_0', 'reduce_sum_164.tmp_0', 'reduce_sum_165.tmp_0', 'reduce_sum_166.tmp_0', 'reduce_sum_167.tmp_0', 'reduce_sum_168.tmp_0', 'reduce_sum_169.tmp_0', 'reduce_sum_170.tmp_0', 'reduce_sum_171.tmp_0', 'reduce_sum_172.tmp_0', 'reduce_sum_173.tmp_0', 'reduce_sum_174.tmp_0', 'reduce_sum_175.tmp_0', 'reduce_sum_176.tmp_0', 'reduce_sum_177.tmp_0', 'reduce_sum_178.tmp_0', 'reduce_sum_179.tmp_0', 'reduce_sum_180.tmp_0', 'reduce_sum_181.tmp_0', 'reduce_sum_182.tmp_0', 'reduce_sum_183.tmp_0', 'reduce_sum_184.tmp_0', 'reduce_sum_185.tmp_0', 'reduce_sum_186.tmp_0', 'reduce_sum_187.tmp_0', 'reduce_sum_188.tmp_0', 'reduce_sum_189.tmp_0', 'reduce_sum_190.tmp_0', 'reduce_sum_191.tmp_0', 'reduce_sum_192.tmp_0', 'reduce_sum_193.tmp_0', 'reduce_sum_194.tmp_0', 'reduce_sum_195.tmp_0', 'reduce_sum_196.tmp_0', 'reduce_sum_197.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['word_embedding', 'elementwise_mul_202.tmp_0'], use_mkldnn = False)
    {Out=['sum_0.tmp_0']} = c_allreduce_sum(inputs={X=['sum_0.tmp_0']}, op_device = , op_namescope = /, op_role = 2, op_role_var = [], ring_id = 3, tag = tag, use_calc_stream = True, use_model_parallel = False)
    {Out=['sqrt_0.tmp_0']} = sqrt(inputs={X=['sum_0.tmp_0']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['word_embedding', 'elementwise_mul_202.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['fill_constant_7.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['word_embedding', 'elementwise_mul_202.tmp_0'], place_type = -1, shape = [1], str_value = 1.0, value = 1.0)
    {Out=['elementwise_max_0.tmp_0']} = elementwise_max(inputs={X=['fill_constant_7.tmp_0'], Y=['sqrt_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['word_embedding', 'elementwise_mul_202.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_div_0.tmp_0']} = elementwise_div(inputs={X=['fill_constant_7.tmp_0'], Y=['elementwise_max_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['word_embedding', 'elementwise_mul_202.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_16.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_10_ffn_fc_0.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_ffn_fc_0.b_0', 'elementwise_mul_16.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_17.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_10_ffn_fc_0.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_ffn_fc_0.w_0', 'elementwise_mul_17.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_18.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_10_ffn_fc_1.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_ffn_fc_1.b_0', 'elementwise_mul_18.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_19.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_10_ffn_fc_1.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_ffn_fc_1.w_0', 'elementwise_mul_19.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_20.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_10_multi_head_att_key_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_key_fc.b_0', 'elementwise_mul_20.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_21.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_10_multi_head_att_key_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_key_fc.w_0', 'elementwise_mul_21.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_22.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_10_multi_head_att_output_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_output_fc.b_0', 'elementwise_mul_22.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_23.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_10_multi_head_att_output_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_output_fc.w_0', 'elementwise_mul_23.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_24.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_10_multi_head_att_query_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_query_fc.b_0', 'elementwise_mul_24.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_25.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_10_multi_head_att_query_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_query_fc.w_0', 'elementwise_mul_25.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_26.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_10_multi_head_att_value_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_value_fc.b_0', 'elementwise_mul_26.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_27.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_10_multi_head_att_value_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_value_fc.w_0', 'elementwise_mul_27.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_28.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_10_post_att_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_post_att_layer_norm_bias', 'elementwise_mul_28.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_29.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_10_post_att_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_post_att_layer_norm_scale', 'elementwise_mul_29.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_30.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_10_post_ffn_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_post_ffn_layer_norm_bias', 'elementwise_mul_30.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_31.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_10_post_ffn_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_10_post_ffn_layer_norm_scale', 'elementwise_mul_31.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_32.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_11_ffn_fc_0.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_ffn_fc_0.b_0', 'elementwise_mul_32.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_33.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_11_ffn_fc_0.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_ffn_fc_0.w_0', 'elementwise_mul_33.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_34.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_11_ffn_fc_1.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_ffn_fc_1.b_0', 'elementwise_mul_34.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_35.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_11_ffn_fc_1.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_ffn_fc_1.w_0', 'elementwise_mul_35.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_36.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_11_multi_head_att_key_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_key_fc.b_0', 'elementwise_mul_36.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_37.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_11_multi_head_att_key_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_key_fc.w_0', 'elementwise_mul_37.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_38.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_11_multi_head_att_output_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_output_fc.b_0', 'elementwise_mul_38.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_39.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_11_multi_head_att_output_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_output_fc.w_0', 'elementwise_mul_39.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_40.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_11_multi_head_att_query_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_query_fc.b_0', 'elementwise_mul_40.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_41.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_11_multi_head_att_query_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_query_fc.w_0', 'elementwise_mul_41.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_42.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_11_multi_head_att_value_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_value_fc.b_0', 'elementwise_mul_42.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_43.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_11_multi_head_att_value_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_value_fc.w_0', 'elementwise_mul_43.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_44.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_11_post_att_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_post_att_layer_norm_bias', 'elementwise_mul_44.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_45.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_11_post_att_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_post_att_layer_norm_scale', 'elementwise_mul_45.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_46.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_11_post_ffn_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_post_ffn_layer_norm_bias', 'elementwise_mul_46.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_47.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_11_post_ffn_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_11_post_ffn_layer_norm_scale', 'elementwise_mul_47.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_128.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_6_ffn_fc_0.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_ffn_fc_0.b_0', 'elementwise_mul_128.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_129.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_6_ffn_fc_0.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_ffn_fc_0.w_0', 'elementwise_mul_129.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_130.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_6_ffn_fc_1.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_ffn_fc_1.b_0', 'elementwise_mul_130.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_131.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_6_ffn_fc_1.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_ffn_fc_1.w_0', 'elementwise_mul_131.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_132.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_6_multi_head_att_key_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_key_fc.b_0', 'elementwise_mul_132.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_133.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_6_multi_head_att_key_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_key_fc.w_0', 'elementwise_mul_133.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_134.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_6_multi_head_att_output_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_output_fc.b_0', 'elementwise_mul_134.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_135.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_6_multi_head_att_output_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_output_fc.w_0', 'elementwise_mul_135.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_136.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_6_multi_head_att_query_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_query_fc.b_0', 'elementwise_mul_136.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_137.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_6_multi_head_att_query_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_query_fc.w_0', 'elementwise_mul_137.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_138.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_6_multi_head_att_value_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_value_fc.b_0', 'elementwise_mul_138.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_139.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_6_multi_head_att_value_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_value_fc.w_0', 'elementwise_mul_139.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_140.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_6_post_att_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_post_att_layer_norm_bias', 'elementwise_mul_140.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_141.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_6_post_att_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_post_att_layer_norm_scale', 'elementwise_mul_141.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_142.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_6_post_ffn_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_post_ffn_layer_norm_bias', 'elementwise_mul_142.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_143.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_6_post_ffn_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_6_post_ffn_layer_norm_scale', 'elementwise_mul_143.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_144.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_7_ffn_fc_0.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_ffn_fc_0.b_0', 'elementwise_mul_144.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_145.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_7_ffn_fc_0.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_ffn_fc_0.w_0', 'elementwise_mul_145.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_146.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_7_ffn_fc_1.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_ffn_fc_1.b_0', 'elementwise_mul_146.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_147.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_7_ffn_fc_1.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_ffn_fc_1.w_0', 'elementwise_mul_147.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_148.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_7_multi_head_att_key_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_key_fc.b_0', 'elementwise_mul_148.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_149.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_7_multi_head_att_key_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_key_fc.w_0', 'elementwise_mul_149.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_150.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_7_multi_head_att_output_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_output_fc.b_0', 'elementwise_mul_150.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_151.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_7_multi_head_att_output_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_output_fc.w_0', 'elementwise_mul_151.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_152.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_7_multi_head_att_query_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_query_fc.b_0', 'elementwise_mul_152.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_153.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_7_multi_head_att_query_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_query_fc.w_0', 'elementwise_mul_153.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_154.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_7_multi_head_att_value_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_value_fc.b_0', 'elementwise_mul_154.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_155.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_7_multi_head_att_value_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_value_fc.w_0', 'elementwise_mul_155.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_156.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_7_post_att_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_post_att_layer_norm_bias', 'elementwise_mul_156.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_157.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_7_post_att_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_post_att_layer_norm_scale', 'elementwise_mul_157.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_158.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_7_post_ffn_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_post_ffn_layer_norm_bias', 'elementwise_mul_158.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_159.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_7_post_ffn_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_7_post_ffn_layer_norm_scale', 'elementwise_mul_159.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_160.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_8_ffn_fc_0.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_ffn_fc_0.b_0', 'elementwise_mul_160.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_161.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_8_ffn_fc_0.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_ffn_fc_0.w_0', 'elementwise_mul_161.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_162.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_8_ffn_fc_1.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_ffn_fc_1.b_0', 'elementwise_mul_162.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_163.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_8_ffn_fc_1.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_ffn_fc_1.w_0', 'elementwise_mul_163.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_164.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_8_multi_head_att_key_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_key_fc.b_0', 'elementwise_mul_164.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_165.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_8_multi_head_att_key_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_key_fc.w_0', 'elementwise_mul_165.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_166.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_8_multi_head_att_output_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_output_fc.b_0', 'elementwise_mul_166.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_167.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_8_multi_head_att_output_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_output_fc.w_0', 'elementwise_mul_167.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_168.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_8_multi_head_att_query_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_query_fc.b_0', 'elementwise_mul_168.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_169.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_8_multi_head_att_query_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_query_fc.w_0', 'elementwise_mul_169.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_170.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_8_multi_head_att_value_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_value_fc.b_0', 'elementwise_mul_170.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_171.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_8_multi_head_att_value_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_value_fc.w_0', 'elementwise_mul_171.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_172.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_8_post_att_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_post_att_layer_norm_bias', 'elementwise_mul_172.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_173.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_8_post_att_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_post_att_layer_norm_scale', 'elementwise_mul_173.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_174.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_8_post_ffn_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_post_ffn_layer_norm_bias', 'elementwise_mul_174.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_175.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_8_post_ffn_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_8_post_ffn_layer_norm_scale', 'elementwise_mul_175.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_176.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_9_ffn_fc_0.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_ffn_fc_0.b_0', 'elementwise_mul_176.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_177.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_9_ffn_fc_0.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_ffn_fc_0.w_0', 'elementwise_mul_177.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_178.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_9_ffn_fc_1.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_ffn_fc_1.b_0', 'elementwise_mul_178.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_179.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_9_ffn_fc_1.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_ffn_fc_1.w_0', 'elementwise_mul_179.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_180.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_9_multi_head_att_key_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_key_fc.b_0', 'elementwise_mul_180.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_181.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_9_multi_head_att_key_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_key_fc.w_0', 'elementwise_mul_181.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_182.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_9_multi_head_att_output_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_output_fc.b_0', 'elementwise_mul_182.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_183.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_9_multi_head_att_output_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_output_fc.w_0', 'elementwise_mul_183.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_184.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_9_multi_head_att_query_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_query_fc.b_0', 'elementwise_mul_184.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_185.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_9_multi_head_att_query_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_query_fc.w_0', 'elementwise_mul_185.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_186.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_9_multi_head_att_value_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_value_fc.b_0', 'elementwise_mul_186.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_187.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_9_multi_head_att_value_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_value_fc.w_0', 'elementwise_mul_187.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_188.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_9_post_att_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_post_att_layer_norm_bias', 'elementwise_mul_188.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_189.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_9_post_att_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_post_att_layer_norm_scale', 'elementwise_mul_189.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_190.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_9_post_ffn_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_post_ffn_layer_norm_bias', 'elementwise_mul_190.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_191.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_9_post_ffn_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_9_post_ffn_layer_norm_scale', 'elementwise_mul_191.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_192.tmp_0']} = elementwise_mul(inputs={X=['mask_lm_out_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['mask_lm_out_fc.b_0', 'elementwise_mul_192.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_193.tmp_0']} = elementwise_mul(inputs={X=['mask_lm_out_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['mask_lm_out_fc.w_0', 'elementwise_mul_193.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_194.tmp_0']} = elementwise_mul(inputs={X=['mask_lm_trans_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['mask_lm_trans_fc.b_0', 'elementwise_mul_194.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_195.tmp_0']} = elementwise_mul(inputs={X=['mask_lm_trans_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['mask_lm_trans_fc.w_0', 'elementwise_mul_195.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_196.tmp_0']} = elementwise_mul(inputs={X=['mask_lm_trans_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['mask_lm_trans_layer_norm_bias', 'elementwise_mul_196.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_197.tmp_0']} = elementwise_mul(inputs={X=['mask_lm_trans_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['mask_lm_trans_layer_norm_scale', 'elementwise_mul_197.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Beta1PowOut=['encoder_layer_10_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_10_ffn_fc_0.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_10_ffn_fc_0.b_0_moment1_0'], Moment2Out=['encoder_layer_10_ffn_fc_0.b_0_moment2_0'], ParamOut=['encoder_layer_10_ffn_fc_0.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_10_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_10_ffn_fc_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_16.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_10_ffn_fc_0.b_0_moment1_0'], Moment2=['encoder_layer_10_ffn_fc_0.b_0_moment2_0'], Param=['encoder_layer_10_ffn_fc_0.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_16/, op_role = 2, op_role_var = ['encoder_layer_10_ffn_fc_0.b_0', 'elementwise_mul_16.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_10_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_10_ffn_fc_0.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_10_ffn_fc_0.w_0_moment1_0'], Moment2Out=['encoder_layer_10_ffn_fc_0.w_0_moment2_0'], ParamOut=['encoder_layer_10_ffn_fc_0.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_10_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_10_ffn_fc_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_17.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_10_ffn_fc_0.w_0_moment1_0'], Moment2=['encoder_layer_10_ffn_fc_0.w_0_moment2_0'], Param=['encoder_layer_10_ffn_fc_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_17/, op_role = 2, op_role_var = ['encoder_layer_10_ffn_fc_0.w_0', 'elementwise_mul_17.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_10_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_10_ffn_fc_1.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_10_ffn_fc_1.b_0_moment1_0'], Moment2Out=['encoder_layer_10_ffn_fc_1.b_0_moment2_0'], ParamOut=['encoder_layer_10_ffn_fc_1.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_10_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_10_ffn_fc_1.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_18.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_10_ffn_fc_1.b_0_moment1_0'], Moment2=['encoder_layer_10_ffn_fc_1.b_0_moment2_0'], Param=['encoder_layer_10_ffn_fc_1.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_18/, op_role = 2, op_role_var = ['encoder_layer_10_ffn_fc_1.b_0', 'elementwise_mul_18.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_10_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_10_ffn_fc_1.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_10_ffn_fc_1.w_0_moment1_0'], Moment2Out=['encoder_layer_10_ffn_fc_1.w_0_moment2_0'], ParamOut=['encoder_layer_10_ffn_fc_1.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_10_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_10_ffn_fc_1.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_19.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_10_ffn_fc_1.w_0_moment1_0'], Moment2=['encoder_layer_10_ffn_fc_1.w_0_moment2_0'], Param=['encoder_layer_10_ffn_fc_1.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_19/, op_role = 2, op_role_var = ['encoder_layer_10_ffn_fc_1.w_0', 'elementwise_mul_19.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_10_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_10_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_10_multi_head_att_key_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_10_multi_head_att_key_fc.b_0_moment2_0'], ParamOut=['encoder_layer_10_multi_head_att_key_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_10_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_10_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_20.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_10_multi_head_att_key_fc.b_0_moment1_0'], Moment2=['encoder_layer_10_multi_head_att_key_fc.b_0_moment2_0'], Param=['encoder_layer_10_multi_head_att_key_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_20/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_key_fc.b_0', 'elementwise_mul_20.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_10_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_10_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_10_multi_head_att_key_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_10_multi_head_att_key_fc.w_0_moment2_0'], ParamOut=['encoder_layer_10_multi_head_att_key_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_10_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_10_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_21.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_10_multi_head_att_key_fc.w_0_moment1_0'], Moment2=['encoder_layer_10_multi_head_att_key_fc.w_0_moment2_0'], Param=['encoder_layer_10_multi_head_att_key_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_21/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_key_fc.w_0', 'elementwise_mul_21.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_10_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_10_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_10_multi_head_att_output_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_10_multi_head_att_output_fc.b_0_moment2_0'], ParamOut=['encoder_layer_10_multi_head_att_output_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_10_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_10_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_22.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_10_multi_head_att_output_fc.b_0_moment1_0'], Moment2=['encoder_layer_10_multi_head_att_output_fc.b_0_moment2_0'], Param=['encoder_layer_10_multi_head_att_output_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_22/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_output_fc.b_0', 'elementwise_mul_22.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_10_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_10_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_10_multi_head_att_output_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_10_multi_head_att_output_fc.w_0_moment2_0'], ParamOut=['encoder_layer_10_multi_head_att_output_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_10_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_10_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_23.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_10_multi_head_att_output_fc.w_0_moment1_0'], Moment2=['encoder_layer_10_multi_head_att_output_fc.w_0_moment2_0'], Param=['encoder_layer_10_multi_head_att_output_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_23/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_output_fc.w_0', 'elementwise_mul_23.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_10_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_10_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_10_multi_head_att_query_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_10_multi_head_att_query_fc.b_0_moment2_0'], ParamOut=['encoder_layer_10_multi_head_att_query_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_10_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_10_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_24.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_10_multi_head_att_query_fc.b_0_moment1_0'], Moment2=['encoder_layer_10_multi_head_att_query_fc.b_0_moment2_0'], Param=['encoder_layer_10_multi_head_att_query_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_24/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_query_fc.b_0', 'elementwise_mul_24.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_10_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_10_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_10_multi_head_att_query_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_10_multi_head_att_query_fc.w_0_moment2_0'], ParamOut=['encoder_layer_10_multi_head_att_query_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_10_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_10_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_25.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_10_multi_head_att_query_fc.w_0_moment1_0'], Moment2=['encoder_layer_10_multi_head_att_query_fc.w_0_moment2_0'], Param=['encoder_layer_10_multi_head_att_query_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_25/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_query_fc.w_0', 'elementwise_mul_25.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_10_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_10_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_10_multi_head_att_value_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_10_multi_head_att_value_fc.b_0_moment2_0'], ParamOut=['encoder_layer_10_multi_head_att_value_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_10_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_10_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_26.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_10_multi_head_att_value_fc.b_0_moment1_0'], Moment2=['encoder_layer_10_multi_head_att_value_fc.b_0_moment2_0'], Param=['encoder_layer_10_multi_head_att_value_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_26/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_value_fc.b_0', 'elementwise_mul_26.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_10_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_10_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_10_multi_head_att_value_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_10_multi_head_att_value_fc.w_0_moment2_0'], ParamOut=['encoder_layer_10_multi_head_att_value_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_10_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_10_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_27.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_10_multi_head_att_value_fc.w_0_moment1_0'], Moment2=['encoder_layer_10_multi_head_att_value_fc.w_0_moment2_0'], Param=['encoder_layer_10_multi_head_att_value_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_27/, op_role = 2, op_role_var = ['encoder_layer_10_multi_head_att_value_fc.w_0', 'elementwise_mul_27.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_10_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_10_post_att_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_10_post_att_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_10_post_att_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_10_post_att_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_10_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_10_post_att_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_28.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_10_post_att_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_10_post_att_layer_norm_bias_moment2_0'], Param=['encoder_layer_10_post_att_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_28/, op_role = 2, op_role_var = ['encoder_layer_10_post_att_layer_norm_bias', 'elementwise_mul_28.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_10_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_10_post_att_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_10_post_att_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_10_post_att_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_10_post_att_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_10_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_10_post_att_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_29.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_10_post_att_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_10_post_att_layer_norm_scale_moment2_0'], Param=['encoder_layer_10_post_att_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_29/, op_role = 2, op_role_var = ['encoder_layer_10_post_att_layer_norm_scale', 'elementwise_mul_29.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_10_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_10_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_10_post_ffn_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_10_post_ffn_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_10_post_ffn_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_10_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_10_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_30.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_10_post_ffn_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_10_post_ffn_layer_norm_bias_moment2_0'], Param=['encoder_layer_10_post_ffn_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_30/, op_role = 2, op_role_var = ['encoder_layer_10_post_ffn_layer_norm_bias', 'elementwise_mul_30.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_10_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_10_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_10_post_ffn_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_10_post_ffn_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_10_post_ffn_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_10_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_10_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_31.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_10_post_ffn_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_10_post_ffn_layer_norm_scale_moment2_0'], Param=['encoder_layer_10_post_ffn_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_31/, op_role = 2, op_role_var = ['encoder_layer_10_post_ffn_layer_norm_scale', 'elementwise_mul_31.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_11_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_11_ffn_fc_0.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_11_ffn_fc_0.b_0_moment1_0'], Moment2Out=['encoder_layer_11_ffn_fc_0.b_0_moment2_0'], ParamOut=['encoder_layer_11_ffn_fc_0.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_11_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_11_ffn_fc_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_32.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_11_ffn_fc_0.b_0_moment1_0'], Moment2=['encoder_layer_11_ffn_fc_0.b_0_moment2_0'], Param=['encoder_layer_11_ffn_fc_0.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_32/, op_role = 2, op_role_var = ['encoder_layer_11_ffn_fc_0.b_0', 'elementwise_mul_32.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_11_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_11_ffn_fc_0.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_11_ffn_fc_0.w_0_moment1_0'], Moment2Out=['encoder_layer_11_ffn_fc_0.w_0_moment2_0'], ParamOut=['encoder_layer_11_ffn_fc_0.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_11_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_11_ffn_fc_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_33.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_11_ffn_fc_0.w_0_moment1_0'], Moment2=['encoder_layer_11_ffn_fc_0.w_0_moment2_0'], Param=['encoder_layer_11_ffn_fc_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_33/, op_role = 2, op_role_var = ['encoder_layer_11_ffn_fc_0.w_0', 'elementwise_mul_33.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_11_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_11_ffn_fc_1.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_11_ffn_fc_1.b_0_moment1_0'], Moment2Out=['encoder_layer_11_ffn_fc_1.b_0_moment2_0'], ParamOut=['encoder_layer_11_ffn_fc_1.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_11_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_11_ffn_fc_1.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_34.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_11_ffn_fc_1.b_0_moment1_0'], Moment2=['encoder_layer_11_ffn_fc_1.b_0_moment2_0'], Param=['encoder_layer_11_ffn_fc_1.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_34/, op_role = 2, op_role_var = ['encoder_layer_11_ffn_fc_1.b_0', 'elementwise_mul_34.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_11_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_11_ffn_fc_1.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_11_ffn_fc_1.w_0_moment1_0'], Moment2Out=['encoder_layer_11_ffn_fc_1.w_0_moment2_0'], ParamOut=['encoder_layer_11_ffn_fc_1.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_11_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_11_ffn_fc_1.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_35.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_11_ffn_fc_1.w_0_moment1_0'], Moment2=['encoder_layer_11_ffn_fc_1.w_0_moment2_0'], Param=['encoder_layer_11_ffn_fc_1.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_35/, op_role = 2, op_role_var = ['encoder_layer_11_ffn_fc_1.w_0', 'elementwise_mul_35.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_11_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_11_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_11_multi_head_att_key_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_11_multi_head_att_key_fc.b_0_moment2_0'], ParamOut=['encoder_layer_11_multi_head_att_key_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_11_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_11_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_36.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_11_multi_head_att_key_fc.b_0_moment1_0'], Moment2=['encoder_layer_11_multi_head_att_key_fc.b_0_moment2_0'], Param=['encoder_layer_11_multi_head_att_key_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_36/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_key_fc.b_0', 'elementwise_mul_36.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_11_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_11_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_11_multi_head_att_key_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_11_multi_head_att_key_fc.w_0_moment2_0'], ParamOut=['encoder_layer_11_multi_head_att_key_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_11_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_11_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_37.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_11_multi_head_att_key_fc.w_0_moment1_0'], Moment2=['encoder_layer_11_multi_head_att_key_fc.w_0_moment2_0'], Param=['encoder_layer_11_multi_head_att_key_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_37/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_key_fc.w_0', 'elementwise_mul_37.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_11_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_11_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_11_multi_head_att_output_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_11_multi_head_att_output_fc.b_0_moment2_0'], ParamOut=['encoder_layer_11_multi_head_att_output_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_11_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_11_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_38.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_11_multi_head_att_output_fc.b_0_moment1_0'], Moment2=['encoder_layer_11_multi_head_att_output_fc.b_0_moment2_0'], Param=['encoder_layer_11_multi_head_att_output_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_38/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_output_fc.b_0', 'elementwise_mul_38.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_11_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_11_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_11_multi_head_att_output_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_11_multi_head_att_output_fc.w_0_moment2_0'], ParamOut=['encoder_layer_11_multi_head_att_output_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_11_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_11_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_39.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_11_multi_head_att_output_fc.w_0_moment1_0'], Moment2=['encoder_layer_11_multi_head_att_output_fc.w_0_moment2_0'], Param=['encoder_layer_11_multi_head_att_output_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_39/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_output_fc.w_0', 'elementwise_mul_39.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_11_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_11_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_11_multi_head_att_query_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_11_multi_head_att_query_fc.b_0_moment2_0'], ParamOut=['encoder_layer_11_multi_head_att_query_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_11_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_11_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_40.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_11_multi_head_att_query_fc.b_0_moment1_0'], Moment2=['encoder_layer_11_multi_head_att_query_fc.b_0_moment2_0'], Param=['encoder_layer_11_multi_head_att_query_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_40/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_query_fc.b_0', 'elementwise_mul_40.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_11_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_11_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_11_multi_head_att_query_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_11_multi_head_att_query_fc.w_0_moment2_0'], ParamOut=['encoder_layer_11_multi_head_att_query_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_11_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_11_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_41.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_11_multi_head_att_query_fc.w_0_moment1_0'], Moment2=['encoder_layer_11_multi_head_att_query_fc.w_0_moment2_0'], Param=['encoder_layer_11_multi_head_att_query_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_41/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_query_fc.w_0', 'elementwise_mul_41.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_11_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_11_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_11_multi_head_att_value_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_11_multi_head_att_value_fc.b_0_moment2_0'], ParamOut=['encoder_layer_11_multi_head_att_value_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_11_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_11_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_42.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_11_multi_head_att_value_fc.b_0_moment1_0'], Moment2=['encoder_layer_11_multi_head_att_value_fc.b_0_moment2_0'], Param=['encoder_layer_11_multi_head_att_value_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_42/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_value_fc.b_0', 'elementwise_mul_42.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_11_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_11_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_11_multi_head_att_value_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_11_multi_head_att_value_fc.w_0_moment2_0'], ParamOut=['encoder_layer_11_multi_head_att_value_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_11_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_11_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_43.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_11_multi_head_att_value_fc.w_0_moment1_0'], Moment2=['encoder_layer_11_multi_head_att_value_fc.w_0_moment2_0'], Param=['encoder_layer_11_multi_head_att_value_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_43/, op_role = 2, op_role_var = ['encoder_layer_11_multi_head_att_value_fc.w_0', 'elementwise_mul_43.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_11_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_11_post_att_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_11_post_att_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_11_post_att_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_11_post_att_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_11_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_11_post_att_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_44.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_11_post_att_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_11_post_att_layer_norm_bias_moment2_0'], Param=['encoder_layer_11_post_att_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_44/, op_role = 2, op_role_var = ['encoder_layer_11_post_att_layer_norm_bias', 'elementwise_mul_44.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_11_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_11_post_att_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_11_post_att_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_11_post_att_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_11_post_att_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_11_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_11_post_att_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_45.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_11_post_att_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_11_post_att_layer_norm_scale_moment2_0'], Param=['encoder_layer_11_post_att_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_45/, op_role = 2, op_role_var = ['encoder_layer_11_post_att_layer_norm_scale', 'elementwise_mul_45.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_11_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_11_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_11_post_ffn_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_11_post_ffn_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_11_post_ffn_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_11_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_11_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_46.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_11_post_ffn_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_11_post_ffn_layer_norm_bias_moment2_0'], Param=['encoder_layer_11_post_ffn_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_46/, op_role = 2, op_role_var = ['encoder_layer_11_post_ffn_layer_norm_bias', 'elementwise_mul_46.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_11_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_11_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_11_post_ffn_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_11_post_ffn_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_11_post_ffn_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_11_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_11_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_47.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_11_post_ffn_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_11_post_ffn_layer_norm_scale_moment2_0'], Param=['encoder_layer_11_post_ffn_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_47/, op_role = 2, op_role_var = ['encoder_layer_11_post_ffn_layer_norm_scale', 'elementwise_mul_47.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_6_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_6_ffn_fc_0.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_6_ffn_fc_0.b_0_moment1_0'], Moment2Out=['encoder_layer_6_ffn_fc_0.b_0_moment2_0'], ParamOut=['encoder_layer_6_ffn_fc_0.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_6_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_6_ffn_fc_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_128.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_6_ffn_fc_0.b_0_moment1_0'], Moment2=['encoder_layer_6_ffn_fc_0.b_0_moment2_0'], Param=['encoder_layer_6_ffn_fc_0.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_128/, op_role = 2, op_role_var = ['encoder_layer_6_ffn_fc_0.b_0', 'elementwise_mul_128.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_6_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_6_ffn_fc_0.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_6_ffn_fc_0.w_0_moment1_0'], Moment2Out=['encoder_layer_6_ffn_fc_0.w_0_moment2_0'], ParamOut=['encoder_layer_6_ffn_fc_0.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_6_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_6_ffn_fc_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_129.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_6_ffn_fc_0.w_0_moment1_0'], Moment2=['encoder_layer_6_ffn_fc_0.w_0_moment2_0'], Param=['encoder_layer_6_ffn_fc_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_129/, op_role = 2, op_role_var = ['encoder_layer_6_ffn_fc_0.w_0', 'elementwise_mul_129.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_6_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_6_ffn_fc_1.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_6_ffn_fc_1.b_0_moment1_0'], Moment2Out=['encoder_layer_6_ffn_fc_1.b_0_moment2_0'], ParamOut=['encoder_layer_6_ffn_fc_1.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_6_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_6_ffn_fc_1.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_130.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_6_ffn_fc_1.b_0_moment1_0'], Moment2=['encoder_layer_6_ffn_fc_1.b_0_moment2_0'], Param=['encoder_layer_6_ffn_fc_1.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_130/, op_role = 2, op_role_var = ['encoder_layer_6_ffn_fc_1.b_0', 'elementwise_mul_130.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_6_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_6_ffn_fc_1.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_6_ffn_fc_1.w_0_moment1_0'], Moment2Out=['encoder_layer_6_ffn_fc_1.w_0_moment2_0'], ParamOut=['encoder_layer_6_ffn_fc_1.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_6_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_6_ffn_fc_1.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_131.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_6_ffn_fc_1.w_0_moment1_0'], Moment2=['encoder_layer_6_ffn_fc_1.w_0_moment2_0'], Param=['encoder_layer_6_ffn_fc_1.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_131/, op_role = 2, op_role_var = ['encoder_layer_6_ffn_fc_1.w_0', 'elementwise_mul_131.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_6_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_6_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_6_multi_head_att_key_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_6_multi_head_att_key_fc.b_0_moment2_0'], ParamOut=['encoder_layer_6_multi_head_att_key_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_6_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_6_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_132.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_6_multi_head_att_key_fc.b_0_moment1_0'], Moment2=['encoder_layer_6_multi_head_att_key_fc.b_0_moment2_0'], Param=['encoder_layer_6_multi_head_att_key_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_132/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_key_fc.b_0', 'elementwise_mul_132.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_6_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_6_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_6_multi_head_att_key_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_6_multi_head_att_key_fc.w_0_moment2_0'], ParamOut=['encoder_layer_6_multi_head_att_key_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_6_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_6_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_133.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_6_multi_head_att_key_fc.w_0_moment1_0'], Moment2=['encoder_layer_6_multi_head_att_key_fc.w_0_moment2_0'], Param=['encoder_layer_6_multi_head_att_key_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_133/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_key_fc.w_0', 'elementwise_mul_133.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_6_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_6_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_6_multi_head_att_output_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_6_multi_head_att_output_fc.b_0_moment2_0'], ParamOut=['encoder_layer_6_multi_head_att_output_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_6_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_6_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_134.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_6_multi_head_att_output_fc.b_0_moment1_0'], Moment2=['encoder_layer_6_multi_head_att_output_fc.b_0_moment2_0'], Param=['encoder_layer_6_multi_head_att_output_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_134/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_output_fc.b_0', 'elementwise_mul_134.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_6_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_6_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_6_multi_head_att_output_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_6_multi_head_att_output_fc.w_0_moment2_0'], ParamOut=['encoder_layer_6_multi_head_att_output_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_6_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_6_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_135.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_6_multi_head_att_output_fc.w_0_moment1_0'], Moment2=['encoder_layer_6_multi_head_att_output_fc.w_0_moment2_0'], Param=['encoder_layer_6_multi_head_att_output_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_135/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_output_fc.w_0', 'elementwise_mul_135.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_6_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_6_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_6_multi_head_att_query_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_6_multi_head_att_query_fc.b_0_moment2_0'], ParamOut=['encoder_layer_6_multi_head_att_query_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_6_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_6_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_136.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_6_multi_head_att_query_fc.b_0_moment1_0'], Moment2=['encoder_layer_6_multi_head_att_query_fc.b_0_moment2_0'], Param=['encoder_layer_6_multi_head_att_query_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_136/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_query_fc.b_0', 'elementwise_mul_136.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_6_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_6_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_6_multi_head_att_query_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_6_multi_head_att_query_fc.w_0_moment2_0'], ParamOut=['encoder_layer_6_multi_head_att_query_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_6_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_6_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_137.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_6_multi_head_att_query_fc.w_0_moment1_0'], Moment2=['encoder_layer_6_multi_head_att_query_fc.w_0_moment2_0'], Param=['encoder_layer_6_multi_head_att_query_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_137/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_query_fc.w_0', 'elementwise_mul_137.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_6_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_6_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_6_multi_head_att_value_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_6_multi_head_att_value_fc.b_0_moment2_0'], ParamOut=['encoder_layer_6_multi_head_att_value_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_6_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_6_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_138.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_6_multi_head_att_value_fc.b_0_moment1_0'], Moment2=['encoder_layer_6_multi_head_att_value_fc.b_0_moment2_0'], Param=['encoder_layer_6_multi_head_att_value_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_138/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_value_fc.b_0', 'elementwise_mul_138.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_6_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_6_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_6_multi_head_att_value_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_6_multi_head_att_value_fc.w_0_moment2_0'], ParamOut=['encoder_layer_6_multi_head_att_value_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_6_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_6_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_139.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_6_multi_head_att_value_fc.w_0_moment1_0'], Moment2=['encoder_layer_6_multi_head_att_value_fc.w_0_moment2_0'], Param=['encoder_layer_6_multi_head_att_value_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_139/, op_role = 2, op_role_var = ['encoder_layer_6_multi_head_att_value_fc.w_0', 'elementwise_mul_139.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_6_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_6_post_att_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_6_post_att_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_6_post_att_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_6_post_att_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_6_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_6_post_att_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_140.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_6_post_att_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_6_post_att_layer_norm_bias_moment2_0'], Param=['encoder_layer_6_post_att_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_140/, op_role = 2, op_role_var = ['encoder_layer_6_post_att_layer_norm_bias', 'elementwise_mul_140.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_6_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_6_post_att_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_6_post_att_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_6_post_att_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_6_post_att_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_6_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_6_post_att_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_141.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_6_post_att_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_6_post_att_layer_norm_scale_moment2_0'], Param=['encoder_layer_6_post_att_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_141/, op_role = 2, op_role_var = ['encoder_layer_6_post_att_layer_norm_scale', 'elementwise_mul_141.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_6_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_6_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_6_post_ffn_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_6_post_ffn_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_6_post_ffn_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_6_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_6_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_142.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_6_post_ffn_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_6_post_ffn_layer_norm_bias_moment2_0'], Param=['encoder_layer_6_post_ffn_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_142/, op_role = 2, op_role_var = ['encoder_layer_6_post_ffn_layer_norm_bias', 'elementwise_mul_142.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_6_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_6_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_6_post_ffn_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_6_post_ffn_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_6_post_ffn_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_6_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_6_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_143.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_6_post_ffn_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_6_post_ffn_layer_norm_scale_moment2_0'], Param=['encoder_layer_6_post_ffn_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_143/, op_role = 2, op_role_var = ['encoder_layer_6_post_ffn_layer_norm_scale', 'elementwise_mul_143.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_7_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_7_ffn_fc_0.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_7_ffn_fc_0.b_0_moment1_0'], Moment2Out=['encoder_layer_7_ffn_fc_0.b_0_moment2_0'], ParamOut=['encoder_layer_7_ffn_fc_0.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_7_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_7_ffn_fc_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_144.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_7_ffn_fc_0.b_0_moment1_0'], Moment2=['encoder_layer_7_ffn_fc_0.b_0_moment2_0'], Param=['encoder_layer_7_ffn_fc_0.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_144/, op_role = 2, op_role_var = ['encoder_layer_7_ffn_fc_0.b_0', 'elementwise_mul_144.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_7_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_7_ffn_fc_0.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_7_ffn_fc_0.w_0_moment1_0'], Moment2Out=['encoder_layer_7_ffn_fc_0.w_0_moment2_0'], ParamOut=['encoder_layer_7_ffn_fc_0.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_7_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_7_ffn_fc_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_145.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_7_ffn_fc_0.w_0_moment1_0'], Moment2=['encoder_layer_7_ffn_fc_0.w_0_moment2_0'], Param=['encoder_layer_7_ffn_fc_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_145/, op_role = 2, op_role_var = ['encoder_layer_7_ffn_fc_0.w_0', 'elementwise_mul_145.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_7_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_7_ffn_fc_1.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_7_ffn_fc_1.b_0_moment1_0'], Moment2Out=['encoder_layer_7_ffn_fc_1.b_0_moment2_0'], ParamOut=['encoder_layer_7_ffn_fc_1.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_7_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_7_ffn_fc_1.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_146.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_7_ffn_fc_1.b_0_moment1_0'], Moment2=['encoder_layer_7_ffn_fc_1.b_0_moment2_0'], Param=['encoder_layer_7_ffn_fc_1.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_146/, op_role = 2, op_role_var = ['encoder_layer_7_ffn_fc_1.b_0', 'elementwise_mul_146.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_7_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_7_ffn_fc_1.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_7_ffn_fc_1.w_0_moment1_0'], Moment2Out=['encoder_layer_7_ffn_fc_1.w_0_moment2_0'], ParamOut=['encoder_layer_7_ffn_fc_1.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_7_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_7_ffn_fc_1.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_147.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_7_ffn_fc_1.w_0_moment1_0'], Moment2=['encoder_layer_7_ffn_fc_1.w_0_moment2_0'], Param=['encoder_layer_7_ffn_fc_1.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_147/, op_role = 2, op_role_var = ['encoder_layer_7_ffn_fc_1.w_0', 'elementwise_mul_147.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_7_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_7_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_7_multi_head_att_key_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_7_multi_head_att_key_fc.b_0_moment2_0'], ParamOut=['encoder_layer_7_multi_head_att_key_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_7_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_7_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_148.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_7_multi_head_att_key_fc.b_0_moment1_0'], Moment2=['encoder_layer_7_multi_head_att_key_fc.b_0_moment2_0'], Param=['encoder_layer_7_multi_head_att_key_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_148/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_key_fc.b_0', 'elementwise_mul_148.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_7_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_7_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_7_multi_head_att_key_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_7_multi_head_att_key_fc.w_0_moment2_0'], ParamOut=['encoder_layer_7_multi_head_att_key_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_7_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_7_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_149.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_7_multi_head_att_key_fc.w_0_moment1_0'], Moment2=['encoder_layer_7_multi_head_att_key_fc.w_0_moment2_0'], Param=['encoder_layer_7_multi_head_att_key_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_149/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_key_fc.w_0', 'elementwise_mul_149.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_7_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_7_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_7_multi_head_att_output_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_7_multi_head_att_output_fc.b_0_moment2_0'], ParamOut=['encoder_layer_7_multi_head_att_output_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_7_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_7_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_150.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_7_multi_head_att_output_fc.b_0_moment1_0'], Moment2=['encoder_layer_7_multi_head_att_output_fc.b_0_moment2_0'], Param=['encoder_layer_7_multi_head_att_output_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_150/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_output_fc.b_0', 'elementwise_mul_150.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_7_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_7_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_7_multi_head_att_output_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_7_multi_head_att_output_fc.w_0_moment2_0'], ParamOut=['encoder_layer_7_multi_head_att_output_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_7_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_7_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_151.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_7_multi_head_att_output_fc.w_0_moment1_0'], Moment2=['encoder_layer_7_multi_head_att_output_fc.w_0_moment2_0'], Param=['encoder_layer_7_multi_head_att_output_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_151/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_output_fc.w_0', 'elementwise_mul_151.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_7_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_7_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_7_multi_head_att_query_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_7_multi_head_att_query_fc.b_0_moment2_0'], ParamOut=['encoder_layer_7_multi_head_att_query_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_7_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_7_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_152.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_7_multi_head_att_query_fc.b_0_moment1_0'], Moment2=['encoder_layer_7_multi_head_att_query_fc.b_0_moment2_0'], Param=['encoder_layer_7_multi_head_att_query_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_152/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_query_fc.b_0', 'elementwise_mul_152.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_7_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_7_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_7_multi_head_att_query_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_7_multi_head_att_query_fc.w_0_moment2_0'], ParamOut=['encoder_layer_7_multi_head_att_query_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_7_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_7_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_153.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_7_multi_head_att_query_fc.w_0_moment1_0'], Moment2=['encoder_layer_7_multi_head_att_query_fc.w_0_moment2_0'], Param=['encoder_layer_7_multi_head_att_query_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_153/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_query_fc.w_0', 'elementwise_mul_153.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_7_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_7_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_7_multi_head_att_value_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_7_multi_head_att_value_fc.b_0_moment2_0'], ParamOut=['encoder_layer_7_multi_head_att_value_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_7_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_7_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_154.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_7_multi_head_att_value_fc.b_0_moment1_0'], Moment2=['encoder_layer_7_multi_head_att_value_fc.b_0_moment2_0'], Param=['encoder_layer_7_multi_head_att_value_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_154/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_value_fc.b_0', 'elementwise_mul_154.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_7_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_7_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_7_multi_head_att_value_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_7_multi_head_att_value_fc.w_0_moment2_0'], ParamOut=['encoder_layer_7_multi_head_att_value_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_7_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_7_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_155.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_7_multi_head_att_value_fc.w_0_moment1_0'], Moment2=['encoder_layer_7_multi_head_att_value_fc.w_0_moment2_0'], Param=['encoder_layer_7_multi_head_att_value_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_155/, op_role = 2, op_role_var = ['encoder_layer_7_multi_head_att_value_fc.w_0', 'elementwise_mul_155.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_7_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_7_post_att_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_7_post_att_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_7_post_att_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_7_post_att_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_7_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_7_post_att_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_156.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_7_post_att_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_7_post_att_layer_norm_bias_moment2_0'], Param=['encoder_layer_7_post_att_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_156/, op_role = 2, op_role_var = ['encoder_layer_7_post_att_layer_norm_bias', 'elementwise_mul_156.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_7_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_7_post_att_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_7_post_att_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_7_post_att_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_7_post_att_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_7_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_7_post_att_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_157.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_7_post_att_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_7_post_att_layer_norm_scale_moment2_0'], Param=['encoder_layer_7_post_att_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_157/, op_role = 2, op_role_var = ['encoder_layer_7_post_att_layer_norm_scale', 'elementwise_mul_157.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_7_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_7_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_7_post_ffn_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_7_post_ffn_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_7_post_ffn_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_7_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_7_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_158.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_7_post_ffn_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_7_post_ffn_layer_norm_bias_moment2_0'], Param=['encoder_layer_7_post_ffn_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_158/, op_role = 2, op_role_var = ['encoder_layer_7_post_ffn_layer_norm_bias', 'elementwise_mul_158.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_7_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_7_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_7_post_ffn_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_7_post_ffn_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_7_post_ffn_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_7_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_7_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_159.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_7_post_ffn_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_7_post_ffn_layer_norm_scale_moment2_0'], Param=['encoder_layer_7_post_ffn_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_159/, op_role = 2, op_role_var = ['encoder_layer_7_post_ffn_layer_norm_scale', 'elementwise_mul_159.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_8_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_8_ffn_fc_0.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_8_ffn_fc_0.b_0_moment1_0'], Moment2Out=['encoder_layer_8_ffn_fc_0.b_0_moment2_0'], ParamOut=['encoder_layer_8_ffn_fc_0.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_8_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_8_ffn_fc_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_160.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_8_ffn_fc_0.b_0_moment1_0'], Moment2=['encoder_layer_8_ffn_fc_0.b_0_moment2_0'], Param=['encoder_layer_8_ffn_fc_0.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_160/, op_role = 2, op_role_var = ['encoder_layer_8_ffn_fc_0.b_0', 'elementwise_mul_160.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_8_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_8_ffn_fc_0.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_8_ffn_fc_0.w_0_moment1_0'], Moment2Out=['encoder_layer_8_ffn_fc_0.w_0_moment2_0'], ParamOut=['encoder_layer_8_ffn_fc_0.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_8_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_8_ffn_fc_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_161.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_8_ffn_fc_0.w_0_moment1_0'], Moment2=['encoder_layer_8_ffn_fc_0.w_0_moment2_0'], Param=['encoder_layer_8_ffn_fc_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_161/, op_role = 2, op_role_var = ['encoder_layer_8_ffn_fc_0.w_0', 'elementwise_mul_161.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_8_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_8_ffn_fc_1.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_8_ffn_fc_1.b_0_moment1_0'], Moment2Out=['encoder_layer_8_ffn_fc_1.b_0_moment2_0'], ParamOut=['encoder_layer_8_ffn_fc_1.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_8_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_8_ffn_fc_1.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_162.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_8_ffn_fc_1.b_0_moment1_0'], Moment2=['encoder_layer_8_ffn_fc_1.b_0_moment2_0'], Param=['encoder_layer_8_ffn_fc_1.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_162/, op_role = 2, op_role_var = ['encoder_layer_8_ffn_fc_1.b_0', 'elementwise_mul_162.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_8_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_8_ffn_fc_1.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_8_ffn_fc_1.w_0_moment1_0'], Moment2Out=['encoder_layer_8_ffn_fc_1.w_0_moment2_0'], ParamOut=['encoder_layer_8_ffn_fc_1.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_8_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_8_ffn_fc_1.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_163.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_8_ffn_fc_1.w_0_moment1_0'], Moment2=['encoder_layer_8_ffn_fc_1.w_0_moment2_0'], Param=['encoder_layer_8_ffn_fc_1.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_163/, op_role = 2, op_role_var = ['encoder_layer_8_ffn_fc_1.w_0', 'elementwise_mul_163.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_8_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_8_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_8_multi_head_att_key_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_8_multi_head_att_key_fc.b_0_moment2_0'], ParamOut=['encoder_layer_8_multi_head_att_key_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_8_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_8_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_164.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_8_multi_head_att_key_fc.b_0_moment1_0'], Moment2=['encoder_layer_8_multi_head_att_key_fc.b_0_moment2_0'], Param=['encoder_layer_8_multi_head_att_key_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_164/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_key_fc.b_0', 'elementwise_mul_164.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_8_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_8_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_8_multi_head_att_key_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_8_multi_head_att_key_fc.w_0_moment2_0'], ParamOut=['encoder_layer_8_multi_head_att_key_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_8_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_8_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_165.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_8_multi_head_att_key_fc.w_0_moment1_0'], Moment2=['encoder_layer_8_multi_head_att_key_fc.w_0_moment2_0'], Param=['encoder_layer_8_multi_head_att_key_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_165/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_key_fc.w_0', 'elementwise_mul_165.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_8_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_8_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_8_multi_head_att_output_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_8_multi_head_att_output_fc.b_0_moment2_0'], ParamOut=['encoder_layer_8_multi_head_att_output_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_8_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_8_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_166.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_8_multi_head_att_output_fc.b_0_moment1_0'], Moment2=['encoder_layer_8_multi_head_att_output_fc.b_0_moment2_0'], Param=['encoder_layer_8_multi_head_att_output_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_166/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_output_fc.b_0', 'elementwise_mul_166.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_8_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_8_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_8_multi_head_att_output_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_8_multi_head_att_output_fc.w_0_moment2_0'], ParamOut=['encoder_layer_8_multi_head_att_output_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_8_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_8_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_167.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_8_multi_head_att_output_fc.w_0_moment1_0'], Moment2=['encoder_layer_8_multi_head_att_output_fc.w_0_moment2_0'], Param=['encoder_layer_8_multi_head_att_output_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_167/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_output_fc.w_0', 'elementwise_mul_167.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_8_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_8_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_8_multi_head_att_query_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_8_multi_head_att_query_fc.b_0_moment2_0'], ParamOut=['encoder_layer_8_multi_head_att_query_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_8_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_8_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_168.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_8_multi_head_att_query_fc.b_0_moment1_0'], Moment2=['encoder_layer_8_multi_head_att_query_fc.b_0_moment2_0'], Param=['encoder_layer_8_multi_head_att_query_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_168/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_query_fc.b_0', 'elementwise_mul_168.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_8_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_8_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_8_multi_head_att_query_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_8_multi_head_att_query_fc.w_0_moment2_0'], ParamOut=['encoder_layer_8_multi_head_att_query_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_8_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_8_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_169.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_8_multi_head_att_query_fc.w_0_moment1_0'], Moment2=['encoder_layer_8_multi_head_att_query_fc.w_0_moment2_0'], Param=['encoder_layer_8_multi_head_att_query_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_169/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_query_fc.w_0', 'elementwise_mul_169.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_8_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_8_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_8_multi_head_att_value_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_8_multi_head_att_value_fc.b_0_moment2_0'], ParamOut=['encoder_layer_8_multi_head_att_value_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_8_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_8_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_170.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_8_multi_head_att_value_fc.b_0_moment1_0'], Moment2=['encoder_layer_8_multi_head_att_value_fc.b_0_moment2_0'], Param=['encoder_layer_8_multi_head_att_value_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_170/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_value_fc.b_0', 'elementwise_mul_170.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_8_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_8_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_8_multi_head_att_value_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_8_multi_head_att_value_fc.w_0_moment2_0'], ParamOut=['encoder_layer_8_multi_head_att_value_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_8_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_8_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_171.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_8_multi_head_att_value_fc.w_0_moment1_0'], Moment2=['encoder_layer_8_multi_head_att_value_fc.w_0_moment2_0'], Param=['encoder_layer_8_multi_head_att_value_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_171/, op_role = 2, op_role_var = ['encoder_layer_8_multi_head_att_value_fc.w_0', 'elementwise_mul_171.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_8_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_8_post_att_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_8_post_att_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_8_post_att_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_8_post_att_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_8_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_8_post_att_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_172.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_8_post_att_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_8_post_att_layer_norm_bias_moment2_0'], Param=['encoder_layer_8_post_att_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_172/, op_role = 2, op_role_var = ['encoder_layer_8_post_att_layer_norm_bias', 'elementwise_mul_172.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_8_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_8_post_att_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_8_post_att_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_8_post_att_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_8_post_att_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_8_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_8_post_att_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_173.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_8_post_att_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_8_post_att_layer_norm_scale_moment2_0'], Param=['encoder_layer_8_post_att_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_173/, op_role = 2, op_role_var = ['encoder_layer_8_post_att_layer_norm_scale', 'elementwise_mul_173.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_8_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_8_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_8_post_ffn_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_8_post_ffn_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_8_post_ffn_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_8_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_8_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_174.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_8_post_ffn_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_8_post_ffn_layer_norm_bias_moment2_0'], Param=['encoder_layer_8_post_ffn_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_174/, op_role = 2, op_role_var = ['encoder_layer_8_post_ffn_layer_norm_bias', 'elementwise_mul_174.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_8_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_8_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_8_post_ffn_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_8_post_ffn_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_8_post_ffn_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_8_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_8_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_175.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_8_post_ffn_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_8_post_ffn_layer_norm_scale_moment2_0'], Param=['encoder_layer_8_post_ffn_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_175/, op_role = 2, op_role_var = ['encoder_layer_8_post_ffn_layer_norm_scale', 'elementwise_mul_175.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_9_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_9_ffn_fc_0.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_9_ffn_fc_0.b_0_moment1_0'], Moment2Out=['encoder_layer_9_ffn_fc_0.b_0_moment2_0'], ParamOut=['encoder_layer_9_ffn_fc_0.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_9_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_9_ffn_fc_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_176.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_9_ffn_fc_0.b_0_moment1_0'], Moment2=['encoder_layer_9_ffn_fc_0.b_0_moment2_0'], Param=['encoder_layer_9_ffn_fc_0.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_176/, op_role = 2, op_role_var = ['encoder_layer_9_ffn_fc_0.b_0', 'elementwise_mul_176.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_9_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_9_ffn_fc_0.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_9_ffn_fc_0.w_0_moment1_0'], Moment2Out=['encoder_layer_9_ffn_fc_0.w_0_moment2_0'], ParamOut=['encoder_layer_9_ffn_fc_0.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_9_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_9_ffn_fc_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_177.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_9_ffn_fc_0.w_0_moment1_0'], Moment2=['encoder_layer_9_ffn_fc_0.w_0_moment2_0'], Param=['encoder_layer_9_ffn_fc_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_177/, op_role = 2, op_role_var = ['encoder_layer_9_ffn_fc_0.w_0', 'elementwise_mul_177.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_9_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_9_ffn_fc_1.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_9_ffn_fc_1.b_0_moment1_0'], Moment2Out=['encoder_layer_9_ffn_fc_1.b_0_moment2_0'], ParamOut=['encoder_layer_9_ffn_fc_1.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_9_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_9_ffn_fc_1.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_178.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_9_ffn_fc_1.b_0_moment1_0'], Moment2=['encoder_layer_9_ffn_fc_1.b_0_moment2_0'], Param=['encoder_layer_9_ffn_fc_1.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_178/, op_role = 2, op_role_var = ['encoder_layer_9_ffn_fc_1.b_0', 'elementwise_mul_178.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_9_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_9_ffn_fc_1.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_9_ffn_fc_1.w_0_moment1_0'], Moment2Out=['encoder_layer_9_ffn_fc_1.w_0_moment2_0'], ParamOut=['encoder_layer_9_ffn_fc_1.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_9_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_9_ffn_fc_1.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_179.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_9_ffn_fc_1.w_0_moment1_0'], Moment2=['encoder_layer_9_ffn_fc_1.w_0_moment2_0'], Param=['encoder_layer_9_ffn_fc_1.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_179/, op_role = 2, op_role_var = ['encoder_layer_9_ffn_fc_1.w_0', 'elementwise_mul_179.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_9_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_9_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_9_multi_head_att_key_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_9_multi_head_att_key_fc.b_0_moment2_0'], ParamOut=['encoder_layer_9_multi_head_att_key_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_9_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_9_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_180.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_9_multi_head_att_key_fc.b_0_moment1_0'], Moment2=['encoder_layer_9_multi_head_att_key_fc.b_0_moment2_0'], Param=['encoder_layer_9_multi_head_att_key_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_180/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_key_fc.b_0', 'elementwise_mul_180.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_9_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_9_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_9_multi_head_att_key_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_9_multi_head_att_key_fc.w_0_moment2_0'], ParamOut=['encoder_layer_9_multi_head_att_key_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_9_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_9_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_181.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_9_multi_head_att_key_fc.w_0_moment1_0'], Moment2=['encoder_layer_9_multi_head_att_key_fc.w_0_moment2_0'], Param=['encoder_layer_9_multi_head_att_key_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_181/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_key_fc.w_0', 'elementwise_mul_181.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_9_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_9_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_9_multi_head_att_output_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_9_multi_head_att_output_fc.b_0_moment2_0'], ParamOut=['encoder_layer_9_multi_head_att_output_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_9_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_9_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_182.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_9_multi_head_att_output_fc.b_0_moment1_0'], Moment2=['encoder_layer_9_multi_head_att_output_fc.b_0_moment2_0'], Param=['encoder_layer_9_multi_head_att_output_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_182/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_output_fc.b_0', 'elementwise_mul_182.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_9_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_9_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_9_multi_head_att_output_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_9_multi_head_att_output_fc.w_0_moment2_0'], ParamOut=['encoder_layer_9_multi_head_att_output_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_9_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_9_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_183.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_9_multi_head_att_output_fc.w_0_moment1_0'], Moment2=['encoder_layer_9_multi_head_att_output_fc.w_0_moment2_0'], Param=['encoder_layer_9_multi_head_att_output_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_183/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_output_fc.w_0', 'elementwise_mul_183.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_9_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_9_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_9_multi_head_att_query_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_9_multi_head_att_query_fc.b_0_moment2_0'], ParamOut=['encoder_layer_9_multi_head_att_query_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_9_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_9_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_184.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_9_multi_head_att_query_fc.b_0_moment1_0'], Moment2=['encoder_layer_9_multi_head_att_query_fc.b_0_moment2_0'], Param=['encoder_layer_9_multi_head_att_query_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_184/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_query_fc.b_0', 'elementwise_mul_184.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_9_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_9_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_9_multi_head_att_query_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_9_multi_head_att_query_fc.w_0_moment2_0'], ParamOut=['encoder_layer_9_multi_head_att_query_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_9_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_9_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_185.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_9_multi_head_att_query_fc.w_0_moment1_0'], Moment2=['encoder_layer_9_multi_head_att_query_fc.w_0_moment2_0'], Param=['encoder_layer_9_multi_head_att_query_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_185/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_query_fc.w_0', 'elementwise_mul_185.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_9_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_9_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_9_multi_head_att_value_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_9_multi_head_att_value_fc.b_0_moment2_0'], ParamOut=['encoder_layer_9_multi_head_att_value_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_9_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_9_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_186.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_9_multi_head_att_value_fc.b_0_moment1_0'], Moment2=['encoder_layer_9_multi_head_att_value_fc.b_0_moment2_0'], Param=['encoder_layer_9_multi_head_att_value_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_186/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_value_fc.b_0', 'elementwise_mul_186.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_9_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_9_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_9_multi_head_att_value_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_9_multi_head_att_value_fc.w_0_moment2_0'], ParamOut=['encoder_layer_9_multi_head_att_value_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_9_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_9_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_187.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_9_multi_head_att_value_fc.w_0_moment1_0'], Moment2=['encoder_layer_9_multi_head_att_value_fc.w_0_moment2_0'], Param=['encoder_layer_9_multi_head_att_value_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_187/, op_role = 2, op_role_var = ['encoder_layer_9_multi_head_att_value_fc.w_0', 'elementwise_mul_187.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_9_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_9_post_att_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_9_post_att_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_9_post_att_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_9_post_att_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_9_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_9_post_att_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_188.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_9_post_att_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_9_post_att_layer_norm_bias_moment2_0'], Param=['encoder_layer_9_post_att_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_188/, op_role = 2, op_role_var = ['encoder_layer_9_post_att_layer_norm_bias', 'elementwise_mul_188.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_9_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_9_post_att_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_9_post_att_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_9_post_att_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_9_post_att_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_9_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_9_post_att_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_189.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_9_post_att_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_9_post_att_layer_norm_scale_moment2_0'], Param=['encoder_layer_9_post_att_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_189/, op_role = 2, op_role_var = ['encoder_layer_9_post_att_layer_norm_scale', 'elementwise_mul_189.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_9_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_9_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_9_post_ffn_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_9_post_ffn_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_9_post_ffn_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_9_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_9_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_190.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_9_post_ffn_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_9_post_ffn_layer_norm_bias_moment2_0'], Param=['encoder_layer_9_post_ffn_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_190/, op_role = 2, op_role_var = ['encoder_layer_9_post_ffn_layer_norm_bias', 'elementwise_mul_190.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_9_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_9_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_9_post_ffn_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_9_post_ffn_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_9_post_ffn_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_9_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_9_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_191.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_9_post_ffn_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_9_post_ffn_layer_norm_scale_moment2_0'], Param=['encoder_layer_9_post_ffn_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_191/, op_role = 2, op_role_var = ['encoder_layer_9_post_ffn_layer_norm_scale', 'elementwise_mul_191.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['mask_lm_out_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['mask_lm_out_fc.b_0_beta2_pow_acc_0'], Moment1Out=['mask_lm_out_fc.b_0_moment1_0'], Moment2Out=['mask_lm_out_fc.b_0_moment2_0'], ParamOut=['mask_lm_out_fc.b_0']} = adam(inputs={Beta1Pow=['mask_lm_out_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['mask_lm_out_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_192.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['mask_lm_out_fc.b_0_moment1_0'], Moment2=['mask_lm_out_fc.b_0_moment2_0'], Param=['mask_lm_out_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_192/, op_role = 2, op_role_var = ['mask_lm_out_fc.b_0', 'elementwise_mul_192.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['mask_lm_out_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['mask_lm_out_fc.w_0_beta2_pow_acc_0'], Moment1Out=['mask_lm_out_fc.w_0_moment1_0'], Moment2Out=['mask_lm_out_fc.w_0_moment2_0'], ParamOut=['mask_lm_out_fc.w_0']} = adam(inputs={Beta1Pow=['mask_lm_out_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['mask_lm_out_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_193.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['mask_lm_out_fc.w_0_moment1_0'], Moment2=['mask_lm_out_fc.w_0_moment2_0'], Param=['mask_lm_out_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_193/, op_role = 2, op_role_var = ['mask_lm_out_fc.w_0', 'elementwise_mul_193.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['mask_lm_trans_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['mask_lm_trans_fc.b_0_beta2_pow_acc_0'], Moment1Out=['mask_lm_trans_fc.b_0_moment1_0'], Moment2Out=['mask_lm_trans_fc.b_0_moment2_0'], ParamOut=['mask_lm_trans_fc.b_0']} = adam(inputs={Beta1Pow=['mask_lm_trans_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['mask_lm_trans_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_194.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['mask_lm_trans_fc.b_0_moment1_0'], Moment2=['mask_lm_trans_fc.b_0_moment2_0'], Param=['mask_lm_trans_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_194/, op_role = 2, op_role_var = ['mask_lm_trans_fc.b_0', 'elementwise_mul_194.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['mask_lm_trans_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['mask_lm_trans_fc.w_0_beta2_pow_acc_0'], Moment1Out=['mask_lm_trans_fc.w_0_moment1_0'], Moment2Out=['mask_lm_trans_fc.w_0_moment2_0'], ParamOut=['mask_lm_trans_fc.w_0']} = adam(inputs={Beta1Pow=['mask_lm_trans_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['mask_lm_trans_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_195.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['mask_lm_trans_fc.w_0_moment1_0'], Moment2=['mask_lm_trans_fc.w_0_moment2_0'], Param=['mask_lm_trans_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_195/, op_role = 2, op_role_var = ['mask_lm_trans_fc.w_0', 'elementwise_mul_195.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['mask_lm_trans_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['mask_lm_trans_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['mask_lm_trans_layer_norm_bias_moment1_0'], Moment2Out=['mask_lm_trans_layer_norm_bias_moment2_0'], ParamOut=['mask_lm_trans_layer_norm_bias']} = adam(inputs={Beta1Pow=['mask_lm_trans_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['mask_lm_trans_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_196.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['mask_lm_trans_layer_norm_bias_moment1_0'], Moment2=['mask_lm_trans_layer_norm_bias_moment2_0'], Param=['mask_lm_trans_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_196/, op_role = 2, op_role_var = ['mask_lm_trans_layer_norm_bias', 'elementwise_mul_196.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['mask_lm_trans_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['mask_lm_trans_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['mask_lm_trans_layer_norm_scale_moment1_0'], Moment2Out=['mask_lm_trans_layer_norm_scale_moment2_0'], ParamOut=['mask_lm_trans_layer_norm_scale']} = adam(inputs={Beta1Pow=['mask_lm_trans_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['mask_lm_trans_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_197.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['mask_lm_trans_layer_norm_scale_moment1_0'], Moment2=['mask_lm_trans_layer_norm_scale_moment2_0'], Param=['mask_lm_trans_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_197/, op_role = 2, op_role_var = ['mask_lm_trans_layer_norm_scale', 'elementwise_mul_197.tmp_0'], use_global_beta_pow = False)
}
{ // block 1
    var tmp_120 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var tmp_121 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var tmp_122 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)

    {Out=['tmp_120']} = fill_constant(inputs={}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [1], str_value = , value = 10000.0)
    {Out=['tmp_121']} = elementwise_div(inputs={X=['cast_2.tmp_0'], Y=['tmp_120']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 16, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_122']} = scale(inputs={ScaleTensor=[], X=['tmp_121']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 16, op_role_var = [], scale = 9.999999747378752e-05, use_mkldnn = False)
    {Out=['scheduled_learning_rate']} = assign(inputs={X=['tmp_122']}, op_device = , op_namescope = /, op_role = 16, op_role_var = [])
}
{ // block 2
    var cast_3.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var fill_constant_5.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var elementwise_min_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var tmp_123 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var tmp_124 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var tmp_125 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var tmp_126 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var tmp_127 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var tmp_128 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var tmp_129 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)

    {Out=['cast_3.tmp_0']} = cast(inputs={X=['@LR_DECAY_COUNTER@']}, in_dtype = 3, op_device = , op_namescope = /, op_role = 16, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['fill_constant_5.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [1], str_value = 2000.0, value = 2000.0)
    {Out=['elementwise_min_0.tmp_0']} = elementwise_min(inputs={X=['cast_3.tmp_0'], Y=['fill_constant_5.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 16, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_123']} = fill_constant(inputs={}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [1], str_value = , value = 2000.0)
    {Out=['tmp_124']} = elementwise_div(inputs={X=['elementwise_min_0.tmp_0'], Y=['tmp_123']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 16, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_125']} = scale(inputs={ScaleTensor=[], X=['tmp_124']}, bias = 1.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 16, op_role_var = [], scale = -1.0, use_mkldnn = False)
    {Out=['tmp_126']} = fill_constant(inputs={}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [1], str_value = , value = 1.0)
    {Out=['tmp_127']} = elementwise_pow(inputs={X=['tmp_125'], Y=['tmp_126']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 16, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_128']} = scale(inputs={ScaleTensor=[], X=['tmp_127']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 16, op_role_var = [], scale = 9.999999747378752e-05, use_mkldnn = False)
    {Out=['tmp_129']} = scale(inputs={ScaleTensor=[], X=['tmp_128']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 16, op_role_var = [], scale = 1.0, use_mkldnn = False)
    {Out=['scheduled_learning_rate']} = assign(inputs={X=['tmp_129']}, op_device = , op_namescope = /, op_role = 16, op_role_var = [])
}
