{ // block 0
    persist var @LR_DECAY_COUNTER@ : LOD_TENSOR.shape(1,).dtype(int64).stop_gradient(True)
    persist var create_py_reader_0 : READER)
    persist var double_buffer_0 : READER)
    var src_ids : LOD_TENSOR.shape(-1, 512).dtype(int32).stop_gradient(True)
    var sent_ids : LOD_TENSOR.shape(-1, 512).dtype(int32).stop_gradient(True)
    var pos_ids : LOD_TENSOR.shape(-1, 512).dtype(int32).stop_gradient(True)
    var mask_label : LOD_TENSOR.shape(-1, 1).dtype(int32).stop_gradient(True)
    var mask_pos : LOD_TENSOR.shape(-1, 1).dtype(int32).stop_gradient(True)
    var fill_constant_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    var equal_0.tmp_0 : LOD_TENSOR.shape(-1, 512).dtype(bool).stop_gradient(True)
    var logical_not_0.tmp_0 : LOD_TENSOR.shape(-1, 512).dtype(bool).stop_gradient(False)
    var cast_0.tmp_0 : LOD_TENSOR.shape(-1, 512).dtype(float32).stop_gradient(False)
    var unsqueeze2_0.tmp_0 : LOD_TENSOR.shape(-1, 512, 1).dtype(float32).stop_gradient(True)
    var unsqueeze2_0.tmp_1 : LOD_TENSOR.shape(0, -1, 512).dtype(float32).stop_gradient(False)
    persist var word_embedding : LOD_TENSOR.shape(30000, 768).dtype(float32).stop_gradient(False)
    var embedding_1.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    persist var pos_embedding : LOD_TENSOR.shape(512, 768).dtype(float32).stop_gradient(False)
    var embedding_3.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    persist var sent_embedding : LOD_TENSOR.shape(4, 768).dtype(float32).stop_gradient(False)
    var embedding_5.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_1.cast_fp16 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var pre_encoder_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var pre_encoder_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_0.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_0.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_0.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var unsqueeze2_0.tmp_0.cast_fp16 : LOD_TENSOR.shape(-1, 512, 1).dtype(float16).stop_gradient(True)
    var matmul_v2_0.tmp_0 : LOD_TENSOR.shape(-1, 512, 512).dtype(float16).stop_gradient(False)
    var scale_0.tmp_0 : LOD_TENSOR.shape(-1, 512, 512).dtype(float16).stop_gradient(False)
    persist var stack_0.tmp_0 : LOD_TENSOR.shape(1, 12, 512, 512).dtype(float16).stop_gradient(True)
    persist var encoder_layer_0_multi_head_att_query_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_query_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_0.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_query_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_query_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_0.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_key_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_key_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_1.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_key_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_key_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_1.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_value_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_value_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_2.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_value_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_value_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_2.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var scale_1.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_1.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var tmp_2 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_2.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_1 : LOD_TENSOR.shape(0, -1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var reshape2_3.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_output_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_output_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_3.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_output_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_output_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_3.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_3 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_0_post_att_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_att_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_1.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_1.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_1.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_0.w_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_0_ffn_fc_0.w_0.cast_fp16 : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var fc_4.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_0.b_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_0_ffn_fc_0.b_0.cast_fp16 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var fc_4.tmp_1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_4.tmp_1.cast_fp32 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_0.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_4 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_5 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_6 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_0.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_7 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_8 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_9 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_1.w_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_0_ffn_fc_1.w_0.cast_fp16 : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_9.cast_fp16 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_5.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_1.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_0_ffn_fc_1.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_5.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_10 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_0_post_ffn_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_ffn_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_2.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_2.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_2.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_query_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_query_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_6.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_query_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_query_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_6.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_key_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_key_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_7.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_key_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_key_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_7.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_value_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_value_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_8.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_value_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_value_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_8.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_4.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_5.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_6.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var scale_2.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_3.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var tmp_11 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_4.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_1 : LOD_TENSOR.shape(0, -1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var reshape2_7.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_output_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_output_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_9.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_output_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_output_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_9.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_12 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_1_post_att_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_att_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_3.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_3.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_3.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_0.w_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_1_ffn_fc_0.w_0.cast_fp16 : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var fc_10.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_0.b_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_ffn_fc_0.b_0.cast_fp16 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var fc_10.tmp_1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_10.tmp_1.cast_fp32 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_1.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_13 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_14 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_15 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_1.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_16 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_17 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_18 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_1.w_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_1_ffn_fc_1.w_0.cast_fp16 : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_18.cast_fp16 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_11.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_1.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_ffn_fc_1.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_11.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_19 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_1_post_ffn_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_ffn_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_4.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_4.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_4.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_query_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_query_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_12.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_query_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_query_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_12.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_key_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_key_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_13.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_key_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_key_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_13.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_value_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_value_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_14.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_value_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_value_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_14.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_8.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_9.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_10.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_10.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_10.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var scale_3.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_5.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var tmp_20 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var softmax_2.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_6.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_11.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_11.tmp_1 : LOD_TENSOR.shape(0, -1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var reshape2_11.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_output_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_output_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_15.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_output_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_output_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_15.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_21 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_2_post_att_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_att_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_5.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_5.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_5.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_0.w_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_2_ffn_fc_0.w_0.cast_fp16 : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var fc_16.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_0.b_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_ffn_fc_0.b_0.cast_fp16 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var fc_16.tmp_1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_16.tmp_1.cast_fp32 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_2.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_22 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_23 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_24 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_2.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_25 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_26 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_27 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_1.w_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_2_ffn_fc_1.w_0.cast_fp16 : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_27.cast_fp16 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_17.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_1.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_ffn_fc_1.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_17.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_28 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_2_post_ffn_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_ffn_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_6.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_6.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_6.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_query_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_query_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_18.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_query_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_query_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_18.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_key_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_key_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_19.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_key_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_key_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_19.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_value_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_value_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_20.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_value_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_value_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_20.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_12.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_12.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_12.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_13.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_13.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_13.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_14.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_14.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_14.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var scale_4.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_7.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var tmp_29 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var softmax_3.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_8.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_15.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_15.tmp_1 : LOD_TENSOR.shape(0, -1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var reshape2_15.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_output_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_output_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_21.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_output_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_output_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_21.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_30 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_3_post_att_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_att_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_7.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_7.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_7.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_0.w_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_3_ffn_fc_0.w_0.cast_fp16 : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var fc_22.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_0.b_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_ffn_fc_0.b_0.cast_fp16 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var fc_22.tmp_1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_22.tmp_1.cast_fp32 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_3.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_31 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_32 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_33 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_3.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_34 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_35 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_36 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_1.w_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_3_ffn_fc_1.w_0.cast_fp16 : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_36.cast_fp16 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_23.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_1.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_ffn_fc_1.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_23.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_37 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_3_post_ffn_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_ffn_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_8.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_8.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_8.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_query_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_query_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_24.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_query_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_query_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_24.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_key_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_key_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_25.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_key_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_key_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_25.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_value_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_value_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_26.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_value_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_value_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_26.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_16.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_16.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_16.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_17.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_17.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_17.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_18.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_18.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_18.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var scale_5.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_9.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var tmp_38 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var softmax_4.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_10.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_19.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_19.tmp_1 : LOD_TENSOR.shape(0, -1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var reshape2_19.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_output_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_output_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_27.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_output_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_output_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_27.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_39 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_4_post_att_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_att_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_9.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_9.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_9.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_0.w_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_4_ffn_fc_0.w_0.cast_fp16 : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var fc_28.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_0.b_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_ffn_fc_0.b_0.cast_fp16 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var fc_28.tmp_1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_28.tmp_1.cast_fp32 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_4.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_40 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_41 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_42 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_4.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_43 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_44 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_45 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_1.w_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_4_ffn_fc_1.w_0.cast_fp16 : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_45.cast_fp16 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_29.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_1.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_ffn_fc_1.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_29.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_46 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_4_post_ffn_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_ffn_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_10.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_10.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_10.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_query_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_query_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_30.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_query_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_query_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_30.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_key_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_key_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_31.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_key_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_key_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_31.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_value_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_value_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_32.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_value_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_value_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_32.tmp_1 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_20.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_20.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_20.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_21.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_21.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_21.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_22.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_22.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_22.tmp_1 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var scale_6.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_11.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var tmp_47 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var softmax_5.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_12.tmp_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_23.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_23.tmp_1 : LOD_TENSOR.shape(0, -1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var reshape2_23.tmp_0 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_output_fc.w_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_output_fc.w_0.cast_fp16 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_33.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_output_fc.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_output_fc.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_33.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_48 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_5_post_att_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_att_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_11.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_11.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_11.tmp_2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_0.w_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_5_ffn_fc_0.w_0.cast_fp16 : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var fc_34.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_0.b_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_ffn_fc_0.b_0.cast_fp16 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var fc_34.tmp_1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_34.tmp_1.cast_fp32 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_5.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_49 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_50 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_51 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_5.tmp_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_52 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_53 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_54 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_1.w_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_5_ffn_fc_1.w_0.cast_fp16 : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_54.cast_fp16 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_35.tmp_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_1.b_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_ffn_fc_1.b_0.cast_fp16 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_35.tmp_1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_55 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    persist var encoder_layer_5_post_ffn_layer_norm_bias : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_ffn_layer_norm_scale : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var layer_norm_12.tmp_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_12.tmp_1 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_12.tmp_2 : LOD_TENSOR.shape(1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_116 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var cast_2.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var tmp_118 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var tmp_119 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    var logical_not_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    persist var scheduled_learning_rate : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var _generated_var_0 : STEP_SCOPES)
    var _generated_var_1 : STEP_SCOPES)
    var float_status : LOD_TENSOR.shape(8,).dtype(float32).stop_gradient(True)
    var layer_norm_12.tmp_2@GRAD : LOD_TENSOR.shape(1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_5_post_ffn_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_post_ffn_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_55@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_35.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_11.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_35.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_5_ffn_fc_1.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var tmp_54.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_5_ffn_fc_1.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_54@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_34.tmp_1.cast_fp32@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_53@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_52@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_51@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_50@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_34.tmp_1.cast_fp32@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_49@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_34.tmp_1.cast_fp32@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_34.tmp_1.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_34.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_34.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_5_ffn_fc_0.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_11.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_5_ffn_fc_0.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_11.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_5_post_att_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_post_att_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_48@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_33.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_10.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_33.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_5_multi_head_att_output_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var transpose_23.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var encoder_layer_5_multi_head_att_output_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var matmul_v2_12.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var softmax_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var transpose_22.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var tmp_47@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var scale_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_21.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_20.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var fc_32.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_31.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_30.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_32.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_5_multi_head_att_value_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_10.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_5_multi_head_att_value_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_31.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_5_multi_head_att_key_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_10.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_5_multi_head_att_key_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_30.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_5_multi_head_att_query_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_10.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_5_multi_head_att_query_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var layer_norm_10.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_4_post_ffn_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_post_ffn_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_46@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_29.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_9.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_29.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_4_ffn_fc_1.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var tmp_45.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_4_ffn_fc_1.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_45@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_28.tmp_1.cast_fp32@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_44@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_43@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_42@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_41@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_28.tmp_1.cast_fp32@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_40@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_28.tmp_1.cast_fp32@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_28.tmp_1.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_28.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_28.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_4_ffn_fc_0.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_9.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_4_ffn_fc_0.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_9.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_4_post_att_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_post_att_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_39@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_27.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_8.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_27.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_4_multi_head_att_output_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var transpose_19.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var encoder_layer_4_multi_head_att_output_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var matmul_v2_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var softmax_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var transpose_18.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var tmp_38@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var scale_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_17.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_16.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var fc_26.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_25.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_24.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_26.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_4_multi_head_att_value_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_8.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_4_multi_head_att_value_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_25.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_4_multi_head_att_key_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_8.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_4_multi_head_att_key_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_24.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_4_multi_head_att_query_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_8.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_4_multi_head_att_query_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var layer_norm_8.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_3_post_ffn_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_post_ffn_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_37@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_23.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_7.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_23.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_3_ffn_fc_1.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var tmp_36.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_3_ffn_fc_1.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_36@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_22.tmp_1.cast_fp32@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_35@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_34@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_33@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_32@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_22.tmp_1.cast_fp32@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_31@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_22.tmp_1.cast_fp32@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_22.tmp_1.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_22.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_22.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_3_ffn_fc_0.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_7.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_3_ffn_fc_0.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_7.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_3_post_att_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_post_att_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_30@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_21.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_6.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_21.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_3_multi_head_att_output_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var transpose_15.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var encoder_layer_3_multi_head_att_output_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var matmul_v2_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var softmax_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var transpose_14.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var tmp_29@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var scale_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_13.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_12.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var fc_20.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_19.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_18.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_20.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_3_multi_head_att_value_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_6.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_3_multi_head_att_value_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_19.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_3_multi_head_att_key_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_6.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_3_multi_head_att_key_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_18.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_3_multi_head_att_query_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_6.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_3_multi_head_att_query_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var layer_norm_6.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_2_post_ffn_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_post_ffn_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_28@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_17.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_5.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_17.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_2_ffn_fc_1.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var tmp_27.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_2_ffn_fc_1.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_27@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_16.tmp_1.cast_fp32@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_26@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_25@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_24@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_23@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_16.tmp_1.cast_fp32@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_22@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_16.tmp_1.cast_fp32@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_16.tmp_1.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_16.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_16.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_2_ffn_fc_0.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_5.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_2_ffn_fc_0.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_5.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_2_post_att_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_post_att_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_21@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_15.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_4.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_15.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_2_multi_head_att_output_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var transpose_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var encoder_layer_2_multi_head_att_output_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var matmul_v2_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var softmax_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var transpose_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var tmp_20@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var scale_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var fc_14.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_13.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_12.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_14.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_2_multi_head_att_value_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_4.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_2_multi_head_att_value_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_13.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_2_multi_head_att_key_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_4.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_2_multi_head_att_key_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_12.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_2_multi_head_att_query_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_4.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_2_multi_head_att_query_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var layer_norm_4.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_1_post_ffn_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_post_ffn_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_19@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_11.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_3.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_11.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_1_ffn_fc_1.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var tmp_18.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_1_ffn_fc_1.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_18@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_10.tmp_1.cast_fp32@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_17@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_16@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_15@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_14@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_10.tmp_1.cast_fp32@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_13@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_10.tmp_1.cast_fp32@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_10.tmp_1.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_10.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_10.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_1_ffn_fc_0.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_3.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_1_ffn_fc_0.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_3.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_1_post_att_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_post_att_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_12@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_9.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_2.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_9.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_1_multi_head_att_output_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var transpose_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var encoder_layer_1_multi_head_att_output_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var matmul_v2_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var softmax_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var transpose_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var tmp_11@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var scale_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var fc_8.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_7.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_6.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_8.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_1_multi_head_att_value_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_2.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_1_multi_head_att_value_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_7.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_1_multi_head_att_key_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_2.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_1_multi_head_att_key_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_6.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_1_multi_head_att_query_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_2.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_1_multi_head_att_query_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var embedding_1.tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var embedding_3.tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var embedding_5.tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_1.subprog_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_1.cast_fp16.subprog_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_0.tmp_0.subprog_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_0.tmp_1.subprog_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_0.tmp_2.subprog_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_0_multi_head_att_query_fc.w_0.cast_fp16.subprog_0 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_0.tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_0_multi_head_att_query_fc.b_0.cast_fp16.subprog_0 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_0.tmp_1.subprog_0 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var encoder_layer_0_multi_head_att_key_fc.w_0.cast_fp16.subprog_0 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_1.tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_0_multi_head_att_key_fc.b_0.cast_fp16.subprog_0 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_1.tmp_1.subprog_0 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var encoder_layer_0_multi_head_att_value_fc.w_0.cast_fp16.subprog_0 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_2.tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_0_multi_head_att_value_fc.b_0.cast_fp16.subprog_0 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_2.tmp_1.subprog_0 : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_0.tmp_0.subprog_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_1.subprog_0 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_1.tmp_0.subprog_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_1.subprog_0 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var reshape2_2.tmp_0.subprog_0 : LOD_TENSOR.shape(0, -1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_1.subprog_0 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var scale_1.tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var matmul_v2_1.tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var tmp_2.subprog_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_2.tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_1.subprog_0 : LOD_TENSOR.shape(0, -1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var reshape2_3.tmp_0.subprog_0 : LOD_TENSOR.shape(0, -1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var encoder_layer_0_multi_head_att_output_fc.w_0.cast_fp16.subprog_0 : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_3.tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_0_multi_head_att_output_fc.b_0.cast_fp16.subprog_0 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_3.tmp_1.subprog_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_3.subprog_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_1.tmp_0.subprog_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_1.tmp_1.subprog_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_1.tmp_2.subprog_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_0_ffn_fc_0.w_0.cast_fp16.subprog_0 : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var fc_4.tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_0_ffn_fc_0.b_0.cast_fp16.subprog_0 : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var fc_4.tmp_1.subprog_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_4.tmp_1.cast_fp32.subprog_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_0.tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_4.subprog_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_5.subprog_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_6.subprog_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_0.tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_7.subprog_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_8.subprog_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_9.subprog_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_0_ffn_fc_1.w_0.cast_fp16.subprog_0 : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_9.cast_fp16.subprog_0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_5.tmp_0.subprog_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_0_ffn_fc_1.b_0.cast_fp16.subprog_0 : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var fc_5.tmp_1.subprog_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_10.subprog_0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_2.tmp_0.subprog_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_2.tmp_1.subprog_0 : LOD_TENSOR.shape(-512,).dtype(float32).stop_gradient(True)
    var layer_norm_2.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_0_post_ffn_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_0_post_ffn_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_10@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_5.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_1.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_0_ffn_fc_1.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var tmp_9.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_0_ffn_fc_1.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float16).stop_gradient(False)
    var tmp_9@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_4.tmp_1.cast_fp32@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_8@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_7@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tanh_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_6@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_5@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_4.tmp_1.cast_fp32@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var tmp_4@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var pow_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_4.tmp_1.cast_fp32@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_4.tmp_1.cast_fp32@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float32).stop_gradient(False)
    var fc_4.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var fc_4.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 3072).dtype(float16).stop_gradient(False)
    var encoder_layer_0_ffn_fc_0.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(3072,).dtype(float16).stop_gradient(False)
    var layer_norm_1.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_0_ffn_fc_0.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float16).stop_gradient(False)
    var layer_norm_1.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_0_post_att_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_0_post_att_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_3@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_3.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var layer_norm_0.tmp_2@GRAD@RENAME@block0@0 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_0_multi_head_att_output_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var transpose_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 12, 64).dtype(float16).stop_gradient(False)
    var encoder_layer_0_multi_head_att_output_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var matmul_v2_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var softmax_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var transpose_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var tmp_2@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var matmul_v2_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 512).dtype(float16).stop_gradient(False)
    var scale_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var transpose_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 12, 512, 64).dtype(float16).stop_gradient(False)
    var fc_2.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_1.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_0.tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var fc_2.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_0_multi_head_att_value_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_0.tmp_2@GRAD@RENAME@block0@1 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_0_multi_head_att_value_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_0_multi_head_att_key_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_0.tmp_2@GRAD@RENAME@block0@2 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_0_multi_head_att_key_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var fc_0.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_0_multi_head_att_query_fc.b_0.cast_fp16@GRAD : LOD_TENSOR.shape(768,).dtype(float16).stop_gradient(False)
    var layer_norm_0.tmp_2@GRAD@RENAME@block0@3 : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var encoder_layer_0_multi_head_att_query_fc.w_0.cast_fp16@GRAD : LOD_TENSOR.shape(768, 768).dtype(float16).stop_gradient(False)
    var layer_norm_0.tmp_2@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var pre_encoder_layer_norm_bias@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var pre_encoder_layer_norm_scale@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var tmp_1.cast_fp16@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float16).stop_gradient(False)
    var tmp_1@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var embedding_5.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var embedding_1.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var embedding_3.tmp_0@GRAD : LOD_TENSOR.shape(-1, 512, 768).dtype(float32).stop_gradient(False)
    var sent_embedding@GRAD : LOD_TENSOR.shape(4, 768).dtype(float32).stop_gradient(False)
    var pos_embedding@GRAD : LOD_TENSOR.shape(512, 768).dtype(float32).stop_gradient(False)
    var word_embedding@GRAD : LOD_TENSOR.shape(30000, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_query_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_query_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_key_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_key_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_value_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_value_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_output_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_output_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_0_ffn_fc_0.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_0_ffn_fc_0.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_0_ffn_fc_1.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_0_ffn_fc_1.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_query_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_query_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_key_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_key_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_value_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_value_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_output_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_output_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_ffn_fc_0.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_1_ffn_fc_0.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_ffn_fc_1.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_1_ffn_fc_1.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_query_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_query_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_key_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_key_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_value_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_value_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_output_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_output_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_ffn_fc_0.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_2_ffn_fc_0.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_ffn_fc_1.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_2_ffn_fc_1.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_query_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_query_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_key_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_key_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_value_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_value_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_output_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_output_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_ffn_fc_0.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_3_ffn_fc_0.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_ffn_fc_1.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_3_ffn_fc_1.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_query_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_query_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_key_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_key_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_value_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_value_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_output_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_output_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_ffn_fc_0.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_4_ffn_fc_0.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_ffn_fc_1.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_4_ffn_fc_1.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_query_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_query_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_key_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_key_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_value_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_value_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_output_fc.w_0@GRAD : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_output_fc.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_ffn_fc_0.w_0@GRAD : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_5_ffn_fc_0.b_0@GRAD : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_ffn_fc_1.w_0@GRAD : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_5_ffn_fc_1.b_0@GRAD : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var loss_scaling_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var find_infinite_scale.tmp_0 : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)
    persist var num_bad_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    persist var num_good_steps_0 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(True)
    var square_0.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var reduce_sum_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_1.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var reduce_sum_1.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_2.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_2.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_3.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_3.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_4.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_4.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_5.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_5.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_6.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_6.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_7.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_7.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_8.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_8.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_9.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_9.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_10.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_10.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_11.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_11.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_12.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_12.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_13.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_13.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_14.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_14.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_15.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_15.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_48.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var reduce_sum_48.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_49.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var reduce_sum_49.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_50.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_50.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_51.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_51.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_52.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_52.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_53.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_53.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_54.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_54.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_55.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_55.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_56.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_56.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_57.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_57.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_58.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_58.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_59.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_59.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_60.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_60.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_61.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_61.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_62.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_62.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_63.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_63.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_64.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var reduce_sum_64.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_65.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var reduce_sum_65.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_66.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_66.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_67.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_67.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_68.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_68.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_69.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_69.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_70.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_70.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_71.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_71.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_72.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_72.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_73.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_73.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_74.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_74.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_75.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_75.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_76.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_76.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_77.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_77.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_78.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_78.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_79.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_79.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_80.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var reduce_sum_80.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_81.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var reduce_sum_81.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_82.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_82.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_83.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_83.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_84.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_84.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_85.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_85.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_86.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_86.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_87.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_87.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_88.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_88.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_89.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_89.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_90.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_90.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_91.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_91.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_92.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_92.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_93.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_93.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_94.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_94.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_95.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_95.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_96.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var reduce_sum_96.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_97.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var reduce_sum_97.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_98.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_98.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_99.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_99.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_100.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_100.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_101.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_101.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_102.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_102.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_103.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_103.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_104.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_104.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_105.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_105.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_106.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_106.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_107.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_107.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_108.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_108.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_109.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_109.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_110.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_110.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_111.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_111.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_112.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var reduce_sum_112.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_113.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var reduce_sum_113.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_114.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_114.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_115.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_115.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_116.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_116.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_117.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_117.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_118.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_118.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_119.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_119.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_120.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_120.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_121.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_121.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_122.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_122.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_123.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_123.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_124.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_124.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_125.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_125.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_126.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_126.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_127.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_127.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_198.tmp_0 : LOD_TENSOR.shape(512, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_198.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_199.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_199.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_200.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var reduce_sum_200.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_201.tmp_0 : LOD_TENSOR.shape(4, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_201.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var square_202.tmp_0 : LOD_TENSOR.shape(30000, 768).dtype(float32).stop_gradient(False)
    var reduce_sum_202.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var sum_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var sqrt_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var fill_constant_7.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var elementwise_max_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var elementwise_div_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var elementwise_mul_0.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var elementwise_mul_1.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var elementwise_mul_2.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_3.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_4.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_5.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_6.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_7.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_8.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_9.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_10.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_11.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_12.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_13.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_14.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_15.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_48.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var elementwise_mul_49.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var elementwise_mul_50.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_51.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_52.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_53.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_54.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_55.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_56.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_57.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_58.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_59.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_60.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_61.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_62.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_63.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_64.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var elementwise_mul_65.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var elementwise_mul_66.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_67.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_68.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_69.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_70.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_71.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_72.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_73.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_74.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_75.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_76.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_77.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_78.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_79.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_80.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var elementwise_mul_81.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var elementwise_mul_82.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_83.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_84.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_85.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_86.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_87.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_88.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_89.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_90.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_91.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_92.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_93.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_94.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_95.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_96.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var elementwise_mul_97.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var elementwise_mul_98.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_99.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_100.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_101.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_102.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_103.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_104.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_105.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_106.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_107.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_108.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_109.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_110.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_111.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_112.tmp_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var elementwise_mul_113.tmp_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var elementwise_mul_114.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_115.tmp_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_116.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_117.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_118.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_119.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_120.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_121.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_122.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_123.tmp_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_124.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_125.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_126.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_127.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_198.tmp_0 : LOD_TENSOR.shape(512, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_199.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_200.tmp_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var elementwise_mul_201.tmp_0 : LOD_TENSOR.shape(4, 768).dtype(float32).stop_gradient(False)
    var elementwise_mul_202.tmp_0 : LOD_TENSOR.shape(30000, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_0.b_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_0.b_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_0.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_0.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_1.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_1.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_1.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_1.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_1.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_1.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_1.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_1.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_key_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_key_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_key_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_key_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_key_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_key_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_key_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_key_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_output_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_output_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_output_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_output_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_output_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_output_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_output_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_output_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_query_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_query_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_query_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_query_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_query_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_query_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_query_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_query_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_value_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_value_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_value_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_value_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_value_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_value_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_value_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_value_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_att_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_att_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_att_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_att_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_att_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_att_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_att_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_att_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_ffn_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_ffn_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_ffn_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_ffn_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_ffn_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_ffn_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_ffn_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_ffn_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_0.b_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_0.b_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_0.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_0.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_1.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_1.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_1.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_1.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_1.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_1.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_1.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_1.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_key_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_key_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_key_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_key_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_key_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_key_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_key_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_key_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_output_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_output_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_output_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_output_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_output_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_output_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_output_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_output_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_query_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_query_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_query_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_query_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_query_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_query_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_query_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_query_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_value_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_value_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_value_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_value_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_value_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_value_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_value_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_value_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_att_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_att_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_att_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_att_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_att_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_att_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_att_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_att_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_ffn_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_ffn_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_ffn_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_ffn_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_ffn_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_ffn_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_ffn_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_ffn_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_0.b_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_0.b_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_0.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_0.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_1.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_1.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_1.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_1.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_1.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_1.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_1.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_1.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_key_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_key_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_key_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_key_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_key_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_key_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_key_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_key_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_output_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_output_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_output_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_output_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_output_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_output_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_output_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_output_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_query_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_query_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_query_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_query_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_query_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_query_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_query_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_query_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_value_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_value_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_value_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_value_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_value_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_value_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_value_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_value_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_att_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_att_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_att_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_att_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_att_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_att_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_att_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_att_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_ffn_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_ffn_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_ffn_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_ffn_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_ffn_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_ffn_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_ffn_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_ffn_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_0.b_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_0.b_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_0.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_0.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_1.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_1.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_1.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_1.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_1.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_1.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_1.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_1.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_key_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_key_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_key_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_key_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_key_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_key_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_key_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_key_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_output_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_output_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_output_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_output_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_output_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_output_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_output_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_output_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_query_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_query_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_query_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_query_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_query_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_query_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_query_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_query_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_value_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_value_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_value_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_value_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_value_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_value_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_value_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_value_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_att_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_att_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_att_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_att_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_att_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_att_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_att_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_att_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_ffn_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_ffn_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_ffn_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_ffn_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_ffn_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_ffn_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_ffn_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_ffn_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_0.b_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_0.b_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_0.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_0.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_1.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_1.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_1.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_1.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_1.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_1.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_1.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_1.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_key_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_key_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_key_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_key_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_key_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_key_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_key_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_key_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_output_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_output_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_output_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_output_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_output_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_output_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_output_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_output_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_query_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_query_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_query_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_query_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_query_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_query_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_query_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_query_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_value_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_value_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_value_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_value_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_value_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_value_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_value_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_value_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_att_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_att_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_att_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_att_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_att_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_att_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_att_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_att_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_ffn_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_ffn_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_ffn_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_ffn_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_ffn_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_ffn_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_ffn_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_ffn_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_0.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_0.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_0.b_0_moment1_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_0.b_0_moment2_0 : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_0.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_0.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_0.w_0_moment1_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_0.w_0_moment2_0 : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_1.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_1.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_1.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_1.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_1.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_1.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_1.w_0_moment1_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_1.w_0_moment2_0 : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_key_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_key_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_key_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_key_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_key_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_key_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_key_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_key_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_output_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_output_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_output_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_output_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_output_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_output_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_output_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_output_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_query_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_query_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_query_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_query_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_query_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_query_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_query_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_query_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_value_fc.b_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_value_fc.b_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_value_fc.b_0_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_value_fc.b_0_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_value_fc.w_0_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_value_fc.w_0_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_value_fc.w_0_moment1_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_value_fc.w_0_moment2_0 : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_att_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_att_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_att_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_att_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_att_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_att_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_att_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_att_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_ffn_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_ffn_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_ffn_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_ffn_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_ffn_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_ffn_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_ffn_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_ffn_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var pos_embedding_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var pos_embedding_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var pos_embedding_moment1_0 : LOD_TENSOR.shape(512, 768).dtype(float32).stop_gradient(False)
    persist var pos_embedding_moment2_0 : LOD_TENSOR.shape(512, 768).dtype(float32).stop_gradient(False)
    persist var pre_encoder_layer_norm_bias_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var pre_encoder_layer_norm_bias_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var pre_encoder_layer_norm_bias_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var pre_encoder_layer_norm_bias_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var pre_encoder_layer_norm_scale_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var pre_encoder_layer_norm_scale_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var pre_encoder_layer_norm_scale_moment1_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var pre_encoder_layer_norm_scale_moment2_0 : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var sent_embedding_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var sent_embedding_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var sent_embedding_moment1_0 : LOD_TENSOR.shape(4, 768).dtype(float32).stop_gradient(False)
    persist var sent_embedding_moment2_0 : LOD_TENSOR.shape(4, 768).dtype(float32).stop_gradient(False)
    persist var word_embedding_beta1_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var word_embedding_beta2_pow_acc_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    persist var word_embedding_moment1_0 : LOD_TENSOR.shape(30000, 768).dtype(float32).stop_gradient(False)
    persist var word_embedding_moment2_0 : LOD_TENSOR.shape(30000, 768).dtype(float32).stop_gradient(False)
    persist var word_embedding@GRAD@MERGED : LOD_TENSOR.shape(30000, 768).dtype(float32).stop_gradient(False)
    persist var pos_embedding@GRAD@MERGED : LOD_TENSOR.shape(512, 768).dtype(float32).stop_gradient(False)
    persist var sent_embedding@GRAD@MERGED : LOD_TENSOR.shape(4, 768).dtype(float32).stop_gradient(False)
    persist var pre_encoder_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var pre_encoder_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_query_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_query_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_query_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_query_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_key_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_key_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_key_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_key_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_value_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_value_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_value_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_value_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_output_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_output_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_multi_head_att_output_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_0_multi_head_att_output_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_att_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_att_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_0.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_0_ffn_fc_0.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_0.b_0@GRAD@MERGED : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_0_ffn_fc_0.b_0@GRAD@TMP : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_1.w_0@GRAD@MERGED : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_0_ffn_fc_1.w_0@GRAD@TMP : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_ffn_fc_1.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_0_ffn_fc_1.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_ffn_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_0_post_ffn_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_query_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_query_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_query_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_query_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_key_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_key_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_key_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_key_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_value_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_value_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_value_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_value_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_output_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_output_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_multi_head_att_output_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_multi_head_att_output_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_att_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_att_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_0.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_1_ffn_fc_0.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_0.b_0@GRAD@MERGED : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_ffn_fc_0.b_0@GRAD@TMP : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_1.w_0@GRAD@MERGED : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_1_ffn_fc_1.w_0@GRAD@TMP : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_ffn_fc_1.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_1_ffn_fc_1.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_ffn_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_1_post_ffn_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_query_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_query_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_query_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_query_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_key_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_key_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_key_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_key_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_value_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_value_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_value_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_value_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_output_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_output_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_multi_head_att_output_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_multi_head_att_output_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_att_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_att_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_0.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_2_ffn_fc_0.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_0.b_0@GRAD@MERGED : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_ffn_fc_0.b_0@GRAD@TMP : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_1.w_0@GRAD@MERGED : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_2_ffn_fc_1.w_0@GRAD@TMP : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_ffn_fc_1.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_2_ffn_fc_1.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_ffn_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_2_post_ffn_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_query_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_query_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_query_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_query_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_key_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_key_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_key_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_key_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_value_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_value_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_value_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_value_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_output_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_output_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_multi_head_att_output_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_multi_head_att_output_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_att_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_att_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_0.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_3_ffn_fc_0.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_0.b_0@GRAD@MERGED : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_ffn_fc_0.b_0@GRAD@TMP : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_1.w_0@GRAD@MERGED : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_3_ffn_fc_1.w_0@GRAD@TMP : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_ffn_fc_1.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_3_ffn_fc_1.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_ffn_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_3_post_ffn_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_query_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_query_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_query_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_query_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_key_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_key_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_key_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_key_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_value_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_value_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_value_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_value_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_output_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_output_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_multi_head_att_output_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_multi_head_att_output_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_att_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_att_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_0.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_4_ffn_fc_0.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_0.b_0@GRAD@MERGED : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_ffn_fc_0.b_0@GRAD@TMP : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_1.w_0@GRAD@MERGED : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_4_ffn_fc_1.w_0@GRAD@TMP : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_ffn_fc_1.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_4_ffn_fc_1.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_ffn_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_4_post_ffn_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_query_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_query_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_query_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_query_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_key_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_key_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_key_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_key_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_value_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_value_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_value_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_value_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_output_fc.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_output_fc.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_multi_head_att_output_fc.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_multi_head_att_output_fc.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_att_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_att_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_0.w_0@GRAD@MERGED : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    var encoder_layer_5_ffn_fc_0.w_0@GRAD@TMP : LOD_TENSOR.shape(768, 3072).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_0.b_0@GRAD@MERGED : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_ffn_fc_0.b_0@GRAD@TMP : LOD_TENSOR.shape(3072,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_1.w_0@GRAD@MERGED : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    var encoder_layer_5_ffn_fc_1.w_0@GRAD@TMP : LOD_TENSOR.shape(3072, 768).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_ffn_fc_1.b_0@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var encoder_layer_5_ffn_fc_1.b_0@GRAD@TMP : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_ffn_layer_norm_bias@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    persist var encoder_layer_5_post_ffn_layer_norm_scale@GRAD@MERGED : LOD_TENSOR.shape(768,).dtype(float32).stop_gradient(False)
    var find_infinite_scale.tmp_0@cast_int32 : LOD_TENSOR.shape(1,).dtype(int32).stop_gradient(False)
    var find_infinite_scale.tmp_0@GLOBAL_WORLD : LOD_TENSOR.shape(1,).dtype(bool).stop_gradient(False)

    {Out=['@LR_DECAY_COUNTER@']} = increment(inputs={X=['@LR_DECAY_COUNTER@']}, op_device = , op_namescope = /, op_role = 16, op_role_var = [], step = 1.0)
    {Out=['create_py_reader_0']} = create_py_reader(inputs={blocking_queue=['lod_tensor_blocking_queue_0']}, device_count = 1, device_index = 0, dtypes = [2, 2, 2, 2, 2], lod_levels = [0, 0, 0, 0, 0], need_check_feed = [1, 1, 1, 1, 1], op_device = , op_namescope = /, op_role = 0, op_role_var = [], ranks = [2, 2, 2, 2, 2], shape_concat = [-1, 512, -1, 512, -1, 512, -1, 1, -1, 1], use_data_config = True)
    {Out=['double_buffer_0']} = create_double_buffer_reader(inputs={UnderlyingReader=['create_py_reader_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place = AUTO)
    {Out=['src_ids', 'sent_ids', 'pos_ids', 'mask_label', 'mask_pos']} = read(inputs={Reader=['double_buffer_0']}, drop_last = True, infer_out = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], throw_eof_exp = True)
    {Out=['fill_constant_1.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 2, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = 0, value = 0.0)
    {Out=['equal_0.tmp_0']} = equal(inputs={X=['src_ids'], Y=['fill_constant_1.tmp_0']}, axis = -1, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['logical_not_0.tmp_0']} = logical_not(inputs={X=['equal_0.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['cast_0.tmp_0']} = cast(inputs={X=['logical_not_0.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['unsqueeze2_0.tmp_0'], XShape=['unsqueeze2_0.tmp_1']} = unsqueeze2(inputs={AxesTensor=[], AxesTensorList=[], X=['cast_0.tmp_0']}, axes = [-1], op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['embedding_1.tmp_0']} = lookup_table_v2(inputs={Ids=['src_ids'], W=['word_embedding']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Out=['embedding_3.tmp_0']} = lookup_table_v2(inputs={Ids=['pos_ids'], W=['pos_embedding']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Out=['embedding_5.tmp_0']} = lookup_table_v2(inputs={Ids=['sent_ids'], W=['sent_embedding']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Out=['tmp_0']} = elementwise_add(inputs={X=['embedding_1.tmp_0'], Y=['embedding_3.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_1']} = elementwise_add(inputs={X=['tmp_0'], Y=['embedding_5.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_1.cast_fp16']} = cast(inputs={X=['tmp_1']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Mean=['layer_norm_0.tmp_0'], Variance=['layer_norm_0.tmp_1'], Y=['layer_norm_0.tmp_2']} = layer_norm(inputs={Bias=['pre_encoder_layer_norm_bias'], Scale=['pre_encoder_layer_norm_scale'], X=['tmp_1.cast_fp16']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['unsqueeze2_0.tmp_0.cast_fp16']} = cast(inputs={X=['unsqueeze2_0.tmp_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['matmul_v2_0.tmp_0']} = matmul_v2(inputs={X=['unsqueeze2_0.tmp_0.cast_fp16'], Y=['unsqueeze2_0.tmp_0.cast_fp16']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['scale_0.tmp_0']} = scale(inputs={ScaleTensor=[], X=['matmul_v2_0.tmp_0']}, bias = -1.0, bias_after_scale = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 10000.0, use_mkldnn = False)
    {Y=['stack_0.tmp_0']} = stack(inputs={X=['scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0', 'scale_0.tmp_0']}, axis = 1, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['encoder_layer_0_multi_head_att_query_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_0_multi_head_att_query_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_0.tmp_0']} = mul(inputs={X=['layer_norm_0.tmp_2'], Y=['encoder_layer_0_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_0_multi_head_att_query_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_0_multi_head_att_query_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_0.tmp_1']} = elementwise_add(inputs={X=['fc_0.tmp_0'], Y=['encoder_layer_0_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_0_multi_head_att_key_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_0_multi_head_att_key_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_1.tmp_0']} = mul(inputs={X=['layer_norm_0.tmp_2'], Y=['encoder_layer_0_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_0_multi_head_att_key_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_0_multi_head_att_key_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_1.tmp_1']} = elementwise_add(inputs={X=['fc_1.tmp_0'], Y=['encoder_layer_0_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_0_multi_head_att_value_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_0_multi_head_att_value_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_2.tmp_0']} = mul(inputs={X=['layer_norm_0.tmp_2'], Y=['encoder_layer_0_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_0_multi_head_att_value_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_0_multi_head_att_value_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_2.tmp_1']} = elementwise_add(inputs={X=['fc_2.tmp_0'], Y=['encoder_layer_0_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_0.tmp_1'], XShape=['reshape2_0.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_0.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_0.tmp_0'], XShape=['transpose_0.tmp_1']} = transpose2(inputs={X=['fc_0.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_1.tmp_1'], XShape=['reshape2_1.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_1.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_1.tmp_0'], XShape=['transpose_1.tmp_1']} = transpose2(inputs={X=['fc_1.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_2.tmp_1'], XShape=['reshape2_2.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_2.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_2.tmp_0'], XShape=['transpose_2.tmp_1']} = transpose2(inputs={X=['fc_2.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['scale_1.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_0.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.125, use_mkldnn = False)
    {Out=['matmul_v2_1.tmp_0']} = matmul_v2(inputs={X=['scale_1.tmp_0'], Y=['transpose_1.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['tmp_2']} = elementwise_add(inputs={X=['matmul_v2_1.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_0.tmp_0']} = softmax(inputs={X=['tmp_2']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_2.tmp_0']} = matmul_v2(inputs={X=['softmax_0.tmp_0'], Y=['transpose_2.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {Out=['transpose_3.tmp_0'], XShape=['transpose_3.tmp_1']} = transpose2(inputs={X=['matmul_v2_2.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['transpose_3.tmp_0'], XShape=['reshape2_3.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_3.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['encoder_layer_0_multi_head_att_output_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_0_multi_head_att_output_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_3.tmp_0']} = mul(inputs={X=['transpose_3.tmp_0'], Y=['encoder_layer_0_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_0_multi_head_att_output_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_0_multi_head_att_output_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_3.tmp_1']} = elementwise_add(inputs={X=['fc_3.tmp_0'], Y=['encoder_layer_0_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_3']} = elementwise_add(inputs={X=['fc_3.tmp_1'], Y=['layer_norm_0.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_1.tmp_0'], Variance=['layer_norm_1.tmp_1'], Y=['layer_norm_1.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_0_post_att_layer_norm_bias'], Scale=['encoder_layer_0_post_att_layer_norm_scale'], X=['tmp_3']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_0_ffn_fc_0.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_0_ffn_fc_0.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_4.tmp_0']} = mul(inputs={X=['layer_norm_1.tmp_2'], Y=['encoder_layer_0_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_0_ffn_fc_0.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_0_ffn_fc_0.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_4.tmp_1']} = elementwise_add(inputs={X=['fc_4.tmp_0'], Y=['encoder_layer_0_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_4.tmp_1.cast_fp32']} = cast(inputs={X=['fc_4.tmp_1']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['pow_0.tmp_0']} = pow(inputs={FactorTensor=[], X=['fc_4.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['tmp_4']} = scale(inputs={ScaleTensor=[], X=['pow_0.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.044714998453855515, use_mkldnn = False)
    {Out=['tmp_5']} = elementwise_add(inputs={X=['fc_4.tmp_1.cast_fp32'], Y=['tmp_4']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_6']} = scale(inputs={ScaleTensor=[], X=['tmp_5']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.7978845834732056, use_mkldnn = False)
    {Out=['tanh_0.tmp_0']} = tanh(inputs={X=['tmp_6']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_7']} = scale(inputs={ScaleTensor=[], X=['tanh_0.tmp_0']}, bias = 1.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0, use_mkldnn = False)
    {Out=['tmp_8']} = scale(inputs={ScaleTensor=[], X=['tmp_7']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.5, use_mkldnn = False)
    {Out=['tmp_9']} = elementwise_mul(inputs={X=['fc_4.tmp_1.cast_fp32'], Y=['tmp_8']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_0_ffn_fc_1.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_0_ffn_fc_1.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['tmp_9.cast_fp16']} = cast(inputs={X=['tmp_9']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_5.tmp_0']} = mul(inputs={X=['tmp_9.cast_fp16'], Y=['encoder_layer_0_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_0_ffn_fc_1.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_0_ffn_fc_1.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_5.tmp_1']} = elementwise_add(inputs={X=['fc_5.tmp_0'], Y=['encoder_layer_0_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_10']} = elementwise_add(inputs={X=['fc_5.tmp_1'], Y=['layer_norm_1.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_2.tmp_0'], Variance=['layer_norm_2.tmp_1'], Y=['layer_norm_2.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_0_post_ffn_layer_norm_bias'], Scale=['encoder_layer_0_post_ffn_layer_norm_scale'], X=['tmp_10']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_1_multi_head_att_query_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_1_multi_head_att_query_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_6.tmp_0']} = mul(inputs={X=['layer_norm_2.tmp_2'], Y=['encoder_layer_1_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_1_multi_head_att_query_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_1_multi_head_att_query_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_6.tmp_1']} = elementwise_add(inputs={X=['fc_6.tmp_0'], Y=['encoder_layer_1_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_1_multi_head_att_key_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_1_multi_head_att_key_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_7.tmp_0']} = mul(inputs={X=['layer_norm_2.tmp_2'], Y=['encoder_layer_1_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_1_multi_head_att_key_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_1_multi_head_att_key_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_7.tmp_1']} = elementwise_add(inputs={X=['fc_7.tmp_0'], Y=['encoder_layer_1_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_1_multi_head_att_value_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_1_multi_head_att_value_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_8.tmp_0']} = mul(inputs={X=['layer_norm_2.tmp_2'], Y=['encoder_layer_1_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_1_multi_head_att_value_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_1_multi_head_att_value_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_8.tmp_1']} = elementwise_add(inputs={X=['fc_8.tmp_0'], Y=['encoder_layer_1_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_6.tmp_1'], XShape=['reshape2_4.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_6.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_4.tmp_0'], XShape=['transpose_4.tmp_1']} = transpose2(inputs={X=['fc_6.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_7.tmp_1'], XShape=['reshape2_5.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_7.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_5.tmp_0'], XShape=['transpose_5.tmp_1']} = transpose2(inputs={X=['fc_7.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_8.tmp_1'], XShape=['reshape2_6.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_8.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_6.tmp_0'], XShape=['transpose_6.tmp_1']} = transpose2(inputs={X=['fc_8.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['scale_2.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_4.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.125, use_mkldnn = False)
    {Out=['matmul_v2_3.tmp_0']} = matmul_v2(inputs={X=['scale_2.tmp_0'], Y=['transpose_5.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['tmp_11']} = elementwise_add(inputs={X=['matmul_v2_3.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_1.tmp_0']} = softmax(inputs={X=['tmp_11']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_4.tmp_0']} = matmul_v2(inputs={X=['softmax_1.tmp_0'], Y=['transpose_6.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {Out=['transpose_7.tmp_0'], XShape=['transpose_7.tmp_1']} = transpose2(inputs={X=['matmul_v2_4.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['transpose_7.tmp_0'], XShape=['reshape2_7.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_7.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['encoder_layer_1_multi_head_att_output_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_1_multi_head_att_output_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_9.tmp_0']} = mul(inputs={X=['transpose_7.tmp_0'], Y=['encoder_layer_1_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_1_multi_head_att_output_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_1_multi_head_att_output_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_9.tmp_1']} = elementwise_add(inputs={X=['fc_9.tmp_0'], Y=['encoder_layer_1_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_12']} = elementwise_add(inputs={X=['fc_9.tmp_1'], Y=['layer_norm_2.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_3.tmp_0'], Variance=['layer_norm_3.tmp_1'], Y=['layer_norm_3.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_1_post_att_layer_norm_bias'], Scale=['encoder_layer_1_post_att_layer_norm_scale'], X=['tmp_12']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_1_ffn_fc_0.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_1_ffn_fc_0.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_10.tmp_0']} = mul(inputs={X=['layer_norm_3.tmp_2'], Y=['encoder_layer_1_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_1_ffn_fc_0.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_1_ffn_fc_0.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_10.tmp_1']} = elementwise_add(inputs={X=['fc_10.tmp_0'], Y=['encoder_layer_1_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_10.tmp_1.cast_fp32']} = cast(inputs={X=['fc_10.tmp_1']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['pow_1.tmp_0']} = pow(inputs={FactorTensor=[], X=['fc_10.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['tmp_13']} = scale(inputs={ScaleTensor=[], X=['pow_1.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.044714998453855515, use_mkldnn = False)
    {Out=['tmp_14']} = elementwise_add(inputs={X=['fc_10.tmp_1.cast_fp32'], Y=['tmp_13']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_15']} = scale(inputs={ScaleTensor=[], X=['tmp_14']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.7978845834732056, use_mkldnn = False)
    {Out=['tanh_1.tmp_0']} = tanh(inputs={X=['tmp_15']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_16']} = scale(inputs={ScaleTensor=[], X=['tanh_1.tmp_0']}, bias = 1.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0, use_mkldnn = False)
    {Out=['tmp_17']} = scale(inputs={ScaleTensor=[], X=['tmp_16']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.5, use_mkldnn = False)
    {Out=['tmp_18']} = elementwise_mul(inputs={X=['fc_10.tmp_1.cast_fp32'], Y=['tmp_17']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_1_ffn_fc_1.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_1_ffn_fc_1.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['tmp_18.cast_fp16']} = cast(inputs={X=['tmp_18']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_11.tmp_0']} = mul(inputs={X=['tmp_18.cast_fp16'], Y=['encoder_layer_1_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_1_ffn_fc_1.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_1_ffn_fc_1.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_11.tmp_1']} = elementwise_add(inputs={X=['fc_11.tmp_0'], Y=['encoder_layer_1_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_19']} = elementwise_add(inputs={X=['fc_11.tmp_1'], Y=['layer_norm_3.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_4.tmp_0'], Variance=['layer_norm_4.tmp_1'], Y=['layer_norm_4.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_1_post_ffn_layer_norm_bias'], Scale=['encoder_layer_1_post_ffn_layer_norm_scale'], X=['tmp_19']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_2_multi_head_att_query_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_2_multi_head_att_query_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_12.tmp_0']} = mul(inputs={X=['layer_norm_4.tmp_2'], Y=['encoder_layer_2_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_2_multi_head_att_query_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_2_multi_head_att_query_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_12.tmp_1']} = elementwise_add(inputs={X=['fc_12.tmp_0'], Y=['encoder_layer_2_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_2_multi_head_att_key_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_2_multi_head_att_key_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_13.tmp_0']} = mul(inputs={X=['layer_norm_4.tmp_2'], Y=['encoder_layer_2_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_2_multi_head_att_key_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_2_multi_head_att_key_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_13.tmp_1']} = elementwise_add(inputs={X=['fc_13.tmp_0'], Y=['encoder_layer_2_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_2_multi_head_att_value_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_2_multi_head_att_value_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_14.tmp_0']} = mul(inputs={X=['layer_norm_4.tmp_2'], Y=['encoder_layer_2_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_2_multi_head_att_value_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_2_multi_head_att_value_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_14.tmp_1']} = elementwise_add(inputs={X=['fc_14.tmp_0'], Y=['encoder_layer_2_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_12.tmp_1'], XShape=['reshape2_8.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_12.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_8.tmp_0'], XShape=['transpose_8.tmp_1']} = transpose2(inputs={X=['fc_12.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_13.tmp_1'], XShape=['reshape2_9.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_13.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_9.tmp_0'], XShape=['transpose_9.tmp_1']} = transpose2(inputs={X=['fc_13.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_14.tmp_1'], XShape=['reshape2_10.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_14.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_10.tmp_0'], XShape=['transpose_10.tmp_1']} = transpose2(inputs={X=['fc_14.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['scale_3.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_8.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.125, use_mkldnn = False)
    {Out=['matmul_v2_5.tmp_0']} = matmul_v2(inputs={X=['scale_3.tmp_0'], Y=['transpose_9.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['tmp_20']} = elementwise_add(inputs={X=['matmul_v2_5.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_2.tmp_0']} = softmax(inputs={X=['tmp_20']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_6.tmp_0']} = matmul_v2(inputs={X=['softmax_2.tmp_0'], Y=['transpose_10.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {Out=['transpose_11.tmp_0'], XShape=['transpose_11.tmp_1']} = transpose2(inputs={X=['matmul_v2_6.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['transpose_11.tmp_0'], XShape=['reshape2_11.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_11.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['encoder_layer_2_multi_head_att_output_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_2_multi_head_att_output_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_15.tmp_0']} = mul(inputs={X=['transpose_11.tmp_0'], Y=['encoder_layer_2_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_2_multi_head_att_output_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_2_multi_head_att_output_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_15.tmp_1']} = elementwise_add(inputs={X=['fc_15.tmp_0'], Y=['encoder_layer_2_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_21']} = elementwise_add(inputs={X=['fc_15.tmp_1'], Y=['layer_norm_4.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_5.tmp_0'], Variance=['layer_norm_5.tmp_1'], Y=['layer_norm_5.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_2_post_att_layer_norm_bias'], Scale=['encoder_layer_2_post_att_layer_norm_scale'], X=['tmp_21']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_2_ffn_fc_0.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_2_ffn_fc_0.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_16.tmp_0']} = mul(inputs={X=['layer_norm_5.tmp_2'], Y=['encoder_layer_2_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_2_ffn_fc_0.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_2_ffn_fc_0.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_16.tmp_1']} = elementwise_add(inputs={X=['fc_16.tmp_0'], Y=['encoder_layer_2_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_16.tmp_1.cast_fp32']} = cast(inputs={X=['fc_16.tmp_1']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['pow_2.tmp_0']} = pow(inputs={FactorTensor=[], X=['fc_16.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['tmp_22']} = scale(inputs={ScaleTensor=[], X=['pow_2.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.044714998453855515, use_mkldnn = False)
    {Out=['tmp_23']} = elementwise_add(inputs={X=['fc_16.tmp_1.cast_fp32'], Y=['tmp_22']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_24']} = scale(inputs={ScaleTensor=[], X=['tmp_23']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.7978845834732056, use_mkldnn = False)
    {Out=['tanh_2.tmp_0']} = tanh(inputs={X=['tmp_24']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_25']} = scale(inputs={ScaleTensor=[], X=['tanh_2.tmp_0']}, bias = 1.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0, use_mkldnn = False)
    {Out=['tmp_26']} = scale(inputs={ScaleTensor=[], X=['tmp_25']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.5, use_mkldnn = False)
    {Out=['tmp_27']} = elementwise_mul(inputs={X=['fc_16.tmp_1.cast_fp32'], Y=['tmp_26']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_2_ffn_fc_1.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_2_ffn_fc_1.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['tmp_27.cast_fp16']} = cast(inputs={X=['tmp_27']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_17.tmp_0']} = mul(inputs={X=['tmp_27.cast_fp16'], Y=['encoder_layer_2_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_2_ffn_fc_1.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_2_ffn_fc_1.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_17.tmp_1']} = elementwise_add(inputs={X=['fc_17.tmp_0'], Y=['encoder_layer_2_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_28']} = elementwise_add(inputs={X=['fc_17.tmp_1'], Y=['layer_norm_5.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_6.tmp_0'], Variance=['layer_norm_6.tmp_1'], Y=['layer_norm_6.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_2_post_ffn_layer_norm_bias'], Scale=['encoder_layer_2_post_ffn_layer_norm_scale'], X=['tmp_28']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_3_multi_head_att_query_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_3_multi_head_att_query_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_18.tmp_0']} = mul(inputs={X=['layer_norm_6.tmp_2'], Y=['encoder_layer_3_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_3_multi_head_att_query_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_3_multi_head_att_query_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_18.tmp_1']} = elementwise_add(inputs={X=['fc_18.tmp_0'], Y=['encoder_layer_3_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_3_multi_head_att_key_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_3_multi_head_att_key_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_19.tmp_0']} = mul(inputs={X=['layer_norm_6.tmp_2'], Y=['encoder_layer_3_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_3_multi_head_att_key_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_3_multi_head_att_key_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_19.tmp_1']} = elementwise_add(inputs={X=['fc_19.tmp_0'], Y=['encoder_layer_3_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_3_multi_head_att_value_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_3_multi_head_att_value_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_20.tmp_0']} = mul(inputs={X=['layer_norm_6.tmp_2'], Y=['encoder_layer_3_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_3_multi_head_att_value_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_3_multi_head_att_value_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_20.tmp_1']} = elementwise_add(inputs={X=['fc_20.tmp_0'], Y=['encoder_layer_3_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_18.tmp_1'], XShape=['reshape2_12.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_18.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_12.tmp_0'], XShape=['transpose_12.tmp_1']} = transpose2(inputs={X=['fc_18.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_19.tmp_1'], XShape=['reshape2_13.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_19.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_13.tmp_0'], XShape=['transpose_13.tmp_1']} = transpose2(inputs={X=['fc_19.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_20.tmp_1'], XShape=['reshape2_14.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_20.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_14.tmp_0'], XShape=['transpose_14.tmp_1']} = transpose2(inputs={X=['fc_20.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['scale_4.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_12.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.125, use_mkldnn = False)
    {Out=['matmul_v2_7.tmp_0']} = matmul_v2(inputs={X=['scale_4.tmp_0'], Y=['transpose_13.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['tmp_29']} = elementwise_add(inputs={X=['matmul_v2_7.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_3.tmp_0']} = softmax(inputs={X=['tmp_29']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_8.tmp_0']} = matmul_v2(inputs={X=['softmax_3.tmp_0'], Y=['transpose_14.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {Out=['transpose_15.tmp_0'], XShape=['transpose_15.tmp_1']} = transpose2(inputs={X=['matmul_v2_8.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['transpose_15.tmp_0'], XShape=['reshape2_15.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_15.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['encoder_layer_3_multi_head_att_output_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_3_multi_head_att_output_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_21.tmp_0']} = mul(inputs={X=['transpose_15.tmp_0'], Y=['encoder_layer_3_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_3_multi_head_att_output_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_3_multi_head_att_output_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_21.tmp_1']} = elementwise_add(inputs={X=['fc_21.tmp_0'], Y=['encoder_layer_3_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_30']} = elementwise_add(inputs={X=['fc_21.tmp_1'], Y=['layer_norm_6.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_7.tmp_0'], Variance=['layer_norm_7.tmp_1'], Y=['layer_norm_7.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_3_post_att_layer_norm_bias'], Scale=['encoder_layer_3_post_att_layer_norm_scale'], X=['tmp_30']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_3_ffn_fc_0.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_3_ffn_fc_0.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_22.tmp_0']} = mul(inputs={X=['layer_norm_7.tmp_2'], Y=['encoder_layer_3_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_3_ffn_fc_0.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_3_ffn_fc_0.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_22.tmp_1']} = elementwise_add(inputs={X=['fc_22.tmp_0'], Y=['encoder_layer_3_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_22.tmp_1.cast_fp32']} = cast(inputs={X=['fc_22.tmp_1']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['pow_3.tmp_0']} = pow(inputs={FactorTensor=[], X=['fc_22.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['tmp_31']} = scale(inputs={ScaleTensor=[], X=['pow_3.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.044714998453855515, use_mkldnn = False)
    {Out=['tmp_32']} = elementwise_add(inputs={X=['fc_22.tmp_1.cast_fp32'], Y=['tmp_31']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_33']} = scale(inputs={ScaleTensor=[], X=['tmp_32']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.7978845834732056, use_mkldnn = False)
    {Out=['tanh_3.tmp_0']} = tanh(inputs={X=['tmp_33']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_34']} = scale(inputs={ScaleTensor=[], X=['tanh_3.tmp_0']}, bias = 1.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0, use_mkldnn = False)
    {Out=['tmp_35']} = scale(inputs={ScaleTensor=[], X=['tmp_34']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.5, use_mkldnn = False)
    {Out=['tmp_36']} = elementwise_mul(inputs={X=['fc_22.tmp_1.cast_fp32'], Y=['tmp_35']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_3_ffn_fc_1.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_3_ffn_fc_1.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['tmp_36.cast_fp16']} = cast(inputs={X=['tmp_36']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_23.tmp_0']} = mul(inputs={X=['tmp_36.cast_fp16'], Y=['encoder_layer_3_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_3_ffn_fc_1.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_3_ffn_fc_1.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_23.tmp_1']} = elementwise_add(inputs={X=['fc_23.tmp_0'], Y=['encoder_layer_3_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_37']} = elementwise_add(inputs={X=['fc_23.tmp_1'], Y=['layer_norm_7.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_8.tmp_0'], Variance=['layer_norm_8.tmp_1'], Y=['layer_norm_8.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_3_post_ffn_layer_norm_bias'], Scale=['encoder_layer_3_post_ffn_layer_norm_scale'], X=['tmp_37']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_4_multi_head_att_query_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_4_multi_head_att_query_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_24.tmp_0']} = mul(inputs={X=['layer_norm_8.tmp_2'], Y=['encoder_layer_4_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_4_multi_head_att_query_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_4_multi_head_att_query_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_24.tmp_1']} = elementwise_add(inputs={X=['fc_24.tmp_0'], Y=['encoder_layer_4_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_4_multi_head_att_key_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_4_multi_head_att_key_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_25.tmp_0']} = mul(inputs={X=['layer_norm_8.tmp_2'], Y=['encoder_layer_4_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_4_multi_head_att_key_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_4_multi_head_att_key_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_25.tmp_1']} = elementwise_add(inputs={X=['fc_25.tmp_0'], Y=['encoder_layer_4_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_4_multi_head_att_value_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_4_multi_head_att_value_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_26.tmp_0']} = mul(inputs={X=['layer_norm_8.tmp_2'], Y=['encoder_layer_4_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_4_multi_head_att_value_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_4_multi_head_att_value_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_26.tmp_1']} = elementwise_add(inputs={X=['fc_26.tmp_0'], Y=['encoder_layer_4_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_24.tmp_1'], XShape=['reshape2_16.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_24.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_16.tmp_0'], XShape=['transpose_16.tmp_1']} = transpose2(inputs={X=['fc_24.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_25.tmp_1'], XShape=['reshape2_17.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_25.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_17.tmp_0'], XShape=['transpose_17.tmp_1']} = transpose2(inputs={X=['fc_25.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_26.tmp_1'], XShape=['reshape2_18.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_26.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_18.tmp_0'], XShape=['transpose_18.tmp_1']} = transpose2(inputs={X=['fc_26.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['scale_5.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_16.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.125, use_mkldnn = False)
    {Out=['matmul_v2_9.tmp_0']} = matmul_v2(inputs={X=['scale_5.tmp_0'], Y=['transpose_17.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['tmp_38']} = elementwise_add(inputs={X=['matmul_v2_9.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_4.tmp_0']} = softmax(inputs={X=['tmp_38']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_10.tmp_0']} = matmul_v2(inputs={X=['softmax_4.tmp_0'], Y=['transpose_18.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {Out=['transpose_19.tmp_0'], XShape=['transpose_19.tmp_1']} = transpose2(inputs={X=['matmul_v2_10.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['transpose_19.tmp_0'], XShape=['reshape2_19.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_19.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['encoder_layer_4_multi_head_att_output_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_4_multi_head_att_output_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_27.tmp_0']} = mul(inputs={X=['transpose_19.tmp_0'], Y=['encoder_layer_4_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_4_multi_head_att_output_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_4_multi_head_att_output_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_27.tmp_1']} = elementwise_add(inputs={X=['fc_27.tmp_0'], Y=['encoder_layer_4_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_39']} = elementwise_add(inputs={X=['fc_27.tmp_1'], Y=['layer_norm_8.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_9.tmp_0'], Variance=['layer_norm_9.tmp_1'], Y=['layer_norm_9.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_4_post_att_layer_norm_bias'], Scale=['encoder_layer_4_post_att_layer_norm_scale'], X=['tmp_39']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_4_ffn_fc_0.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_4_ffn_fc_0.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_28.tmp_0']} = mul(inputs={X=['layer_norm_9.tmp_2'], Y=['encoder_layer_4_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_4_ffn_fc_0.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_4_ffn_fc_0.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_28.tmp_1']} = elementwise_add(inputs={X=['fc_28.tmp_0'], Y=['encoder_layer_4_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_28.tmp_1.cast_fp32']} = cast(inputs={X=['fc_28.tmp_1']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['pow_4.tmp_0']} = pow(inputs={FactorTensor=[], X=['fc_28.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['tmp_40']} = scale(inputs={ScaleTensor=[], X=['pow_4.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.044714998453855515, use_mkldnn = False)
    {Out=['tmp_41']} = elementwise_add(inputs={X=['fc_28.tmp_1.cast_fp32'], Y=['tmp_40']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_42']} = scale(inputs={ScaleTensor=[], X=['tmp_41']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.7978845834732056, use_mkldnn = False)
    {Out=['tanh_4.tmp_0']} = tanh(inputs={X=['tmp_42']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_43']} = scale(inputs={ScaleTensor=[], X=['tanh_4.tmp_0']}, bias = 1.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0, use_mkldnn = False)
    {Out=['tmp_44']} = scale(inputs={ScaleTensor=[], X=['tmp_43']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.5, use_mkldnn = False)
    {Out=['tmp_45']} = elementwise_mul(inputs={X=['fc_28.tmp_1.cast_fp32'], Y=['tmp_44']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_4_ffn_fc_1.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_4_ffn_fc_1.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['tmp_45.cast_fp16']} = cast(inputs={X=['tmp_45']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_29.tmp_0']} = mul(inputs={X=['tmp_45.cast_fp16'], Y=['encoder_layer_4_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_4_ffn_fc_1.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_4_ffn_fc_1.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_29.tmp_1']} = elementwise_add(inputs={X=['fc_29.tmp_0'], Y=['encoder_layer_4_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_46']} = elementwise_add(inputs={X=['fc_29.tmp_1'], Y=['layer_norm_9.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_10.tmp_0'], Variance=['layer_norm_10.tmp_1'], Y=['layer_norm_10.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_4_post_ffn_layer_norm_bias'], Scale=['encoder_layer_4_post_ffn_layer_norm_scale'], X=['tmp_46']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_5_multi_head_att_query_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_5_multi_head_att_query_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_30.tmp_0']} = mul(inputs={X=['layer_norm_10.tmp_2'], Y=['encoder_layer_5_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_5_multi_head_att_query_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_5_multi_head_att_query_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_30.tmp_1']} = elementwise_add(inputs={X=['fc_30.tmp_0'], Y=['encoder_layer_5_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_5_multi_head_att_key_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_5_multi_head_att_key_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_31.tmp_0']} = mul(inputs={X=['layer_norm_10.tmp_2'], Y=['encoder_layer_5_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_5_multi_head_att_key_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_5_multi_head_att_key_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_31.tmp_1']} = elementwise_add(inputs={X=['fc_31.tmp_0'], Y=['encoder_layer_5_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_5_multi_head_att_value_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_5_multi_head_att_value_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_32.tmp_0']} = mul(inputs={X=['layer_norm_10.tmp_2'], Y=['encoder_layer_5_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_5_multi_head_att_value_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_5_multi_head_att_value_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_32.tmp_1']} = elementwise_add(inputs={X=['fc_32.tmp_0'], Y=['encoder_layer_5_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_30.tmp_1'], XShape=['reshape2_20.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_30.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_20.tmp_0'], XShape=['transpose_20.tmp_1']} = transpose2(inputs={X=['fc_30.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_31.tmp_1'], XShape=['reshape2_21.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_31.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_21.tmp_0'], XShape=['transpose_21.tmp_1']} = transpose2(inputs={X=['fc_31.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_32.tmp_1'], XShape=['reshape2_22.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_32.tmp_1']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_22.tmp_0'], XShape=['transpose_22.tmp_1']} = transpose2(inputs={X=['fc_32.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['scale_6.tmp_0']} = scale(inputs={ScaleTensor=[], X=['transpose_20.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.125, use_mkldnn = False)
    {Out=['matmul_v2_11.tmp_0']} = matmul_v2(inputs={X=['scale_6.tmp_0'], Y=['transpose_21.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['tmp_47']} = elementwise_add(inputs={X=['matmul_v2_11.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_5.tmp_0']} = softmax(inputs={X=['tmp_47']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_12.tmp_0']} = matmul_v2(inputs={X=['softmax_5.tmp_0'], Y=['transpose_22.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {Out=['transpose_23.tmp_0'], XShape=['transpose_23.tmp_1']} = transpose2(inputs={X=['matmul_v2_12.tmp_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['transpose_23.tmp_0'], XShape=['reshape2_23.tmp_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_23.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['encoder_layer_5_multi_head_att_output_fc.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_5_multi_head_att_output_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_33.tmp_0']} = mul(inputs={X=['transpose_23.tmp_0'], Y=['encoder_layer_5_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_5_multi_head_att_output_fc.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_5_multi_head_att_output_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_33.tmp_1']} = elementwise_add(inputs={X=['fc_33.tmp_0'], Y=['encoder_layer_5_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_48']} = elementwise_add(inputs={X=['fc_33.tmp_1'], Y=['layer_norm_10.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_11.tmp_0'], Variance=['layer_norm_11.tmp_1'], Y=['layer_norm_11.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_5_post_att_layer_norm_bias'], Scale=['encoder_layer_5_post_att_layer_norm_scale'], X=['tmp_48']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_5_ffn_fc_0.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_5_ffn_fc_0.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_34.tmp_0']} = mul(inputs={X=['layer_norm_11.tmp_2'], Y=['encoder_layer_5_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_5_ffn_fc_0.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_5_ffn_fc_0.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_34.tmp_1']} = elementwise_add(inputs={X=['fc_34.tmp_0'], Y=['encoder_layer_5_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_34.tmp_1.cast_fp32']} = cast(inputs={X=['fc_34.tmp_1']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['pow_5.tmp_0']} = pow(inputs={FactorTensor=[], X=['fc_34.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['tmp_49']} = scale(inputs={ScaleTensor=[], X=['pow_5.tmp_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.044714998453855515, use_mkldnn = False)
    {Out=['tmp_50']} = elementwise_add(inputs={X=['fc_34.tmp_1.cast_fp32'], Y=['tmp_49']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_51']} = scale(inputs={ScaleTensor=[], X=['tmp_50']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.7978845834732056, use_mkldnn = False)
    {Out=['tanh_5.tmp_0']} = tanh(inputs={X=['tmp_51']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_52']} = scale(inputs={ScaleTensor=[], X=['tanh_5.tmp_0']}, bias = 1.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 1.0, use_mkldnn = False)
    {Out=['tmp_53']} = scale(inputs={ScaleTensor=[], X=['tmp_52']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale = 0.5, use_mkldnn = False)
    {Out=['tmp_54']} = elementwise_mul(inputs={X=['fc_34.tmp_1.cast_fp32'], Y=['tmp_53']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_5_ffn_fc_1.w_0.cast_fp16']} = cast(inputs={X=['encoder_layer_5_ffn_fc_1.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['tmp_54.cast_fp16']} = cast(inputs={X=['tmp_54']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_35.tmp_0']} = mul(inputs={X=['tmp_54.cast_fp16'], Y=['encoder_layer_5_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_5_ffn_fc_1.b_0.cast_fp16']} = cast(inputs={X=['encoder_layer_5_ffn_fc_1.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 0, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_35.tmp_1']} = elementwise_add(inputs={X=['fc_35.tmp_0'], Y=['encoder_layer_5_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_55']} = elementwise_add(inputs={X=['fc_35.tmp_1'], Y=['layer_norm_11.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_12.tmp_0'], Variance=['layer_norm_12.tmp_1'], Y=['layer_norm_12.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_5_post_ffn_layer_norm_bias'], Scale=['encoder_layer_5_post_ffn_layer_norm_scale'], X=['tmp_55']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 0, op_role_var = [], use_mkldnn = False)
    {Out=['layer_norm_12.tmp_2']} = c_sync_calc_stream(inputs={X=['layer_norm_12.tmp_2']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    send_v2(inputs={X=['layer_norm_12.tmp_2']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], peer = 1, ring_id = 20, srTag = 0, tag = tag, use_calc_stream = False)
    {Out=['layer_norm_12.tmp_2']} = c_sync_comm_stream(inputs={X=['layer_norm_12.tmp_2']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 20)
    {Out=['stack_0.tmp_0']} = c_sync_calc_stream(inputs={X=['stack_0.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    send_v2(inputs={X=['stack_0.tmp_0']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], peer = 1, ring_id = 20, srTag = 0, tag = tag, use_calc_stream = False)
    {Out=['stack_0.tmp_0']} = c_sync_comm_stream(inputs={X=['stack_0.tmp_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 20)
    {Out=['tmp_116']} = fill_constant(inputs={}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 0, op_role_var = [], place_type = -1, shape = [1], str_value = , value = 8.0)
    {Out=['tmp_116']} = c_sync_calc_stream(inputs={X=['tmp_116']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    send_v2(inputs={X=['tmp_116']}, op_device = , op_namescope = /, op_role = 0, op_role_var = [], peer = 1, ring_id = 20, srTag = 0, tag = tag, use_calc_stream = False)
    {Out=['tmp_116']} = c_sync_comm_stream(inputs={X=['tmp_116']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], ring_id = 20)
    {Out=['cast_2.tmp_0']} = cast(inputs={X=['@LR_DECAY_COUNTER@']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 16, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['tmp_118']} = fill_constant(inputs={}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [1], str_value = , value = 10000.0)
    {Out=['tmp_119']} = less_than(inputs={X=['cast_2.tmp_0'], Y=['tmp_118']}, axis = -1, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [])
    {Out=['logical_not_1.tmp_0']} = logical_not(inputs={X=['tmp_119']}, op_device = , op_namescope = /, op_role = 16, op_role_var = [])
    {Out=['scheduled_learning_rate'], Scope=['_generated_var_0']} = conditional_block(inputs={Cond=['tmp_119'], Input=['cast_2.tmp_0']}, is_scalar_condition = True, op_device = , op_namescope = /, op_role = 16, op_role_var = [], skip_eager_deletion_vars = [], sub_block = block[1])
    {Out=['scheduled_learning_rate'], Scope=['_generated_var_1']} = conditional_block(inputs={Cond=['logical_not_1.tmp_0'], Input=['@LR_DECAY_COUNTER@']}, is_scalar_condition = True, op_device = , op_namescope = /, op_role = 16, op_role_var = [], skip_eager_deletion_vars = [], sub_block = block[2])
    {FloatStatus=['float_status']} = alloc_float_status(inputs={}, op_device = , op_namescope = /, op_role = 0, op_role_var = [])
    {Out=['layer_norm_12.tmp_2@GRAD']} = recv_v2(inputs={}, dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_shape = [1, 512, 768], peer = 0, ring_id = 21, srTag = 0, tag = tag, use_calc_stream = True)
    {Bias@GRAD=['encoder_layer_5_post_ffn_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_5_post_ffn_layer_norm_scale@GRAD'], X@GRAD=['tmp_55@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_5_post_ffn_layer_norm_bias'], Mean=['layer_norm_12.tmp_0'], Scale=['encoder_layer_5_post_ffn_layer_norm_scale'], Variance=['layer_norm_12.tmp_1'], X=['tmp_55'], Y@GRAD=['layer_norm_12.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_5_post_ffn_layer_norm_bias', 'encoder_layer_5_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_5_post_ffn_layer_norm_scale', 'encoder_layer_5_post_ffn_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_35.tmp_1@GRAD'], Y@GRAD=['layer_norm_11.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_55@GRAD'], X=['fc_35.tmp_1'], Y=['layer_norm_11.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_35.tmp_0@GRAD'], Y@GRAD=['encoder_layer_5_ffn_fc_1.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_35.tmp_1@GRAD'], X=['fc_35.tmp_0'], Y=['encoder_layer_5_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_5_ffn_fc_1.b_0', 'encoder_layer_5_ffn_fc_1.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['tmp_54.cast_fp16@GRAD'], Y@GRAD=['encoder_layer_5_ffn_fc_1.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_35.tmp_0@GRAD'], X=['tmp_54.cast_fp16'], Y=['encoder_layer_5_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_5_ffn_fc_1.w_0', 'encoder_layer_5_ffn_fc_1.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['tmp_54@GRAD']} = cast(inputs={X=['tmp_54.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_role = 1, out_dtype = 5, use_mkldnn = False)
    {X@GRAD=['fc_34.tmp_1.cast_fp32@GRAD@RENAME@block0@0'], Y@GRAD=['tmp_53@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_54@GRAD'], X=['fc_34.tmp_1.cast_fp32'], Y=['tmp_53']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_52@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_53@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.5)
    {Out=['tanh_5.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_52@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 1.0)
    {X@GRAD=['tmp_51@GRAD']} = tanh_grad(inputs={Out=['tanh_5.tmp_0'], Out@GRAD=['tanh_5.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_50@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_51@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.7978845834732056)
    {X@GRAD=['fc_34.tmp_1.cast_fp32@GRAD@RENAME@block0@1'], Y@GRAD=['tmp_49@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_50@GRAD'], X=['fc_34.tmp_1.cast_fp32'], Y=['tmp_49']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['pow_5.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_49@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.044714998453855515)
    {X@GRAD=['fc_34.tmp_1.cast_fp32@GRAD@RENAME@block0@2']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_5.tmp_0@GRAD'], X=['fc_34.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 1, op_role_var = [])
    {Out=['fc_34.tmp_1.cast_fp32@GRAD']} = sum(inputs={X=['fc_34.tmp_1.cast_fp32@GRAD@RENAME@block0@0', 'fc_34.tmp_1.cast_fp32@GRAD@RENAME@block0@1', 'fc_34.tmp_1.cast_fp32@GRAD@RENAME@block0@2']}, op_device = , op_role = 1, use_mkldnn = False)
    {Out=['fc_34.tmp_1@GRAD']} = cast(inputs={X=['fc_34.tmp_1.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_role = 1, out_dtype = 4, use_mkldnn = False)
    {X@GRAD=['fc_34.tmp_0@GRAD'], Y@GRAD=['encoder_layer_5_ffn_fc_0.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_34.tmp_1@GRAD'], X=['fc_34.tmp_0'], Y=['encoder_layer_5_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_5_ffn_fc_0.b_0', 'encoder_layer_5_ffn_fc_0.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_11.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_5_ffn_fc_0.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_34.tmp_0@GRAD'], X=['layer_norm_11.tmp_2'], Y=['encoder_layer_5_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_5_ffn_fc_0.w_0', 'encoder_layer_5_ffn_fc_0.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_11.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_11.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_11.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_5_post_att_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_5_post_att_layer_norm_scale@GRAD'], X@GRAD=['tmp_48@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_5_post_att_layer_norm_bias'], Mean=['layer_norm_11.tmp_0'], Scale=['encoder_layer_5_post_att_layer_norm_scale'], Variance=['layer_norm_11.tmp_1'], X=['tmp_48'], Y@GRAD=['layer_norm_11.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_5_post_att_layer_norm_bias', 'encoder_layer_5_post_att_layer_norm_bias@GRAD', 'encoder_layer_5_post_att_layer_norm_scale', 'encoder_layer_5_post_att_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_33.tmp_1@GRAD'], Y@GRAD=['layer_norm_10.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_48@GRAD'], X=['fc_33.tmp_1'], Y=['layer_norm_10.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_33.tmp_0@GRAD'], Y@GRAD=['encoder_layer_5_multi_head_att_output_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_33.tmp_1@GRAD'], X=['fc_33.tmp_0'], Y=['encoder_layer_5_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_5_multi_head_att_output_fc.b_0', 'encoder_layer_5_multi_head_att_output_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_23.tmp_0@GRAD'], Y@GRAD=['encoder_layer_5_multi_head_att_output_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_33.tmp_0@GRAD'], X=['transpose_23.tmp_0'], Y=['encoder_layer_5_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_5_multi_head_att_output_fc.w_0', 'encoder_layer_5_multi_head_att_output_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['transpose_23.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['transpose_23.tmp_0@GRAD'], XShape=['reshape2_23.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_12.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_23.tmp_0@GRAD'], XShape=['transpose_23.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_5.tmp_0@GRAD'], Y@GRAD=['transpose_22.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_12.tmp_0@GRAD'], X=['softmax_5.tmp_0'], Y=['transpose_22.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {X@GRAD=['tmp_47@GRAD']} = softmax_grad(inputs={Out=['softmax_5.tmp_0'], Out@GRAD=['softmax_5.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_v2_11.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_47@GRAD'], X=['matmul_v2_11.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['scale_6.tmp_0@GRAD'], Y@GRAD=['transpose_21.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_11.tmp_0@GRAD'], X=['scale_6.tmp_0'], Y=['transpose_21.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['transpose_20.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_6.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.125)
    {X@GRAD=['fc_32.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_22.tmp_0@GRAD'], XShape=['transpose_22.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_32.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_32.tmp_1@GRAD'], XShape=['reshape2_22.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_31.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_21.tmp_0@GRAD'], XShape=['transpose_21.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_31.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_31.tmp_1@GRAD'], XShape=['reshape2_21.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_30.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_20.tmp_0@GRAD'], XShape=['transpose_20.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_30.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_30.tmp_1@GRAD'], XShape=['reshape2_20.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_32.tmp_0@GRAD'], Y@GRAD=['encoder_layer_5_multi_head_att_value_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_32.tmp_1@GRAD'], X=['fc_32.tmp_0'], Y=['encoder_layer_5_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_5_multi_head_att_value_fc.b_0', 'encoder_layer_5_multi_head_att_value_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_10.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_5_multi_head_att_value_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_32.tmp_0@GRAD'], X=['layer_norm_10.tmp_2'], Y=['encoder_layer_5_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_5_multi_head_att_value_fc.w_0', 'encoder_layer_5_multi_head_att_value_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_31.tmp_0@GRAD'], Y@GRAD=['encoder_layer_5_multi_head_att_key_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_31.tmp_1@GRAD'], X=['fc_31.tmp_0'], Y=['encoder_layer_5_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_5_multi_head_att_key_fc.b_0', 'encoder_layer_5_multi_head_att_key_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_10.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['encoder_layer_5_multi_head_att_key_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_31.tmp_0@GRAD'], X=['layer_norm_10.tmp_2'], Y=['encoder_layer_5_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_5_multi_head_att_key_fc.w_0', 'encoder_layer_5_multi_head_att_key_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_30.tmp_0@GRAD'], Y@GRAD=['encoder_layer_5_multi_head_att_query_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_30.tmp_1@GRAD'], X=['fc_30.tmp_0'], Y=['encoder_layer_5_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_5_multi_head_att_query_fc.b_0', 'encoder_layer_5_multi_head_att_query_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_10.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['encoder_layer_5_multi_head_att_query_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_30.tmp_0@GRAD'], X=['layer_norm_10.tmp_2'], Y=['encoder_layer_5_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_5_multi_head_att_query_fc.w_0', 'encoder_layer_5_multi_head_att_query_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_10.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_10.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_10.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_10.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_10.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_4_post_ffn_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_4_post_ffn_layer_norm_scale@GRAD'], X@GRAD=['tmp_46@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_4_post_ffn_layer_norm_bias'], Mean=['layer_norm_10.tmp_0'], Scale=['encoder_layer_4_post_ffn_layer_norm_scale'], Variance=['layer_norm_10.tmp_1'], X=['tmp_46'], Y@GRAD=['layer_norm_10.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_4_post_ffn_layer_norm_bias', 'encoder_layer_4_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_4_post_ffn_layer_norm_scale', 'encoder_layer_4_post_ffn_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_29.tmp_1@GRAD'], Y@GRAD=['layer_norm_9.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_46@GRAD'], X=['fc_29.tmp_1'], Y=['layer_norm_9.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_29.tmp_0@GRAD'], Y@GRAD=['encoder_layer_4_ffn_fc_1.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_29.tmp_1@GRAD'], X=['fc_29.tmp_0'], Y=['encoder_layer_4_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_4_ffn_fc_1.b_0', 'encoder_layer_4_ffn_fc_1.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['tmp_45.cast_fp16@GRAD'], Y@GRAD=['encoder_layer_4_ffn_fc_1.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_29.tmp_0@GRAD'], X=['tmp_45.cast_fp16'], Y=['encoder_layer_4_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_4_ffn_fc_1.w_0', 'encoder_layer_4_ffn_fc_1.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['tmp_45@GRAD']} = cast(inputs={X=['tmp_45.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_role = 1, out_dtype = 5, use_mkldnn = False)
    {X@GRAD=['fc_28.tmp_1.cast_fp32@GRAD@RENAME@block0@0'], Y@GRAD=['tmp_44@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_45@GRAD'], X=['fc_28.tmp_1.cast_fp32'], Y=['tmp_44']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_43@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_44@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.5)
    {Out=['tanh_4.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_43@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 1.0)
    {X@GRAD=['tmp_42@GRAD']} = tanh_grad(inputs={Out=['tanh_4.tmp_0'], Out@GRAD=['tanh_4.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_41@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_42@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.7978845834732056)
    {X@GRAD=['fc_28.tmp_1.cast_fp32@GRAD@RENAME@block0@1'], Y@GRAD=['tmp_40@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_41@GRAD'], X=['fc_28.tmp_1.cast_fp32'], Y=['tmp_40']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['pow_4.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_40@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.044714998453855515)
    {X@GRAD=['fc_28.tmp_1.cast_fp32@GRAD@RENAME@block0@2']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_4.tmp_0@GRAD'], X=['fc_28.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 1, op_role_var = [])
    {Out=['fc_28.tmp_1.cast_fp32@GRAD']} = sum(inputs={X=['fc_28.tmp_1.cast_fp32@GRAD@RENAME@block0@0', 'fc_28.tmp_1.cast_fp32@GRAD@RENAME@block0@1', 'fc_28.tmp_1.cast_fp32@GRAD@RENAME@block0@2']}, op_device = , op_role = 1, use_mkldnn = False)
    {Out=['fc_28.tmp_1@GRAD']} = cast(inputs={X=['fc_28.tmp_1.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_role = 1, out_dtype = 4, use_mkldnn = False)
    {X@GRAD=['fc_28.tmp_0@GRAD'], Y@GRAD=['encoder_layer_4_ffn_fc_0.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_28.tmp_1@GRAD'], X=['fc_28.tmp_0'], Y=['encoder_layer_4_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_4_ffn_fc_0.b_0', 'encoder_layer_4_ffn_fc_0.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_9.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_4_ffn_fc_0.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_28.tmp_0@GRAD'], X=['layer_norm_9.tmp_2'], Y=['encoder_layer_4_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_4_ffn_fc_0.w_0', 'encoder_layer_4_ffn_fc_0.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_9.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_9.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_9.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_4_post_att_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_4_post_att_layer_norm_scale@GRAD'], X@GRAD=['tmp_39@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_4_post_att_layer_norm_bias'], Mean=['layer_norm_9.tmp_0'], Scale=['encoder_layer_4_post_att_layer_norm_scale'], Variance=['layer_norm_9.tmp_1'], X=['tmp_39'], Y@GRAD=['layer_norm_9.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_4_post_att_layer_norm_bias', 'encoder_layer_4_post_att_layer_norm_bias@GRAD', 'encoder_layer_4_post_att_layer_norm_scale', 'encoder_layer_4_post_att_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_27.tmp_1@GRAD'], Y@GRAD=['layer_norm_8.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_39@GRAD'], X=['fc_27.tmp_1'], Y=['layer_norm_8.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_27.tmp_0@GRAD'], Y@GRAD=['encoder_layer_4_multi_head_att_output_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_27.tmp_1@GRAD'], X=['fc_27.tmp_0'], Y=['encoder_layer_4_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_4_multi_head_att_output_fc.b_0', 'encoder_layer_4_multi_head_att_output_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_19.tmp_0@GRAD'], Y@GRAD=['encoder_layer_4_multi_head_att_output_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_27.tmp_0@GRAD'], X=['transpose_19.tmp_0'], Y=['encoder_layer_4_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_4_multi_head_att_output_fc.w_0', 'encoder_layer_4_multi_head_att_output_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['transpose_19.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['transpose_19.tmp_0@GRAD'], XShape=['reshape2_19.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_10.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_19.tmp_0@GRAD'], XShape=['transpose_19.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_4.tmp_0@GRAD'], Y@GRAD=['transpose_18.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_10.tmp_0@GRAD'], X=['softmax_4.tmp_0'], Y=['transpose_18.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {X@GRAD=['tmp_38@GRAD']} = softmax_grad(inputs={Out=['softmax_4.tmp_0'], Out@GRAD=['softmax_4.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_v2_9.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_38@GRAD'], X=['matmul_v2_9.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['scale_5.tmp_0@GRAD'], Y@GRAD=['transpose_17.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_9.tmp_0@GRAD'], X=['scale_5.tmp_0'], Y=['transpose_17.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['transpose_16.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_5.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.125)
    {X@GRAD=['fc_26.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_18.tmp_0@GRAD'], XShape=['transpose_18.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_26.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_26.tmp_1@GRAD'], XShape=['reshape2_18.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_25.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_17.tmp_0@GRAD'], XShape=['transpose_17.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_25.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_25.tmp_1@GRAD'], XShape=['reshape2_17.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_24.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_16.tmp_0@GRAD'], XShape=['transpose_16.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_24.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_24.tmp_1@GRAD'], XShape=['reshape2_16.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_26.tmp_0@GRAD'], Y@GRAD=['encoder_layer_4_multi_head_att_value_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_26.tmp_1@GRAD'], X=['fc_26.tmp_0'], Y=['encoder_layer_4_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_4_multi_head_att_value_fc.b_0', 'encoder_layer_4_multi_head_att_value_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_8.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_4_multi_head_att_value_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_26.tmp_0@GRAD'], X=['layer_norm_8.tmp_2'], Y=['encoder_layer_4_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_4_multi_head_att_value_fc.w_0', 'encoder_layer_4_multi_head_att_value_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_25.tmp_0@GRAD'], Y@GRAD=['encoder_layer_4_multi_head_att_key_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_25.tmp_1@GRAD'], X=['fc_25.tmp_0'], Y=['encoder_layer_4_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_4_multi_head_att_key_fc.b_0', 'encoder_layer_4_multi_head_att_key_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_8.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['encoder_layer_4_multi_head_att_key_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_25.tmp_0@GRAD'], X=['layer_norm_8.tmp_2'], Y=['encoder_layer_4_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_4_multi_head_att_key_fc.w_0', 'encoder_layer_4_multi_head_att_key_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_24.tmp_0@GRAD'], Y@GRAD=['encoder_layer_4_multi_head_att_query_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_24.tmp_1@GRAD'], X=['fc_24.tmp_0'], Y=['encoder_layer_4_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_4_multi_head_att_query_fc.b_0', 'encoder_layer_4_multi_head_att_query_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_8.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['encoder_layer_4_multi_head_att_query_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_24.tmp_0@GRAD'], X=['layer_norm_8.tmp_2'], Y=['encoder_layer_4_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_4_multi_head_att_query_fc.w_0', 'encoder_layer_4_multi_head_att_query_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_8.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_8.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_8.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_8.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_8.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_3_post_ffn_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_3_post_ffn_layer_norm_scale@GRAD'], X@GRAD=['tmp_37@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_3_post_ffn_layer_norm_bias'], Mean=['layer_norm_8.tmp_0'], Scale=['encoder_layer_3_post_ffn_layer_norm_scale'], Variance=['layer_norm_8.tmp_1'], X=['tmp_37'], Y@GRAD=['layer_norm_8.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_3_post_ffn_layer_norm_bias', 'encoder_layer_3_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_3_post_ffn_layer_norm_scale', 'encoder_layer_3_post_ffn_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_23.tmp_1@GRAD'], Y@GRAD=['layer_norm_7.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_37@GRAD'], X=['fc_23.tmp_1'], Y=['layer_norm_7.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_23.tmp_0@GRAD'], Y@GRAD=['encoder_layer_3_ffn_fc_1.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_23.tmp_1@GRAD'], X=['fc_23.tmp_0'], Y=['encoder_layer_3_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_3_ffn_fc_1.b_0', 'encoder_layer_3_ffn_fc_1.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['tmp_36.cast_fp16@GRAD'], Y@GRAD=['encoder_layer_3_ffn_fc_1.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_23.tmp_0@GRAD'], X=['tmp_36.cast_fp16'], Y=['encoder_layer_3_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_3_ffn_fc_1.w_0', 'encoder_layer_3_ffn_fc_1.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['tmp_36@GRAD']} = cast(inputs={X=['tmp_36.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_role = 1, out_dtype = 5, use_mkldnn = False)
    {X@GRAD=['fc_22.tmp_1.cast_fp32@GRAD@RENAME@block0@0'], Y@GRAD=['tmp_35@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_36@GRAD'], X=['fc_22.tmp_1.cast_fp32'], Y=['tmp_35']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_34@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_35@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.5)
    {Out=['tanh_3.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_34@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 1.0)
    {X@GRAD=['tmp_33@GRAD']} = tanh_grad(inputs={Out=['tanh_3.tmp_0'], Out@GRAD=['tanh_3.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_32@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_33@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.7978845834732056)
    {X@GRAD=['fc_22.tmp_1.cast_fp32@GRAD@RENAME@block0@1'], Y@GRAD=['tmp_31@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_32@GRAD'], X=['fc_22.tmp_1.cast_fp32'], Y=['tmp_31']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['pow_3.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_31@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.044714998453855515)
    {X@GRAD=['fc_22.tmp_1.cast_fp32@GRAD@RENAME@block0@2']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_3.tmp_0@GRAD'], X=['fc_22.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 1, op_role_var = [])
    {Out=['fc_22.tmp_1.cast_fp32@GRAD']} = sum(inputs={X=['fc_22.tmp_1.cast_fp32@GRAD@RENAME@block0@0', 'fc_22.tmp_1.cast_fp32@GRAD@RENAME@block0@1', 'fc_22.tmp_1.cast_fp32@GRAD@RENAME@block0@2']}, op_device = , op_role = 1, use_mkldnn = False)
    {Out=['fc_22.tmp_1@GRAD']} = cast(inputs={X=['fc_22.tmp_1.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_role = 1, out_dtype = 4, use_mkldnn = False)
    {X@GRAD=['fc_22.tmp_0@GRAD'], Y@GRAD=['encoder_layer_3_ffn_fc_0.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_22.tmp_1@GRAD'], X=['fc_22.tmp_0'], Y=['encoder_layer_3_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_3_ffn_fc_0.b_0', 'encoder_layer_3_ffn_fc_0.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_7.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_3_ffn_fc_0.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_22.tmp_0@GRAD'], X=['layer_norm_7.tmp_2'], Y=['encoder_layer_3_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_3_ffn_fc_0.w_0', 'encoder_layer_3_ffn_fc_0.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_7.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_7.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_7.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_3_post_att_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_3_post_att_layer_norm_scale@GRAD'], X@GRAD=['tmp_30@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_3_post_att_layer_norm_bias'], Mean=['layer_norm_7.tmp_0'], Scale=['encoder_layer_3_post_att_layer_norm_scale'], Variance=['layer_norm_7.tmp_1'], X=['tmp_30'], Y@GRAD=['layer_norm_7.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_3_post_att_layer_norm_bias', 'encoder_layer_3_post_att_layer_norm_bias@GRAD', 'encoder_layer_3_post_att_layer_norm_scale', 'encoder_layer_3_post_att_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_21.tmp_1@GRAD'], Y@GRAD=['layer_norm_6.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_30@GRAD'], X=['fc_21.tmp_1'], Y=['layer_norm_6.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_21.tmp_0@GRAD'], Y@GRAD=['encoder_layer_3_multi_head_att_output_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_21.tmp_1@GRAD'], X=['fc_21.tmp_0'], Y=['encoder_layer_3_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_3_multi_head_att_output_fc.b_0', 'encoder_layer_3_multi_head_att_output_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_15.tmp_0@GRAD'], Y@GRAD=['encoder_layer_3_multi_head_att_output_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_21.tmp_0@GRAD'], X=['transpose_15.tmp_0'], Y=['encoder_layer_3_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_3_multi_head_att_output_fc.w_0', 'encoder_layer_3_multi_head_att_output_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['transpose_15.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['transpose_15.tmp_0@GRAD'], XShape=['reshape2_15.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_8.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_15.tmp_0@GRAD'], XShape=['transpose_15.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_3.tmp_0@GRAD'], Y@GRAD=['transpose_14.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_8.tmp_0@GRAD'], X=['softmax_3.tmp_0'], Y=['transpose_14.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {X@GRAD=['tmp_29@GRAD']} = softmax_grad(inputs={Out=['softmax_3.tmp_0'], Out@GRAD=['softmax_3.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_v2_7.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_29@GRAD'], X=['matmul_v2_7.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['scale_4.tmp_0@GRAD'], Y@GRAD=['transpose_13.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_7.tmp_0@GRAD'], X=['scale_4.tmp_0'], Y=['transpose_13.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['transpose_12.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_4.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.125)
    {X@GRAD=['fc_20.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_14.tmp_0@GRAD'], XShape=['transpose_14.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_20.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_20.tmp_1@GRAD'], XShape=['reshape2_14.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_19.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_13.tmp_0@GRAD'], XShape=['transpose_13.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_19.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_19.tmp_1@GRAD'], XShape=['reshape2_13.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_18.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_12.tmp_0@GRAD'], XShape=['transpose_12.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_18.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_18.tmp_1@GRAD'], XShape=['reshape2_12.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_20.tmp_0@GRAD'], Y@GRAD=['encoder_layer_3_multi_head_att_value_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_20.tmp_1@GRAD'], X=['fc_20.tmp_0'], Y=['encoder_layer_3_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_3_multi_head_att_value_fc.b_0', 'encoder_layer_3_multi_head_att_value_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_6.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_3_multi_head_att_value_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_20.tmp_0@GRAD'], X=['layer_norm_6.tmp_2'], Y=['encoder_layer_3_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_3_multi_head_att_value_fc.w_0', 'encoder_layer_3_multi_head_att_value_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_19.tmp_0@GRAD'], Y@GRAD=['encoder_layer_3_multi_head_att_key_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_19.tmp_1@GRAD'], X=['fc_19.tmp_0'], Y=['encoder_layer_3_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_3_multi_head_att_key_fc.b_0', 'encoder_layer_3_multi_head_att_key_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_6.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['encoder_layer_3_multi_head_att_key_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_19.tmp_0@GRAD'], X=['layer_norm_6.tmp_2'], Y=['encoder_layer_3_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_3_multi_head_att_key_fc.w_0', 'encoder_layer_3_multi_head_att_key_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_18.tmp_0@GRAD'], Y@GRAD=['encoder_layer_3_multi_head_att_query_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_18.tmp_1@GRAD'], X=['fc_18.tmp_0'], Y=['encoder_layer_3_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_3_multi_head_att_query_fc.b_0', 'encoder_layer_3_multi_head_att_query_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_6.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['encoder_layer_3_multi_head_att_query_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_18.tmp_0@GRAD'], X=['layer_norm_6.tmp_2'], Y=['encoder_layer_3_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_3_multi_head_att_query_fc.w_0', 'encoder_layer_3_multi_head_att_query_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_6.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_6.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_6.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_6.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_6.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_2_post_ffn_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_2_post_ffn_layer_norm_scale@GRAD'], X@GRAD=['tmp_28@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_2_post_ffn_layer_norm_bias'], Mean=['layer_norm_6.tmp_0'], Scale=['encoder_layer_2_post_ffn_layer_norm_scale'], Variance=['layer_norm_6.tmp_1'], X=['tmp_28'], Y@GRAD=['layer_norm_6.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_post_ffn_layer_norm_bias', 'encoder_layer_2_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_2_post_ffn_layer_norm_scale', 'encoder_layer_2_post_ffn_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_17.tmp_1@GRAD'], Y@GRAD=['layer_norm_5.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_28@GRAD'], X=['fc_17.tmp_1'], Y=['layer_norm_5.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_17.tmp_0@GRAD'], Y@GRAD=['encoder_layer_2_ffn_fc_1.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_17.tmp_1@GRAD'], X=['fc_17.tmp_0'], Y=['encoder_layer_2_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_ffn_fc_1.b_0', 'encoder_layer_2_ffn_fc_1.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['tmp_27.cast_fp16@GRAD'], Y@GRAD=['encoder_layer_2_ffn_fc_1.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_17.tmp_0@GRAD'], X=['tmp_27.cast_fp16'], Y=['encoder_layer_2_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_ffn_fc_1.w_0', 'encoder_layer_2_ffn_fc_1.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['tmp_27@GRAD']} = cast(inputs={X=['tmp_27.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_role = 1, out_dtype = 5, use_mkldnn = False)
    {X@GRAD=['fc_16.tmp_1.cast_fp32@GRAD@RENAME@block0@0'], Y@GRAD=['tmp_26@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_27@GRAD'], X=['fc_16.tmp_1.cast_fp32'], Y=['tmp_26']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_25@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_26@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.5)
    {Out=['tanh_2.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_25@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 1.0)
    {X@GRAD=['tmp_24@GRAD']} = tanh_grad(inputs={Out=['tanh_2.tmp_0'], Out@GRAD=['tanh_2.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_23@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_24@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.7978845834732056)
    {X@GRAD=['fc_16.tmp_1.cast_fp32@GRAD@RENAME@block0@1'], Y@GRAD=['tmp_22@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_23@GRAD'], X=['fc_16.tmp_1.cast_fp32'], Y=['tmp_22']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['pow_2.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_22@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.044714998453855515)
    {X@GRAD=['fc_16.tmp_1.cast_fp32@GRAD@RENAME@block0@2']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_2.tmp_0@GRAD'], X=['fc_16.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 1, op_role_var = [])
    {Out=['fc_16.tmp_1.cast_fp32@GRAD']} = sum(inputs={X=['fc_16.tmp_1.cast_fp32@GRAD@RENAME@block0@0', 'fc_16.tmp_1.cast_fp32@GRAD@RENAME@block0@1', 'fc_16.tmp_1.cast_fp32@GRAD@RENAME@block0@2']}, op_device = , op_role = 1, use_mkldnn = False)
    {Out=['fc_16.tmp_1@GRAD']} = cast(inputs={X=['fc_16.tmp_1.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_role = 1, out_dtype = 4, use_mkldnn = False)
    {X@GRAD=['fc_16.tmp_0@GRAD'], Y@GRAD=['encoder_layer_2_ffn_fc_0.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_16.tmp_1@GRAD'], X=['fc_16.tmp_0'], Y=['encoder_layer_2_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_ffn_fc_0.b_0', 'encoder_layer_2_ffn_fc_0.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_5.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_2_ffn_fc_0.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_16.tmp_0@GRAD'], X=['layer_norm_5.tmp_2'], Y=['encoder_layer_2_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_ffn_fc_0.w_0', 'encoder_layer_2_ffn_fc_0.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_5.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_5.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_5.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_2_post_att_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_2_post_att_layer_norm_scale@GRAD'], X@GRAD=['tmp_21@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_2_post_att_layer_norm_bias'], Mean=['layer_norm_5.tmp_0'], Scale=['encoder_layer_2_post_att_layer_norm_scale'], Variance=['layer_norm_5.tmp_1'], X=['tmp_21'], Y@GRAD=['layer_norm_5.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_post_att_layer_norm_bias', 'encoder_layer_2_post_att_layer_norm_bias@GRAD', 'encoder_layer_2_post_att_layer_norm_scale', 'encoder_layer_2_post_att_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_15.tmp_1@GRAD'], Y@GRAD=['layer_norm_4.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_21@GRAD'], X=['fc_15.tmp_1'], Y=['layer_norm_4.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_15.tmp_0@GRAD'], Y@GRAD=['encoder_layer_2_multi_head_att_output_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_15.tmp_1@GRAD'], X=['fc_15.tmp_0'], Y=['encoder_layer_2_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.b_0', 'encoder_layer_2_multi_head_att_output_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_11.tmp_0@GRAD'], Y@GRAD=['encoder_layer_2_multi_head_att_output_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_15.tmp_0@GRAD'], X=['transpose_11.tmp_0'], Y=['encoder_layer_2_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.w_0', 'encoder_layer_2_multi_head_att_output_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['transpose_11.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['transpose_11.tmp_0@GRAD'], XShape=['reshape2_11.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_6.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_11.tmp_0@GRAD'], XShape=['transpose_11.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_2.tmp_0@GRAD'], Y@GRAD=['transpose_10.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_6.tmp_0@GRAD'], X=['softmax_2.tmp_0'], Y=['transpose_10.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {X@GRAD=['tmp_20@GRAD']} = softmax_grad(inputs={Out=['softmax_2.tmp_0'], Out@GRAD=['softmax_2.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_v2_5.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_20@GRAD'], X=['matmul_v2_5.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['scale_3.tmp_0@GRAD'], Y@GRAD=['transpose_9.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_5.tmp_0@GRAD'], X=['scale_3.tmp_0'], Y=['transpose_9.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['transpose_8.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_3.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.125)
    {X@GRAD=['fc_14.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_10.tmp_0@GRAD'], XShape=['transpose_10.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_14.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_14.tmp_1@GRAD'], XShape=['reshape2_10.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_13.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_9.tmp_0@GRAD'], XShape=['transpose_9.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_13.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_13.tmp_1@GRAD'], XShape=['reshape2_9.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_12.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_8.tmp_0@GRAD'], XShape=['transpose_8.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_12.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_12.tmp_1@GRAD'], XShape=['reshape2_8.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_14.tmp_0@GRAD'], Y@GRAD=['encoder_layer_2_multi_head_att_value_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_14.tmp_1@GRAD'], X=['fc_14.tmp_0'], Y=['encoder_layer_2_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.b_0', 'encoder_layer_2_multi_head_att_value_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_4.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_2_multi_head_att_value_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_14.tmp_0@GRAD'], X=['layer_norm_4.tmp_2'], Y=['encoder_layer_2_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.w_0', 'encoder_layer_2_multi_head_att_value_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_13.tmp_0@GRAD'], Y@GRAD=['encoder_layer_2_multi_head_att_key_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_13.tmp_1@GRAD'], X=['fc_13.tmp_0'], Y=['encoder_layer_2_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.b_0', 'encoder_layer_2_multi_head_att_key_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_4.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['encoder_layer_2_multi_head_att_key_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_13.tmp_0@GRAD'], X=['layer_norm_4.tmp_2'], Y=['encoder_layer_2_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.w_0', 'encoder_layer_2_multi_head_att_key_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_12.tmp_0@GRAD'], Y@GRAD=['encoder_layer_2_multi_head_att_query_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_12.tmp_1@GRAD'], X=['fc_12.tmp_0'], Y=['encoder_layer_2_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.b_0', 'encoder_layer_2_multi_head_att_query_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_4.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['encoder_layer_2_multi_head_att_query_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_12.tmp_0@GRAD'], X=['layer_norm_4.tmp_2'], Y=['encoder_layer_2_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.w_0', 'encoder_layer_2_multi_head_att_query_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_4.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_4.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_4.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_4.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_4.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_1_post_ffn_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_1_post_ffn_layer_norm_scale@GRAD'], X@GRAD=['tmp_19@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_1_post_ffn_layer_norm_bias'], Mean=['layer_norm_4.tmp_0'], Scale=['encoder_layer_1_post_ffn_layer_norm_scale'], Variance=['layer_norm_4.tmp_1'], X=['tmp_19'], Y@GRAD=['layer_norm_4.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_post_ffn_layer_norm_bias', 'encoder_layer_1_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_1_post_ffn_layer_norm_scale', 'encoder_layer_1_post_ffn_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_11.tmp_1@GRAD'], Y@GRAD=['layer_norm_3.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_19@GRAD'], X=['fc_11.tmp_1'], Y=['layer_norm_3.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_11.tmp_0@GRAD'], Y@GRAD=['encoder_layer_1_ffn_fc_1.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_11.tmp_1@GRAD'], X=['fc_11.tmp_0'], Y=['encoder_layer_1_ffn_fc_1.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_ffn_fc_1.b_0', 'encoder_layer_1_ffn_fc_1.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['tmp_18.cast_fp16@GRAD'], Y@GRAD=['encoder_layer_1_ffn_fc_1.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_11.tmp_0@GRAD'], X=['tmp_18.cast_fp16'], Y=['encoder_layer_1_ffn_fc_1.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_ffn_fc_1.w_0', 'encoder_layer_1_ffn_fc_1.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['tmp_18@GRAD']} = cast(inputs={X=['tmp_18.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_role = 1, out_dtype = 5, use_mkldnn = False)
    {X@GRAD=['fc_10.tmp_1.cast_fp32@GRAD@RENAME@block0@0'], Y@GRAD=['tmp_17@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_18@GRAD'], X=['fc_10.tmp_1.cast_fp32'], Y=['tmp_17']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_16@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_17@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.5)
    {Out=['tanh_1.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_16@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 1.0)
    {X@GRAD=['tmp_15@GRAD']} = tanh_grad(inputs={Out=['tanh_1.tmp_0'], Out@GRAD=['tanh_1.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_14@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_15@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.7978845834732056)
    {X@GRAD=['fc_10.tmp_1.cast_fp32@GRAD@RENAME@block0@1'], Y@GRAD=['tmp_13@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_14@GRAD'], X=['fc_10.tmp_1.cast_fp32'], Y=['tmp_13']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['pow_1.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_13@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.044714998453855515)
    {X@GRAD=['fc_10.tmp_1.cast_fp32@GRAD@RENAME@block0@2']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_1.tmp_0@GRAD'], X=['fc_10.tmp_1.cast_fp32']}, factor = 3.0, op_device = , op_namescope = /, op_role = 1, op_role_var = [])
    {Out=['fc_10.tmp_1.cast_fp32@GRAD']} = sum(inputs={X=['fc_10.tmp_1.cast_fp32@GRAD@RENAME@block0@0', 'fc_10.tmp_1.cast_fp32@GRAD@RENAME@block0@1', 'fc_10.tmp_1.cast_fp32@GRAD@RENAME@block0@2']}, op_device = , op_role = 1, use_mkldnn = False)
    {Out=['fc_10.tmp_1@GRAD']} = cast(inputs={X=['fc_10.tmp_1.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_role = 1, out_dtype = 4, use_mkldnn = False)
    {X@GRAD=['fc_10.tmp_0@GRAD'], Y@GRAD=['encoder_layer_1_ffn_fc_0.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_10.tmp_1@GRAD'], X=['fc_10.tmp_0'], Y=['encoder_layer_1_ffn_fc_0.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_ffn_fc_0.b_0', 'encoder_layer_1_ffn_fc_0.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_3.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_1_ffn_fc_0.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_10.tmp_0@GRAD'], X=['layer_norm_3.tmp_2'], Y=['encoder_layer_1_ffn_fc_0.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_ffn_fc_0.w_0', 'encoder_layer_1_ffn_fc_0.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_3.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_3.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_3.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_1_post_att_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_1_post_att_layer_norm_scale@GRAD'], X@GRAD=['tmp_12@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_1_post_att_layer_norm_bias'], Mean=['layer_norm_3.tmp_0'], Scale=['encoder_layer_1_post_att_layer_norm_scale'], Variance=['layer_norm_3.tmp_1'], X=['tmp_12'], Y@GRAD=['layer_norm_3.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_post_att_layer_norm_bias', 'encoder_layer_1_post_att_layer_norm_bias@GRAD', 'encoder_layer_1_post_att_layer_norm_scale', 'encoder_layer_1_post_att_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_9.tmp_1@GRAD'], Y@GRAD=['layer_norm_2.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_12@GRAD'], X=['fc_9.tmp_1'], Y=['layer_norm_2.tmp_2']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_9.tmp_0@GRAD'], Y@GRAD=['encoder_layer_1_multi_head_att_output_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_9.tmp_1@GRAD'], X=['fc_9.tmp_0'], Y=['encoder_layer_1_multi_head_att_output_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.b_0', 'encoder_layer_1_multi_head_att_output_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_7.tmp_0@GRAD'], Y@GRAD=['encoder_layer_1_multi_head_att_output_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_9.tmp_0@GRAD'], X=['transpose_7.tmp_0'], Y=['encoder_layer_1_multi_head_att_output_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.w_0', 'encoder_layer_1_multi_head_att_output_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['transpose_7.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['transpose_7.tmp_0@GRAD'], XShape=['reshape2_7.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_4.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_7.tmp_0@GRAD'], XShape=['transpose_7.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_1.tmp_0@GRAD'], Y@GRAD=['transpose_6.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_4.tmp_0@GRAD'], X=['softmax_1.tmp_0'], Y=['transpose_6.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {X@GRAD=['tmp_11@GRAD']} = softmax_grad(inputs={Out=['softmax_1.tmp_0'], Out@GRAD=['softmax_1.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_v2_3.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_11@GRAD'], X=['matmul_v2_3.tmp_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['scale_2.tmp_0@GRAD'], Y@GRAD=['transpose_5.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_3.tmp_0@GRAD'], X=['scale_2.tmp_0'], Y=['transpose_5.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['transpose_4.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_2.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.125)
    {X@GRAD=['fc_8.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_6.tmp_0@GRAD'], XShape=['transpose_6.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_8.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_8.tmp_1@GRAD'], XShape=['reshape2_6.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_7.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_5.tmp_0@GRAD'], XShape=['transpose_5.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_7.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_7.tmp_1@GRAD'], XShape=['reshape2_5.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_6.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_4.tmp_0@GRAD'], XShape=['transpose_4.tmp_1']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_6.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_6.tmp_1@GRAD'], XShape=['reshape2_4.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_8.tmp_0@GRAD'], Y@GRAD=['encoder_layer_1_multi_head_att_value_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_8.tmp_1@GRAD'], X=['fc_8.tmp_0'], Y=['encoder_layer_1_multi_head_att_value_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.b_0', 'encoder_layer_1_multi_head_att_value_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_2.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_1_multi_head_att_value_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_8.tmp_0@GRAD'], X=['layer_norm_2.tmp_2'], Y=['encoder_layer_1_multi_head_att_value_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.w_0', 'encoder_layer_1_multi_head_att_value_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_7.tmp_0@GRAD'], Y@GRAD=['encoder_layer_1_multi_head_att_key_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_7.tmp_1@GRAD'], X=['fc_7.tmp_0'], Y=['encoder_layer_1_multi_head_att_key_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.b_0', 'encoder_layer_1_multi_head_att_key_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_2.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['encoder_layer_1_multi_head_att_key_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_7.tmp_0@GRAD'], X=['layer_norm_2.tmp_2'], Y=['encoder_layer_1_multi_head_att_key_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.w_0', 'encoder_layer_1_multi_head_att_key_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_6.tmp_0@GRAD'], Y@GRAD=['encoder_layer_1_multi_head_att_query_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_6.tmp_1@GRAD'], X=['fc_6.tmp_0'], Y=['encoder_layer_1_multi_head_att_query_fc.b_0.cast_fp16']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.b_0', 'encoder_layer_1_multi_head_att_query_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_2.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['encoder_layer_1_multi_head_att_query_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_6.tmp_0@GRAD'], X=['layer_norm_2.tmp_2'], Y=['encoder_layer_1_multi_head_att_query_fc.w_0.cast_fp16']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.w_0', 'encoder_layer_1_multi_head_att_query_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['embedding_1.tmp_0.subprog_0']} = lookup_table_v2(inputs={Ids=['src_ids'], W=['word_embedding']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Out=['embedding_3.tmp_0.subprog_0']} = lookup_table_v2(inputs={Ids=['pos_ids'], W=['pos_embedding']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Out=['embedding_5.tmp_0.subprog_0']} = lookup_table_v2(inputs={Ids=['sent_ids'], W=['sent_embedding']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Out=['tmp_0.subprog_0']} = elementwise_add(inputs={X=['embedding_1.tmp_0.subprog_0'], Y=['embedding_3.tmp_0.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_1.subprog_0']} = elementwise_add(inputs={X=['tmp_0.subprog_0'], Y=['embedding_5.tmp_0.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_1.cast_fp16.subprog_0']} = cast(inputs={X=['tmp_1.subprog_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Mean=['layer_norm_0.tmp_0.subprog_0'], Variance=['layer_norm_0.tmp_1.subprog_0'], Y=['layer_norm_0.tmp_2.subprog_0']} = layer_norm(inputs={Bias=['pre_encoder_layer_norm_bias'], Scale=['pre_encoder_layer_norm_scale'], X=['tmp_1.cast_fp16.subprog_0']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_0_multi_head_att_query_fc.w_0.cast_fp16.subprog_0']} = cast(inputs={X=['encoder_layer_0_multi_head_att_query_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_0.tmp_0.subprog_0']} = mul(inputs={X=['layer_norm_0.tmp_2.subprog_0'], Y=['encoder_layer_0_multi_head_att_query_fc.w_0.cast_fp16.subprog_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_0_multi_head_att_query_fc.b_0.cast_fp16.subprog_0']} = cast(inputs={X=['encoder_layer_0_multi_head_att_query_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_0.tmp_1.subprog_0']} = elementwise_add(inputs={X=['fc_0.tmp_0.subprog_0'], Y=['encoder_layer_0_multi_head_att_query_fc.b_0.cast_fp16.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_0_multi_head_att_key_fc.w_0.cast_fp16.subprog_0']} = cast(inputs={X=['encoder_layer_0_multi_head_att_key_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_1.tmp_0.subprog_0']} = mul(inputs={X=['layer_norm_0.tmp_2.subprog_0'], Y=['encoder_layer_0_multi_head_att_key_fc.w_0.cast_fp16.subprog_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_0_multi_head_att_key_fc.b_0.cast_fp16.subprog_0']} = cast(inputs={X=['encoder_layer_0_multi_head_att_key_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_1.tmp_1.subprog_0']} = elementwise_add(inputs={X=['fc_1.tmp_0.subprog_0'], Y=['encoder_layer_0_multi_head_att_key_fc.b_0.cast_fp16.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_0_multi_head_att_value_fc.w_0.cast_fp16.subprog_0']} = cast(inputs={X=['encoder_layer_0_multi_head_att_value_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_2.tmp_0.subprog_0']} = mul(inputs={X=['layer_norm_0.tmp_2.subprog_0'], Y=['encoder_layer_0_multi_head_att_value_fc.w_0.cast_fp16.subprog_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_0_multi_head_att_value_fc.b_0.cast_fp16.subprog_0']} = cast(inputs={X=['encoder_layer_0_multi_head_att_value_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_2.tmp_1.subprog_0']} = elementwise_add(inputs={X=['fc_2.tmp_0.subprog_0'], Y=['encoder_layer_0_multi_head_att_value_fc.b_0.cast_fp16.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_0.tmp_1.subprog_0'], XShape=['reshape2_0.tmp_0.subprog_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_0.tmp_1.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_0.tmp_0.subprog_0'], XShape=['transpose_0.tmp_1.subprog_0']} = transpose2(inputs={X=['fc_0.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_1.tmp_1.subprog_0'], XShape=['reshape2_1.tmp_0.subprog_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_1.tmp_1.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_1.tmp_0.subprog_0'], XShape=['transpose_1.tmp_1.subprog_0']} = transpose2(inputs={X=['fc_1.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['fc_2.tmp_1.subprog_0'], XShape=['reshape2_2.tmp_0.subprog_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['fc_2.tmp_1.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {Out=['transpose_2.tmp_0.subprog_0'], XShape=['transpose_2.tmp_1.subprog_0']} = transpose2(inputs={X=['fc_2.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['scale_1.tmp_0.subprog_0']} = scale(inputs={ScaleTensor=[], X=['transpose_0.tmp_0.subprog_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], scale = 0.125, use_mkldnn = False)
    {Out=['matmul_v2_1.tmp_0.subprog_0']} = matmul_v2(inputs={X=['scale_1.tmp_0.subprog_0'], Y=['transpose_1.tmp_0.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['tmp_2.subprog_0']} = elementwise_add(inputs={X=['matmul_v2_1.tmp_0.subprog_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['softmax_0.tmp_0.subprog_0']} = softmax(inputs={X=['tmp_2.subprog_0']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {Out=['matmul_v2_2.tmp_0.subprog_0']} = matmul_v2(inputs={X=['softmax_0.tmp_0.subprog_0'], Y=['transpose_2.tmp_0.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {Out=['transpose_3.tmp_0.subprog_0'], XShape=['transpose_3.tmp_1.subprog_0']} = transpose2(inputs={X=['matmul_v2_2.tmp_0.subprog_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {Out=['transpose_3.tmp_0.subprog_0'], XShape=['reshape2_3.tmp_0.subprog_0']} = reshape2(inputs={Shape=[], ShapeTensor=[], X=['transpose_3.tmp_0.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {Out=['encoder_layer_0_multi_head_att_output_fc.w_0.cast_fp16.subprog_0']} = cast(inputs={X=['encoder_layer_0_multi_head_att_output_fc.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_3.tmp_0.subprog_0']} = mul(inputs={X=['transpose_3.tmp_0.subprog_0'], Y=['encoder_layer_0_multi_head_att_output_fc.w_0.cast_fp16.subprog_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_0_multi_head_att_output_fc.b_0.cast_fp16.subprog_0']} = cast(inputs={X=['encoder_layer_0_multi_head_att_output_fc.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_3.tmp_1.subprog_0']} = elementwise_add(inputs={X=['fc_3.tmp_0.subprog_0'], Y=['encoder_layer_0_multi_head_att_output_fc.b_0.cast_fp16.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_3.subprog_0']} = elementwise_add(inputs={X=['fc_3.tmp_1.subprog_0'], Y=['layer_norm_0.tmp_2.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_1.tmp_0.subprog_0'], Variance=['layer_norm_1.tmp_1.subprog_0'], Y=['layer_norm_1.tmp_2.subprog_0']} = layer_norm(inputs={Bias=['encoder_layer_0_post_att_layer_norm_bias'], Scale=['encoder_layer_0_post_att_layer_norm_scale'], X=['tmp_3.subprog_0']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_0_ffn_fc_0.w_0.cast_fp16.subprog_0']} = cast(inputs={X=['encoder_layer_0_ffn_fc_0.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_4.tmp_0.subprog_0']} = mul(inputs={X=['layer_norm_1.tmp_2.subprog_0'], Y=['encoder_layer_0_ffn_fc_0.w_0.cast_fp16.subprog_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_0_ffn_fc_0.b_0.cast_fp16.subprog_0']} = cast(inputs={X=['encoder_layer_0_ffn_fc_0.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_4.tmp_1.subprog_0']} = elementwise_add(inputs={X=['fc_4.tmp_0.subprog_0'], Y=['encoder_layer_0_ffn_fc_0.b_0.cast_fp16.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['fc_4.tmp_1.cast_fp32.subprog_0']} = cast(inputs={X=['fc_4.tmp_1.subprog_0']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['pow_0.tmp_0.subprog_0']} = pow(inputs={FactorTensor=[], X=['fc_4.tmp_1.cast_fp32.subprog_0']}, factor = 3.0, op_device = , op_namescope = /, op_role = 1, op_role_var = [])
    {Out=['tmp_4.subprog_0']} = scale(inputs={ScaleTensor=[], X=['pow_0.tmp_0.subprog_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], scale = 0.044714998453855515, use_mkldnn = False)
    {Out=['tmp_5.subprog_0']} = elementwise_add(inputs={X=['fc_4.tmp_1.cast_fp32.subprog_0'], Y=['tmp_4.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_6.subprog_0']} = scale(inputs={ScaleTensor=[], X=['tmp_5.subprog_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], scale = 0.7978845834732056, use_mkldnn = False)
    {Out=['tanh_0.tmp_0.subprog_0']} = tanh(inputs={X=['tmp_6.subprog_0']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_7.subprog_0']} = scale(inputs={ScaleTensor=[], X=['tanh_0.tmp_0.subprog_0']}, bias = 1.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], scale = 1.0, use_mkldnn = False)
    {Out=['tmp_8.subprog_0']} = scale(inputs={ScaleTensor=[], X=['tmp_7.subprog_0']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 1, op_role_var = [], scale = 0.5, use_mkldnn = False)
    {Out=['tmp_9.subprog_0']} = elementwise_mul(inputs={X=['fc_4.tmp_1.cast_fp32.subprog_0'], Y=['tmp_8.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['encoder_layer_0_ffn_fc_1.w_0.cast_fp16.subprog_0']} = cast(inputs={X=['encoder_layer_0_ffn_fc_1.w_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['tmp_9.cast_fp16.subprog_0']} = cast(inputs={X=['tmp_9.subprog_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_5.tmp_0.subprog_0']} = mul(inputs={X=['tmp_9.cast_fp16.subprog_0'], Y=['encoder_layer_0_ffn_fc_1.w_0.cast_fp16.subprog_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = [], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['encoder_layer_0_ffn_fc_1.b_0.cast_fp16.subprog_0']} = cast(inputs={X=['encoder_layer_0_ffn_fc_1.b_0']}, in_dtype = 5, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 4, use_mkldnn = False)
    {Out=['fc_5.tmp_1.subprog_0']} = elementwise_add(inputs={X=['fc_5.tmp_0.subprog_0'], Y=['encoder_layer_0_ffn_fc_1.b_0.cast_fp16.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_10.subprog_0']} = elementwise_add(inputs={X=['fc_5.tmp_1.subprog_0'], Y=['layer_norm_1.tmp_2.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Mean=['layer_norm_2.tmp_0.subprog_0'], Variance=['layer_norm_2.tmp_1.subprog_0'], Y=['layer_norm_2.tmp_2']} = layer_norm(inputs={Bias=['encoder_layer_0_post_ffn_layer_norm_bias'], Scale=['encoder_layer_0_post_ffn_layer_norm_scale'], X=['tmp_10.subprog_0']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['layer_norm_2.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_2.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_2.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_2.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_2.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_0_post_ffn_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_0_post_ffn_layer_norm_scale@GRAD'], X@GRAD=['tmp_10@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_0_post_ffn_layer_norm_bias'], Mean=['layer_norm_2.tmp_0.subprog_0'], Scale=['encoder_layer_0_post_ffn_layer_norm_scale'], Variance=['layer_norm_2.tmp_1.subprog_0'], X=['tmp_10.subprog_0'], Y@GRAD=['layer_norm_2.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_post_ffn_layer_norm_bias', 'encoder_layer_0_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_0_post_ffn_layer_norm_scale', 'encoder_layer_0_post_ffn_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_5.tmp_1@GRAD'], Y@GRAD=['layer_norm_1.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_10@GRAD'], X=['fc_5.tmp_1.subprog_0'], Y=['layer_norm_1.tmp_2.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_5.tmp_0@GRAD'], Y@GRAD=['encoder_layer_0_ffn_fc_1.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_5.tmp_1@GRAD'], X=['fc_5.tmp_0.subprog_0'], Y=['encoder_layer_0_ffn_fc_1.b_0.cast_fp16.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_ffn_fc_1.b_0', 'encoder_layer_0_ffn_fc_1.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['tmp_9.cast_fp16@GRAD'], Y@GRAD=['encoder_layer_0_ffn_fc_1.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_5.tmp_0@GRAD'], X=['tmp_9.cast_fp16.subprog_0'], Y=['encoder_layer_0_ffn_fc_1.w_0.cast_fp16.subprog_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_ffn_fc_1.w_0', 'encoder_layer_0_ffn_fc_1.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['tmp_9@GRAD']} = cast(inputs={X=['tmp_9.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_role = 1, out_dtype = 5, use_mkldnn = False)
    {X@GRAD=['fc_4.tmp_1.cast_fp32@GRAD@RENAME@block0@0'], Y@GRAD=['tmp_8@GRAD']} = elementwise_mul_grad(inputs={Out@GRAD=['tmp_9@GRAD'], X=['fc_4.tmp_1.cast_fp32.subprog_0'], Y=['tmp_8.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_7@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_8@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.5)
    {Out=['tanh_0.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_7@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 1.0)
    {X@GRAD=['tmp_6@GRAD']} = tanh_grad(inputs={Out=['tanh_0.tmp_0.subprog_0'], Out@GRAD=['tanh_0.tmp_0@GRAD']}, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = False, use_mkldnn = False)
    {Out=['tmp_5@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_6@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.7978845834732056)
    {X@GRAD=['fc_4.tmp_1.cast_fp32@GRAD@RENAME@block0@1'], Y@GRAD=['tmp_4@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_5@GRAD'], X=['fc_4.tmp_1.cast_fp32.subprog_0'], Y=['tmp_4.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['pow_0.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['tmp_4@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.044714998453855515)
    {X@GRAD=['fc_4.tmp_1.cast_fp32@GRAD@RENAME@block0@2']} = pow_grad(inputs={FactorTensor=[], Out@GRAD=['pow_0.tmp_0@GRAD'], X=['fc_4.tmp_1.cast_fp32.subprog_0']}, factor = 3.0, op_device = , op_namescope = /, op_role = 1, op_role_var = [])
    {Out=['fc_4.tmp_1.cast_fp32@GRAD']} = sum(inputs={X=['fc_4.tmp_1.cast_fp32@GRAD@RENAME@block0@0', 'fc_4.tmp_1.cast_fp32@GRAD@RENAME@block0@1', 'fc_4.tmp_1.cast_fp32@GRAD@RENAME@block0@2']}, op_device = , op_role = 1, use_mkldnn = False)
    {Out=['fc_4.tmp_1@GRAD']} = cast(inputs={X=['fc_4.tmp_1.cast_fp32@GRAD']}, in_dtype = 5, op_device = , op_role = 1, out_dtype = 4, use_mkldnn = False)
    {X@GRAD=['fc_4.tmp_0@GRAD'], Y@GRAD=['encoder_layer_0_ffn_fc_0.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_4.tmp_1@GRAD'], X=['fc_4.tmp_0.subprog_0'], Y=['encoder_layer_0_ffn_fc_0.b_0.cast_fp16.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_ffn_fc_0.b_0', 'encoder_layer_0_ffn_fc_0.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_1.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_0_ffn_fc_0.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_4.tmp_0@GRAD'], X=['layer_norm_1.tmp_2.subprog_0'], Y=['encoder_layer_0_ffn_fc_0.w_0.cast_fp16.subprog_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_ffn_fc_0.w_0', 'encoder_layer_0_ffn_fc_0.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_1.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_1.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_1.tmp_2@GRAD@RENAME@block0@1']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['encoder_layer_0_post_att_layer_norm_bias@GRAD'], Scale@GRAD=['encoder_layer_0_post_att_layer_norm_scale@GRAD'], X@GRAD=['tmp_3@GRAD']} = layer_norm_grad(inputs={Bias=['encoder_layer_0_post_att_layer_norm_bias'], Mean=['layer_norm_1.tmp_0.subprog_0'], Scale=['encoder_layer_0_post_att_layer_norm_scale'], Variance=['layer_norm_1.tmp_1.subprog_0'], X=['tmp_3.subprog_0'], Y@GRAD=['layer_norm_1.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_post_att_layer_norm_bias', 'encoder_layer_0_post_att_layer_norm_bias@GRAD', 'encoder_layer_0_post_att_layer_norm_scale', 'encoder_layer_0_post_att_layer_norm_scale@GRAD'], use_mkldnn = False)
    {X@GRAD=['fc_3.tmp_1@GRAD'], Y@GRAD=['layer_norm_0.tmp_2@GRAD@RENAME@block0@0']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_3@GRAD'], X=['fc_3.tmp_1.subprog_0'], Y=['layer_norm_0.tmp_2.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['fc_3.tmp_0@GRAD'], Y@GRAD=['encoder_layer_0_multi_head_att_output_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_3.tmp_1@GRAD'], X=['fc_3.tmp_0.subprog_0'], Y=['encoder_layer_0_multi_head_att_output_fc.b_0.cast_fp16.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.b_0', 'encoder_layer_0_multi_head_att_output_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['transpose_3.tmp_0@GRAD'], Y@GRAD=['encoder_layer_0_multi_head_att_output_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_3.tmp_0@GRAD'], X=['transpose_3.tmp_0.subprog_0'], Y=['encoder_layer_0_multi_head_att_output_fc.w_0.cast_fp16.subprog_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.w_0', 'encoder_layer_0_multi_head_att_output_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['transpose_3.tmp_0@GRAD']} = reshape2_grad(inputs={Out@GRAD=['transpose_3.tmp_0@GRAD'], XShape=['reshape2_3.tmp_0.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 768], use_quantizer = False)
    {X@GRAD=['matmul_v2_2.tmp_0@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_3.tmp_0@GRAD'], XShape=['transpose_3.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['softmax_0.tmp_0@GRAD'], Y@GRAD=['transpose_2.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_2.tmp_0@GRAD'], X=['softmax_0.tmp_0.subprog_0'], Y=['transpose_2.tmp_0.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = False, use_mkldnn = False)
    {X@GRAD=['tmp_2@GRAD']} = softmax_grad(inputs={Out=['softmax_0.tmp_0.subprog_0'], Out@GRAD=['softmax_0.tmp_0@GRAD']}, axis = -1, data_format = AnyLayout, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_cudnn = True, use_mkldnn = False)
    {X@GRAD=['matmul_v2_1.tmp_0@GRAD'], Y@GRAD=[]} = elementwise_add_grad(inputs={Out@GRAD=['tmp_2@GRAD'], X=['matmul_v2_1.tmp_0.subprog_0'], Y=['stack_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['scale_1.tmp_0@GRAD'], Y@GRAD=['transpose_1.tmp_0@GRAD']} = matmul_v2_grad(inputs={Out@GRAD=['matmul_v2_1.tmp_0@GRAD'], X=['scale_1.tmp_0.subprog_0'], Y=['transpose_1.tmp_0.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], trans_x = False, trans_y = True, use_mkldnn = False)
    {Out=['transpose_0.tmp_0@GRAD']} = scale(inputs={ScaleTensor=[], X=['scale_1.tmp_0@GRAD']}, bias = 0.0, bias_after_scale = True, op_device = , op_role = 1, scale = 0.125)
    {X@GRAD=['fc_2.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_2.tmp_0@GRAD'], XShape=['transpose_2.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_2.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_2.tmp_1@GRAD'], XShape=['reshape2_2.tmp_0.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_1.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_1.tmp_0@GRAD'], XShape=['transpose_1.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_1.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_1.tmp_1@GRAD'], XShape=['reshape2_1.tmp_0.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_0.tmp_1@GRAD']} = transpose2_grad(inputs={Out@GRAD=['transpose_0.tmp_0@GRAD'], XShape=['transpose_0.tmp_1.subprog_0']}, axis = [0, 2, 1, 3], data_format = AnyLayout, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False)
    {X@GRAD=['fc_0.tmp_1@GRAD']} = reshape2_grad(inputs={Out@GRAD=['fc_0.tmp_1@GRAD'], XShape=['reshape2_0.tmp_0.subprog_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], shape = [0, 0, 12, 64], use_quantizer = False)
    {X@GRAD=['fc_2.tmp_0@GRAD'], Y@GRAD=['encoder_layer_0_multi_head_att_value_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_2.tmp_1@GRAD'], X=['fc_2.tmp_0.subprog_0'], Y=['encoder_layer_0_multi_head_att_value_fc.b_0.cast_fp16.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.b_0', 'encoder_layer_0_multi_head_att_value_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_0.tmp_2@GRAD@RENAME@block0@1'], Y@GRAD=['encoder_layer_0_multi_head_att_value_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_2.tmp_0@GRAD'], X=['layer_norm_0.tmp_2.subprog_0'], Y=['encoder_layer_0_multi_head_att_value_fc.w_0.cast_fp16.subprog_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.w_0', 'encoder_layer_0_multi_head_att_value_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_1.tmp_0@GRAD'], Y@GRAD=['encoder_layer_0_multi_head_att_key_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_1.tmp_1@GRAD'], X=['fc_1.tmp_0.subprog_0'], Y=['encoder_layer_0_multi_head_att_key_fc.b_0.cast_fp16.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.b_0', 'encoder_layer_0_multi_head_att_key_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_0.tmp_2@GRAD@RENAME@block0@2'], Y@GRAD=['encoder_layer_0_multi_head_att_key_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_1.tmp_0@GRAD'], X=['layer_norm_0.tmp_2.subprog_0'], Y=['encoder_layer_0_multi_head_att_key_fc.w_0.cast_fp16.subprog_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.w_0', 'encoder_layer_0_multi_head_att_key_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {X@GRAD=['fc_0.tmp_0@GRAD'], Y@GRAD=['encoder_layer_0_multi_head_att_query_fc.b_0.cast_fp16@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['fc_0.tmp_1@GRAD'], X=['fc_0.tmp_0.subprog_0'], Y=['encoder_layer_0_multi_head_att_query_fc.b_0.cast_fp16.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = 2, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.b_0', 'encoder_layer_0_multi_head_att_query_fc.b_0.cast_fp16@GRAD'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['layer_norm_0.tmp_2@GRAD@RENAME@block0@3'], Y@GRAD=['encoder_layer_0_multi_head_att_query_fc.w_0.cast_fp16@GRAD']} = mul_grad(inputs={Out@GRAD=['fc_0.tmp_0@GRAD'], X=['layer_norm_0.tmp_2.subprog_0'], Y=['encoder_layer_0_multi_head_att_query_fc.w_0.cast_fp16.subprog_0']}, force_fp32_output = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.w_0', 'encoder_layer_0_multi_head_att_query_fc.w_0.cast_fp16@GRAD'], scale_out = 1.0, scale_x = 1.0, scale_y = [1.0], use_mkldnn = False, x_num_col_dims = 2, y_num_col_dims = 1)
    {Out=['layer_norm_0.tmp_2@GRAD']} = sum(inputs={X=['layer_norm_0.tmp_2@GRAD@RENAME@block0@0', 'layer_norm_0.tmp_2@GRAD@RENAME@block0@1', 'layer_norm_0.tmp_2@GRAD@RENAME@block0@2', 'layer_norm_0.tmp_2@GRAD@RENAME@block0@3']}, op_device = , op_role = 1, use_mkldnn = False)
    {Bias@GRAD=['pre_encoder_layer_norm_bias@GRAD'], Scale@GRAD=['pre_encoder_layer_norm_scale@GRAD'], X@GRAD=['tmp_1.cast_fp16@GRAD']} = layer_norm_grad(inputs={Bias=['pre_encoder_layer_norm_bias'], Mean=['layer_norm_0.tmp_0.subprog_0'], Scale=['pre_encoder_layer_norm_scale'], Variance=['layer_norm_0.tmp_1.subprog_0'], X=['tmp_1.cast_fp16.subprog_0'], Y@GRAD=['layer_norm_0.tmp_2@GRAD']}, begin_norm_axis = 2, epsilon = 9.999999960041972e-13, is_test = False, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = ['pre_encoder_layer_norm_bias', 'pre_encoder_layer_norm_bias@GRAD', 'pre_encoder_layer_norm_scale', 'pre_encoder_layer_norm_scale@GRAD'], use_mkldnn = False)
    {Out=['tmp_1@GRAD']} = cast(inputs={X=['tmp_1.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_role = 1, out_dtype = 5, use_mkldnn = False)
    {X@GRAD=['tmp_0@GRAD'], Y@GRAD=['embedding_5.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_1@GRAD'], X=['tmp_0.subprog_0'], Y=['embedding_5.tmp_0.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {X@GRAD=['embedding_1.tmp_0@GRAD'], Y@GRAD=['embedding_3.tmp_0@GRAD']} = elementwise_add_grad(inputs={Out@GRAD=['tmp_0@GRAD'], X=['embedding_1.tmp_0.subprog_0'], Y=['embedding_3.tmp_0.subprog_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {W@GRAD=['sent_embedding@GRAD']} = lookup_table_v2_grad(inputs={Ids=['sent_ids'], Out@GRAD=['embedding_5.tmp_0@GRAD'], W=['sent_embedding']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['sent_embedding', 'sent_embedding@GRAD'], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {W@GRAD=['pos_embedding@GRAD']} = lookup_table_v2_grad(inputs={Ids=['pos_ids'], Out@GRAD=['embedding_3.tmp_0@GRAD'], W=['pos_embedding']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['pos_embedding', 'pos_embedding@GRAD'], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {W@GRAD=['word_embedding@GRAD']} = lookup_table_v2_grad(inputs={Ids=['src_ids'], Out@GRAD=['embedding_1.tmp_0@GRAD'], W=['word_embedding']}, epmap = [], height_sections = [], is_distributed = False, is_sparse = False, op_device = , op_namescope = /, op_role = 1, op_role_var = ['word_embedding', 'word_embedding@GRAD'], padding_idx = -1, remote_prefetch = False, table_names = [], trainer_id = 0)
    {Out=['encoder_layer_5_post_ffn_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_5_post_ffn_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_5_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_5_post_ffn_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_5_post_ffn_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_5_post_ffn_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_5_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_5_post_ffn_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_5_ffn_fc_1.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_5_ffn_fc_1.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_5_ffn_fc_1.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_5_ffn_fc_1.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_5_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_5_ffn_fc_1.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_5_ffn_fc_1.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_5_ffn_fc_1.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_5_ffn_fc_1.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_5_ffn_fc_1.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_5_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_5_ffn_fc_1.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_5_ffn_fc_0.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072], str_value = , value = 0.0)
    {Out=['encoder_layer_5_ffn_fc_0.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_5_ffn_fc_0.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_5_ffn_fc_0.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_5_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_5_ffn_fc_0.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_5_ffn_fc_0.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 3072], str_value = , value = 0.0)
    {Out=['encoder_layer_5_ffn_fc_0.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_5_ffn_fc_0.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_5_ffn_fc_0.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_5_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_5_ffn_fc_0.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_5_post_att_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_5_post_att_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_5_post_att_layer_norm_scale@GRAD', 'encoder_layer_5_post_att_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_5_post_att_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_5_post_att_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_5_post_att_layer_norm_bias@GRAD', 'encoder_layer_5_post_att_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_5_multi_head_att_output_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_5_multi_head_att_output_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_5_multi_head_att_output_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_5_multi_head_att_output_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_5_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_output_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_5_multi_head_att_output_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_5_multi_head_att_output_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_5_multi_head_att_output_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_5_multi_head_att_output_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_5_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_output_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_5_multi_head_att_value_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_5_multi_head_att_value_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_5_multi_head_att_value_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_5_multi_head_att_value_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_5_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_value_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_5_multi_head_att_value_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_5_multi_head_att_value_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_5_multi_head_att_value_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_5_multi_head_att_value_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_5_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_value_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_5_multi_head_att_key_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_5_multi_head_att_key_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_5_multi_head_att_key_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_5_multi_head_att_key_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_5_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_key_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_5_multi_head_att_key_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_5_multi_head_att_key_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_5_multi_head_att_key_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_5_multi_head_att_key_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_5_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_key_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_5_multi_head_att_query_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_5_multi_head_att_query_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_5_multi_head_att_query_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_5_multi_head_att_query_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_5_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_query_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_5_multi_head_att_query_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_5_multi_head_att_query_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_5_multi_head_att_query_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_5_multi_head_att_query_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_5_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_query_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_4_post_ffn_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_4_post_ffn_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_4_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_4_post_ffn_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_4_post_ffn_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_4_post_ffn_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_4_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_4_post_ffn_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_4_ffn_fc_1.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_4_ffn_fc_1.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_4_ffn_fc_1.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_4_ffn_fc_1.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_4_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_4_ffn_fc_1.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_4_ffn_fc_1.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_4_ffn_fc_1.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_4_ffn_fc_1.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_4_ffn_fc_1.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_4_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_4_ffn_fc_1.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_4_ffn_fc_0.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072], str_value = , value = 0.0)
    {Out=['encoder_layer_4_ffn_fc_0.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_4_ffn_fc_0.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_4_ffn_fc_0.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_4_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_4_ffn_fc_0.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_4_ffn_fc_0.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 3072], str_value = , value = 0.0)
    {Out=['encoder_layer_4_ffn_fc_0.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_4_ffn_fc_0.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_4_ffn_fc_0.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_4_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_4_ffn_fc_0.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_4_post_att_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_4_post_att_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_4_post_att_layer_norm_scale@GRAD', 'encoder_layer_4_post_att_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_4_post_att_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_4_post_att_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_4_post_att_layer_norm_bias@GRAD', 'encoder_layer_4_post_att_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_4_multi_head_att_output_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_4_multi_head_att_output_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_4_multi_head_att_output_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_4_multi_head_att_output_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_4_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_output_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_4_multi_head_att_output_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_4_multi_head_att_output_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_4_multi_head_att_output_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_4_multi_head_att_output_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_4_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_output_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_4_multi_head_att_value_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_4_multi_head_att_value_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_4_multi_head_att_value_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_4_multi_head_att_value_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_4_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_value_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_4_multi_head_att_value_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_4_multi_head_att_value_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_4_multi_head_att_value_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_4_multi_head_att_value_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_4_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_value_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_4_multi_head_att_key_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_4_multi_head_att_key_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_4_multi_head_att_key_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_4_multi_head_att_key_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_4_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_key_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_4_multi_head_att_key_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_4_multi_head_att_key_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_4_multi_head_att_key_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_4_multi_head_att_key_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_4_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_key_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_4_multi_head_att_query_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_4_multi_head_att_query_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_4_multi_head_att_query_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_4_multi_head_att_query_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_4_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_query_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_4_multi_head_att_query_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_4_multi_head_att_query_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_4_multi_head_att_query_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_4_multi_head_att_query_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_4_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_query_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_3_post_ffn_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_3_post_ffn_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_3_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_3_post_ffn_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_3_post_ffn_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_3_post_ffn_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_3_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_3_post_ffn_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_3_ffn_fc_1.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_3_ffn_fc_1.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_3_ffn_fc_1.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_3_ffn_fc_1.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_3_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_3_ffn_fc_1.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_3_ffn_fc_1.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_3_ffn_fc_1.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_3_ffn_fc_1.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_3_ffn_fc_1.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_3_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_3_ffn_fc_1.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_3_ffn_fc_0.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072], str_value = , value = 0.0)
    {Out=['encoder_layer_3_ffn_fc_0.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_3_ffn_fc_0.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_3_ffn_fc_0.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_3_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_3_ffn_fc_0.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_3_ffn_fc_0.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 3072], str_value = , value = 0.0)
    {Out=['encoder_layer_3_ffn_fc_0.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_3_ffn_fc_0.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_3_ffn_fc_0.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_3_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_3_ffn_fc_0.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_3_post_att_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_3_post_att_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_3_post_att_layer_norm_scale@GRAD', 'encoder_layer_3_post_att_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_3_post_att_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_3_post_att_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_3_post_att_layer_norm_bias@GRAD', 'encoder_layer_3_post_att_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_3_multi_head_att_output_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_3_multi_head_att_output_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_3_multi_head_att_output_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_3_multi_head_att_output_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_3_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_output_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_3_multi_head_att_output_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_3_multi_head_att_output_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_3_multi_head_att_output_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_3_multi_head_att_output_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_3_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_output_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_3_multi_head_att_value_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_3_multi_head_att_value_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_3_multi_head_att_value_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_3_multi_head_att_value_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_3_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_value_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_3_multi_head_att_value_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_3_multi_head_att_value_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_3_multi_head_att_value_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_3_multi_head_att_value_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_3_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_value_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_3_multi_head_att_key_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_3_multi_head_att_key_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_3_multi_head_att_key_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_3_multi_head_att_key_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_3_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_key_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_3_multi_head_att_key_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_3_multi_head_att_key_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_3_multi_head_att_key_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_3_multi_head_att_key_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_3_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_key_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_3_multi_head_att_query_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_3_multi_head_att_query_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_3_multi_head_att_query_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_3_multi_head_att_query_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_3_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_query_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_3_multi_head_att_query_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_3_multi_head_att_query_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_3_multi_head_att_query_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_3_multi_head_att_query_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_3_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_query_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_2_post_ffn_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_2_post_ffn_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_2_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_2_post_ffn_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_2_post_ffn_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_2_post_ffn_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_2_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_2_post_ffn_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_2_ffn_fc_1.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_2_ffn_fc_1.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_2_ffn_fc_1.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_2_ffn_fc_1.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_2_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_2_ffn_fc_1.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_2_ffn_fc_1.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_2_ffn_fc_1.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_2_ffn_fc_1.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_2_ffn_fc_1.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_2_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_2_ffn_fc_1.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_2_ffn_fc_0.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072], str_value = , value = 0.0)
    {Out=['encoder_layer_2_ffn_fc_0.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_2_ffn_fc_0.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_2_ffn_fc_0.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_2_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_2_ffn_fc_0.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_2_ffn_fc_0.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 3072], str_value = , value = 0.0)
    {Out=['encoder_layer_2_ffn_fc_0.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_2_ffn_fc_0.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_2_ffn_fc_0.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_2_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_2_ffn_fc_0.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_2_post_att_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_2_post_att_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_2_post_att_layer_norm_scale@GRAD', 'encoder_layer_2_post_att_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_2_post_att_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_2_post_att_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_2_post_att_layer_norm_bias@GRAD', 'encoder_layer_2_post_att_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_2_multi_head_att_output_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_2_multi_head_att_output_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_2_multi_head_att_output_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_2_multi_head_att_output_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_2_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_output_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_2_multi_head_att_output_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_2_multi_head_att_output_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_2_multi_head_att_output_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_2_multi_head_att_output_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_2_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_output_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_2_multi_head_att_value_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_2_multi_head_att_value_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_2_multi_head_att_value_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_2_multi_head_att_value_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_2_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_value_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_2_multi_head_att_value_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_2_multi_head_att_value_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_2_multi_head_att_value_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_2_multi_head_att_value_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_2_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_value_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_2_multi_head_att_key_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_2_multi_head_att_key_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_2_multi_head_att_key_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_2_multi_head_att_key_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_2_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_key_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_2_multi_head_att_key_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_2_multi_head_att_key_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_2_multi_head_att_key_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_2_multi_head_att_key_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_2_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_key_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_2_multi_head_att_query_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_2_multi_head_att_query_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_2_multi_head_att_query_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_2_multi_head_att_query_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_2_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_query_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_2_multi_head_att_query_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_2_multi_head_att_query_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_2_multi_head_att_query_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_2_multi_head_att_query_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_2_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_query_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_1_post_ffn_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_1_post_ffn_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_1_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_1_post_ffn_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_1_post_ffn_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_1_post_ffn_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_1_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_1_post_ffn_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_1_ffn_fc_1.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_1_ffn_fc_1.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_1_ffn_fc_1.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_1_ffn_fc_1.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_1_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_1_ffn_fc_1.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_1_ffn_fc_1.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_1_ffn_fc_1.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_1_ffn_fc_1.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_1_ffn_fc_1.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_1_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_1_ffn_fc_1.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_1_ffn_fc_0.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072], str_value = , value = 0.0)
    {Out=['encoder_layer_1_ffn_fc_0.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_1_ffn_fc_0.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_1_ffn_fc_0.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_1_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_1_ffn_fc_0.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_1_ffn_fc_0.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 3072], str_value = , value = 0.0)
    {Out=['encoder_layer_1_ffn_fc_0.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_1_ffn_fc_0.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_1_ffn_fc_0.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_1_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_1_ffn_fc_0.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_1_post_att_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_1_post_att_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_1_post_att_layer_norm_scale@GRAD', 'encoder_layer_1_post_att_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_1_post_att_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_1_post_att_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_1_post_att_layer_norm_bias@GRAD', 'encoder_layer_1_post_att_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_1_multi_head_att_output_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_1_multi_head_att_output_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_1_multi_head_att_output_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_1_multi_head_att_output_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_1_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_output_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_1_multi_head_att_output_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_1_multi_head_att_output_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_1_multi_head_att_output_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_1_multi_head_att_output_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_1_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_output_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_1_multi_head_att_value_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_1_multi_head_att_value_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_1_multi_head_att_value_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_1_multi_head_att_value_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_1_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_value_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_1_multi_head_att_value_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_1_multi_head_att_value_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_1_multi_head_att_value_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_1_multi_head_att_value_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_1_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_value_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_1_multi_head_att_key_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_1_multi_head_att_key_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_1_multi_head_att_key_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_1_multi_head_att_key_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_1_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_key_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_1_multi_head_att_key_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_1_multi_head_att_key_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_1_multi_head_att_key_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_1_multi_head_att_key_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_1_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_key_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_1_multi_head_att_query_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_1_multi_head_att_query_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_1_multi_head_att_query_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_1_multi_head_att_query_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_1_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_query_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_1_multi_head_att_query_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_1_multi_head_att_query_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_1_multi_head_att_query_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_1_multi_head_att_query_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_1_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_query_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_0_post_ffn_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_0_post_ffn_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_0_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_0_post_ffn_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_0_post_ffn_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_0_post_ffn_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_0_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_0_post_ffn_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_0_ffn_fc_1.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_0_ffn_fc_1.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_0_ffn_fc_1.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_0_ffn_fc_1.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_0_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_0_ffn_fc_1.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_0_ffn_fc_1.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_0_ffn_fc_1.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_0_ffn_fc_1.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_0_ffn_fc_1.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_0_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_0_ffn_fc_1.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_0_ffn_fc_0.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [3072], str_value = , value = 0.0)
    {Out=['encoder_layer_0_ffn_fc_0.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_0_ffn_fc_0.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_0_ffn_fc_0.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_0_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_0_ffn_fc_0.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_0_ffn_fc_0.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 3072], str_value = , value = 0.0)
    {Out=['encoder_layer_0_ffn_fc_0.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_0_ffn_fc_0.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_0_ffn_fc_0.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_0_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_0_ffn_fc_0.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_0_post_att_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_0_post_att_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_0_post_att_layer_norm_scale@GRAD', 'encoder_layer_0_post_att_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_0_post_att_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_0_post_att_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_0_post_att_layer_norm_bias@GRAD', 'encoder_layer_0_post_att_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_0_multi_head_att_output_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_0_multi_head_att_output_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_0_multi_head_att_output_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_0_multi_head_att_output_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_0_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_output_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_0_multi_head_att_output_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_0_multi_head_att_output_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_0_multi_head_att_output_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_0_multi_head_att_output_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_0_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_output_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_0_multi_head_att_value_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_0_multi_head_att_value_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_0_multi_head_att_value_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_0_multi_head_att_value_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_0_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_value_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_0_multi_head_att_value_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_0_multi_head_att_value_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_0_multi_head_att_value_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_0_multi_head_att_value_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_0_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_value_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_0_multi_head_att_key_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_0_multi_head_att_key_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_0_multi_head_att_key_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_0_multi_head_att_key_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_0_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_key_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_0_multi_head_att_key_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_0_multi_head_att_key_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_0_multi_head_att_key_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_0_multi_head_att_key_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_0_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_key_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_0_multi_head_att_query_fc.b_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['encoder_layer_0_multi_head_att_query_fc.b_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_0_multi_head_att_query_fc.b_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_0_multi_head_att_query_fc.b_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_0_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_query_fc.b_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['encoder_layer_0_multi_head_att_query_fc.w_0@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768, 768], str_value = , value = 0.0)
    {Out=['encoder_layer_0_multi_head_att_query_fc.w_0@GRAD@TMP']} = cast(inputs={X=['encoder_layer_0_multi_head_att_query_fc.w_0.cast_fp16@GRAD']}, in_dtype = 4, op_device = , op_namescope = /, op_role = 1, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['encoder_layer_0_multi_head_att_query_fc.w_0@GRAD@MERGED']} = sum(inputs={X=['encoder_layer_0_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_query_fc.w_0@GRAD@TMP']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['pre_encoder_layer_norm_scale@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['pre_encoder_layer_norm_scale@GRAD@MERGED']} = sum(inputs={X=['pre_encoder_layer_norm_scale@GRAD', 'pre_encoder_layer_norm_scale@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['pre_encoder_layer_norm_bias@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [768], str_value = , value = 0.0)
    {Out=['pre_encoder_layer_norm_bias@GRAD@MERGED']} = sum(inputs={X=['pre_encoder_layer_norm_bias@GRAD', 'pre_encoder_layer_norm_bias@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['sent_embedding@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [4, 768], str_value = , value = 0.0)
    {Out=['sent_embedding@GRAD@MERGED']} = sum(inputs={X=['sent_embedding@GRAD', 'sent_embedding@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['pos_embedding@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [512, 768], str_value = , value = 0.0)
    {Out=['pos_embedding@GRAD@MERGED']} = sum(inputs={X=['pos_embedding@GRAD', 'pos_embedding@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {Out=['word_embedding@GRAD@MERGED']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [30000, 768], str_value = , value = 0.0)
    {Out=['word_embedding@GRAD@MERGED']} = sum(inputs={X=['word_embedding@GRAD', 'word_embedding@GRAD@MERGED']}, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 1, op_role_var = [], use_mkldnn = False)
    {FoundInfinite=['find_infinite_scale.tmp_0'], Out=['word_embedding@GRAD@MERGED', 'pos_embedding@GRAD@MERGED', 'sent_embedding@GRAD@MERGED', 'pre_encoder_layer_norm_scale@GRAD@MERGED', 'pre_encoder_layer_norm_bias@GRAD@MERGED', 'encoder_layer_0_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_0_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_0_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_0_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_0_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_0_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_0_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_0_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_0_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_1_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_1_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_1_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_1_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_1_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_1_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_1_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_1_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_1_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_2_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_2_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_2_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_2_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_2_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_2_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_2_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_2_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_2_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_3_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_3_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_3_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_3_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_3_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_3_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_3_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_3_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_3_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_4_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_4_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_4_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_4_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_4_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_4_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_4_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_4_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_4_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_5_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_5_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_5_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_5_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_5_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_5_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_5_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_5_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_5_post_ffn_layer_norm_bias@GRAD@MERGED']} = check_finite_and_unscale(inputs={FloatStatus=['float_status'], Scale=['loss_scaling_0'], X=['word_embedding@GRAD@MERGED', 'pos_embedding@GRAD@MERGED', 'sent_embedding@GRAD@MERGED', 'pre_encoder_layer_norm_scale@GRAD@MERGED', 'pre_encoder_layer_norm_bias@GRAD@MERGED', 'encoder_layer_0_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_0_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_0_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_0_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_0_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_0_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_0_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_0_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_0_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_1_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_1_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_1_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_1_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_1_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_1_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_1_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_1_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_1_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_2_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_2_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_2_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_2_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_2_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_2_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_2_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_2_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_2_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_3_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_3_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_3_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_3_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_3_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_3_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_3_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_3_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_3_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_4_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_4_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_4_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_4_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_4_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_4_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_4_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_4_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_4_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_5_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_5_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_5_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_5_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_5_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_5_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_5_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_5_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_5_post_ffn_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /, op_role = 2, op_role_var = ['word_embedding@GRAD@MERGED', 'pos_embedding@GRAD@MERGED', 'sent_embedding@GRAD@MERGED', 'pre_encoder_layer_norm_scale@GRAD@MERGED', 'pre_encoder_layer_norm_bias@GRAD@MERGED', 'encoder_layer_0_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_0_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_0_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_0_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_0_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_0_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_0_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_0_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_0_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_1_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_1_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_1_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_1_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_1_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_1_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_1_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_1_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_1_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_2_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_2_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_2_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_2_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_2_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_2_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_2_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_2_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_2_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_3_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_3_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_3_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_3_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_3_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_3_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_3_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_3_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_3_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_4_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_4_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_4_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_4_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_4_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_4_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_4_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_4_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_4_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_5_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_5_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_5_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_5_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_5_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_5_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_5_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_5_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_5_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_6_multi_head_att_query_fc.w_0@GRAD', 'encoder_layer_6_multi_head_att_query_fc.b_0@GRAD', 'encoder_layer_6_multi_head_att_key_fc.w_0@GRAD', 'encoder_layer_6_multi_head_att_key_fc.b_0@GRAD', 'encoder_layer_6_multi_head_att_value_fc.w_0@GRAD', 'encoder_layer_6_multi_head_att_value_fc.b_0@GRAD', 'encoder_layer_6_multi_head_att_output_fc.w_0@GRAD', 'encoder_layer_6_multi_head_att_output_fc.b_0@GRAD', 'encoder_layer_6_post_att_layer_norm_scale@GRAD', 'encoder_layer_6_post_att_layer_norm_bias@GRAD', 'encoder_layer_6_ffn_fc_0.w_0@GRAD', 'encoder_layer_6_ffn_fc_0.b_0@GRAD', 'encoder_layer_6_ffn_fc_1.w_0@GRAD', 'encoder_layer_6_ffn_fc_1.b_0@GRAD', 'encoder_layer_6_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_6_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_7_multi_head_att_query_fc.w_0@GRAD', 'encoder_layer_7_multi_head_att_query_fc.b_0@GRAD', 'encoder_layer_7_multi_head_att_key_fc.w_0@GRAD', 'encoder_layer_7_multi_head_att_key_fc.b_0@GRAD', 'encoder_layer_7_multi_head_att_value_fc.w_0@GRAD', 'encoder_layer_7_multi_head_att_value_fc.b_0@GRAD', 'encoder_layer_7_multi_head_att_output_fc.w_0@GRAD', 'encoder_layer_7_multi_head_att_output_fc.b_0@GRAD', 'encoder_layer_7_post_att_layer_norm_scale@GRAD', 'encoder_layer_7_post_att_layer_norm_bias@GRAD', 'encoder_layer_7_ffn_fc_0.w_0@GRAD', 'encoder_layer_7_ffn_fc_0.b_0@GRAD', 'encoder_layer_7_ffn_fc_1.w_0@GRAD', 'encoder_layer_7_ffn_fc_1.b_0@GRAD', 'encoder_layer_7_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_7_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_8_multi_head_att_query_fc.w_0@GRAD', 'encoder_layer_8_multi_head_att_query_fc.b_0@GRAD', 'encoder_layer_8_multi_head_att_key_fc.w_0@GRAD', 'encoder_layer_8_multi_head_att_key_fc.b_0@GRAD', 'encoder_layer_8_multi_head_att_value_fc.w_0@GRAD', 'encoder_layer_8_multi_head_att_value_fc.b_0@GRAD', 'encoder_layer_8_multi_head_att_output_fc.w_0@GRAD', 'encoder_layer_8_multi_head_att_output_fc.b_0@GRAD', 'encoder_layer_8_post_att_layer_norm_scale@GRAD', 'encoder_layer_8_post_att_layer_norm_bias@GRAD', 'encoder_layer_8_ffn_fc_0.w_0@GRAD', 'encoder_layer_8_ffn_fc_0.b_0@GRAD', 'encoder_layer_8_ffn_fc_1.w_0@GRAD', 'encoder_layer_8_ffn_fc_1.b_0@GRAD', 'encoder_layer_8_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_8_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_9_multi_head_att_query_fc.w_0@GRAD', 'encoder_layer_9_multi_head_att_query_fc.b_0@GRAD', 'encoder_layer_9_multi_head_att_key_fc.w_0@GRAD', 'encoder_layer_9_multi_head_att_key_fc.b_0@GRAD', 'encoder_layer_9_multi_head_att_value_fc.w_0@GRAD', 'encoder_layer_9_multi_head_att_value_fc.b_0@GRAD', 'encoder_layer_9_multi_head_att_output_fc.w_0@GRAD', 'encoder_layer_9_multi_head_att_output_fc.b_0@GRAD', 'encoder_layer_9_post_att_layer_norm_scale@GRAD', 'encoder_layer_9_post_att_layer_norm_bias@GRAD', 'encoder_layer_9_ffn_fc_0.w_0@GRAD', 'encoder_layer_9_ffn_fc_0.b_0@GRAD', 'encoder_layer_9_ffn_fc_1.w_0@GRAD', 'encoder_layer_9_ffn_fc_1.b_0@GRAD', 'encoder_layer_9_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_9_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_10_multi_head_att_query_fc.w_0@GRAD', 'encoder_layer_10_multi_head_att_query_fc.b_0@GRAD', 'encoder_layer_10_multi_head_att_key_fc.w_0@GRAD', 'encoder_layer_10_multi_head_att_key_fc.b_0@GRAD', 'encoder_layer_10_multi_head_att_value_fc.w_0@GRAD', 'encoder_layer_10_multi_head_att_value_fc.b_0@GRAD', 'encoder_layer_10_multi_head_att_output_fc.w_0@GRAD', 'encoder_layer_10_multi_head_att_output_fc.b_0@GRAD', 'encoder_layer_10_post_att_layer_norm_scale@GRAD', 'encoder_layer_10_post_att_layer_norm_bias@GRAD', 'encoder_layer_10_ffn_fc_0.w_0@GRAD', 'encoder_layer_10_ffn_fc_0.b_0@GRAD', 'encoder_layer_10_ffn_fc_1.w_0@GRAD', 'encoder_layer_10_ffn_fc_1.b_0@GRAD', 'encoder_layer_10_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_10_post_ffn_layer_norm_bias@GRAD', 'encoder_layer_11_multi_head_att_query_fc.w_0@GRAD', 'encoder_layer_11_multi_head_att_query_fc.b_0@GRAD', 'encoder_layer_11_multi_head_att_key_fc.w_0@GRAD', 'encoder_layer_11_multi_head_att_key_fc.b_0@GRAD', 'encoder_layer_11_multi_head_att_value_fc.w_0@GRAD', 'encoder_layer_11_multi_head_att_value_fc.b_0@GRAD', 'encoder_layer_11_multi_head_att_output_fc.w_0@GRAD', 'encoder_layer_11_multi_head_att_output_fc.b_0@GRAD', 'encoder_layer_11_post_att_layer_norm_scale@GRAD', 'encoder_layer_11_post_att_layer_norm_bias@GRAD', 'encoder_layer_11_ffn_fc_0.w_0@GRAD', 'encoder_layer_11_ffn_fc_0.b_0@GRAD', 'encoder_layer_11_ffn_fc_1.w_0@GRAD', 'encoder_layer_11_ffn_fc_1.b_0@GRAD', 'encoder_layer_11_post_ffn_layer_norm_scale@GRAD', 'encoder_layer_11_post_ffn_layer_norm_bias@GRAD', 'mask_lm_trans_fc.w_0@GRAD', 'mask_lm_trans_fc.b_0@GRAD', 'mask_lm_trans_layer_norm_scale@GRAD', 'mask_lm_trans_layer_norm_bias@GRAD', 'mask_lm_out_fc.w_0@GRAD', 'mask_lm_out_fc.b_0@GRAD'])
    {Out=['find_infinite_scale.tmp_0@cast_int32']} = cast(inputs={X=['find_infinite_scale.tmp_0']}, in_dtype = 0, op_device = , op_namescope = /, op_role = 2, op_role_var = [], out_dtype = 2, use_mkldnn = False)
    {Out=['find_infinite_scale.tmp_0@cast_int32']} = c_allreduce_max(inputs={X=['find_infinite_scale.tmp_0@cast_int32']}, op_device = , op_namescope = /, op_role = 2, op_role_var = [], ring_id = 3, tag = tag, use_calc_stream = True, use_model_parallel = False)
    {Out=['find_infinite_scale.tmp_0@GLOBAL_WORLD']} = cast(inputs={X=['find_infinite_scale.tmp_0@cast_int32']}, in_dtype = 2, op_device = , op_namescope = /, op_role = 2, op_role_var = [], out_dtype = 0, use_mkldnn = False)
    {LossScaling=['loss_scaling_0'], Out=['word_embedding@GRAD@MERGED', 'pos_embedding@GRAD@MERGED', 'sent_embedding@GRAD@MERGED', 'pre_encoder_layer_norm_scale@GRAD@MERGED', 'pre_encoder_layer_norm_bias@GRAD@MERGED', 'encoder_layer_0_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_0_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_0_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_0_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_0_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_0_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_0_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_0_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_0_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_1_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_1_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_1_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_1_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_1_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_1_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_1_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_1_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_1_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_2_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_2_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_2_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_2_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_2_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_2_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_2_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_2_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_2_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_3_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_3_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_3_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_3_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_3_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_3_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_3_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_3_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_3_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_4_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_4_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_4_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_4_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_4_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_4_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_4_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_4_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_4_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_5_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_5_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_5_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_5_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_5_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_5_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_5_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_5_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_5_post_ffn_layer_norm_bias@GRAD@MERGED'], OutBadSteps=['num_bad_steps_0'], OutGoodSteps=['num_good_steps_0']} = update_loss_scaling(inputs={FoundInfinite=['find_infinite_scale.tmp_0@GLOBAL_WORLD'], InBadSteps=['num_bad_steps_0'], InGoodSteps=['num_good_steps_0'], PrevLossScaling=['loss_scaling_0'], X=['word_embedding@GRAD@MERGED', 'pos_embedding@GRAD@MERGED', 'sent_embedding@GRAD@MERGED', 'pre_encoder_layer_norm_scale@GRAD@MERGED', 'pre_encoder_layer_norm_bias@GRAD@MERGED', 'encoder_layer_0_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_0_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_0_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_0_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_0_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_0_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_0_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_0_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_0_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_0_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_1_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_1_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_1_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_1_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_1_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_1_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_1_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_1_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_1_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_1_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_2_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_2_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_2_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_2_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_2_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_2_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_2_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_2_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_2_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_2_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_3_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_3_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_3_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_3_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_3_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_3_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_3_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_3_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_3_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_3_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_4_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_4_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_4_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_4_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_4_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_4_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_4_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_4_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_4_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_4_post_ffn_layer_norm_bias@GRAD@MERGED', 'encoder_layer_5_multi_head_att_query_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_query_fc.b_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_key_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_key_fc.b_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_value_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_value_fc.b_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_output_fc.w_0@GRAD@MERGED', 'encoder_layer_5_multi_head_att_output_fc.b_0@GRAD@MERGED', 'encoder_layer_5_post_att_layer_norm_scale@GRAD@MERGED', 'encoder_layer_5_post_att_layer_norm_bias@GRAD@MERGED', 'encoder_layer_5_ffn_fc_0.w_0@GRAD@MERGED', 'encoder_layer_5_ffn_fc_0.b_0@GRAD@MERGED', 'encoder_layer_5_ffn_fc_1.w_0@GRAD@MERGED', 'encoder_layer_5_ffn_fc_1.b_0@GRAD@MERGED', 'encoder_layer_5_post_ffn_layer_norm_scale@GRAD@MERGED', 'encoder_layer_5_post_ffn_layer_norm_bias@GRAD@MERGED']}, decr_every_n_nan_or_inf = 2, decr_ratio = 0.5, incr_every_n_steps = 1000, incr_ratio = 2.0, op_device = , op_namescope = /, op_role = 2, op_role_var = [], stop_update = False)
    {Out=['square_0.tmp_0']} = square(inputs={X=['encoder_layer_0_ffn_fc_0.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.b_0', 'elementwise_mul_0.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_0.tmp_0']} = reduce_sum(inputs={X=['square_0.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.b_0', 'elementwise_mul_0.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_1.tmp_0']} = square(inputs={X=['encoder_layer_0_ffn_fc_0.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.w_0', 'elementwise_mul_1.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_1.tmp_0']} = reduce_sum(inputs={X=['square_1.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.w_0', 'elementwise_mul_1.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_2.tmp_0']} = square(inputs={X=['encoder_layer_0_ffn_fc_1.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_1.b_0', 'elementwise_mul_2.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_2.tmp_0']} = reduce_sum(inputs={X=['square_2.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_1.b_0', 'elementwise_mul_2.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_3.tmp_0']} = square(inputs={X=['encoder_layer_0_ffn_fc_1.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_1.w_0', 'elementwise_mul_3.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_3.tmp_0']} = reduce_sum(inputs={X=['square_3.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_1.w_0', 'elementwise_mul_3.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_4.tmp_0']} = square(inputs={X=['encoder_layer_0_multi_head_att_key_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.b_0', 'elementwise_mul_4.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_4.tmp_0']} = reduce_sum(inputs={X=['square_4.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.b_0', 'elementwise_mul_4.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_5.tmp_0']} = square(inputs={X=['encoder_layer_0_multi_head_att_key_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.w_0', 'elementwise_mul_5.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_5.tmp_0']} = reduce_sum(inputs={X=['square_5.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.w_0', 'elementwise_mul_5.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_6.tmp_0']} = square(inputs={X=['encoder_layer_0_multi_head_att_output_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.b_0', 'elementwise_mul_6.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_6.tmp_0']} = reduce_sum(inputs={X=['square_6.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.b_0', 'elementwise_mul_6.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_7.tmp_0']} = square(inputs={X=['encoder_layer_0_multi_head_att_output_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.w_0', 'elementwise_mul_7.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_7.tmp_0']} = reduce_sum(inputs={X=['square_7.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.w_0', 'elementwise_mul_7.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_8.tmp_0']} = square(inputs={X=['encoder_layer_0_multi_head_att_query_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.b_0', 'elementwise_mul_8.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_8.tmp_0']} = reduce_sum(inputs={X=['square_8.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.b_0', 'elementwise_mul_8.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_9.tmp_0']} = square(inputs={X=['encoder_layer_0_multi_head_att_query_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.w_0', 'elementwise_mul_9.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_9.tmp_0']} = reduce_sum(inputs={X=['square_9.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.w_0', 'elementwise_mul_9.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_10.tmp_0']} = square(inputs={X=['encoder_layer_0_multi_head_att_value_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.b_0', 'elementwise_mul_10.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_10.tmp_0']} = reduce_sum(inputs={X=['square_10.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.b_0', 'elementwise_mul_10.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_11.tmp_0']} = square(inputs={X=['encoder_layer_0_multi_head_att_value_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.w_0', 'elementwise_mul_11.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_11.tmp_0']} = reduce_sum(inputs={X=['square_11.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.w_0', 'elementwise_mul_11.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_12.tmp_0']} = square(inputs={X=['encoder_layer_0_post_att_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_post_att_layer_norm_bias', 'elementwise_mul_12.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_12.tmp_0']} = reduce_sum(inputs={X=['square_12.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_post_att_layer_norm_bias', 'elementwise_mul_12.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_13.tmp_0']} = square(inputs={X=['encoder_layer_0_post_att_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_post_att_layer_norm_scale', 'elementwise_mul_13.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_13.tmp_0']} = reduce_sum(inputs={X=['square_13.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_post_att_layer_norm_scale', 'elementwise_mul_13.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_14.tmp_0']} = square(inputs={X=['encoder_layer_0_post_ffn_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_post_ffn_layer_norm_bias', 'elementwise_mul_14.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_14.tmp_0']} = reduce_sum(inputs={X=['square_14.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_post_ffn_layer_norm_bias', 'elementwise_mul_14.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_15.tmp_0']} = square(inputs={X=['encoder_layer_0_post_ffn_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_post_ffn_layer_norm_scale', 'elementwise_mul_15.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_15.tmp_0']} = reduce_sum(inputs={X=['square_15.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_post_ffn_layer_norm_scale', 'elementwise_mul_15.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_48.tmp_0']} = square(inputs={X=['encoder_layer_1_ffn_fc_0.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_0.b_0', 'elementwise_mul_48.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_48.tmp_0']} = reduce_sum(inputs={X=['square_48.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_0.b_0', 'elementwise_mul_48.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_49.tmp_0']} = square(inputs={X=['encoder_layer_1_ffn_fc_0.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_0.w_0', 'elementwise_mul_49.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_49.tmp_0']} = reduce_sum(inputs={X=['square_49.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_0.w_0', 'elementwise_mul_49.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_50.tmp_0']} = square(inputs={X=['encoder_layer_1_ffn_fc_1.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_1.b_0', 'elementwise_mul_50.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_50.tmp_0']} = reduce_sum(inputs={X=['square_50.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_1.b_0', 'elementwise_mul_50.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_51.tmp_0']} = square(inputs={X=['encoder_layer_1_ffn_fc_1.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_1.w_0', 'elementwise_mul_51.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_51.tmp_0']} = reduce_sum(inputs={X=['square_51.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_1.w_0', 'elementwise_mul_51.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_52.tmp_0']} = square(inputs={X=['encoder_layer_1_multi_head_att_key_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.b_0', 'elementwise_mul_52.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_52.tmp_0']} = reduce_sum(inputs={X=['square_52.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.b_0', 'elementwise_mul_52.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_53.tmp_0']} = square(inputs={X=['encoder_layer_1_multi_head_att_key_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.w_0', 'elementwise_mul_53.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_53.tmp_0']} = reduce_sum(inputs={X=['square_53.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.w_0', 'elementwise_mul_53.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_54.tmp_0']} = square(inputs={X=['encoder_layer_1_multi_head_att_output_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.b_0', 'elementwise_mul_54.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_54.tmp_0']} = reduce_sum(inputs={X=['square_54.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.b_0', 'elementwise_mul_54.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_55.tmp_0']} = square(inputs={X=['encoder_layer_1_multi_head_att_output_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.w_0', 'elementwise_mul_55.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_55.tmp_0']} = reduce_sum(inputs={X=['square_55.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.w_0', 'elementwise_mul_55.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_56.tmp_0']} = square(inputs={X=['encoder_layer_1_multi_head_att_query_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.b_0', 'elementwise_mul_56.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_56.tmp_0']} = reduce_sum(inputs={X=['square_56.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.b_0', 'elementwise_mul_56.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_57.tmp_0']} = square(inputs={X=['encoder_layer_1_multi_head_att_query_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.w_0', 'elementwise_mul_57.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_57.tmp_0']} = reduce_sum(inputs={X=['square_57.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.w_0', 'elementwise_mul_57.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_58.tmp_0']} = square(inputs={X=['encoder_layer_1_multi_head_att_value_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.b_0', 'elementwise_mul_58.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_58.tmp_0']} = reduce_sum(inputs={X=['square_58.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.b_0', 'elementwise_mul_58.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_59.tmp_0']} = square(inputs={X=['encoder_layer_1_multi_head_att_value_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.w_0', 'elementwise_mul_59.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_59.tmp_0']} = reduce_sum(inputs={X=['square_59.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.w_0', 'elementwise_mul_59.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_60.tmp_0']} = square(inputs={X=['encoder_layer_1_post_att_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_post_att_layer_norm_bias', 'elementwise_mul_60.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_60.tmp_0']} = reduce_sum(inputs={X=['square_60.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_post_att_layer_norm_bias', 'elementwise_mul_60.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_61.tmp_0']} = square(inputs={X=['encoder_layer_1_post_att_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_post_att_layer_norm_scale', 'elementwise_mul_61.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_61.tmp_0']} = reduce_sum(inputs={X=['square_61.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_post_att_layer_norm_scale', 'elementwise_mul_61.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_62.tmp_0']} = square(inputs={X=['encoder_layer_1_post_ffn_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_post_ffn_layer_norm_bias', 'elementwise_mul_62.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_62.tmp_0']} = reduce_sum(inputs={X=['square_62.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_post_ffn_layer_norm_bias', 'elementwise_mul_62.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_63.tmp_0']} = square(inputs={X=['encoder_layer_1_post_ffn_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_post_ffn_layer_norm_scale', 'elementwise_mul_63.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_63.tmp_0']} = reduce_sum(inputs={X=['square_63.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_post_ffn_layer_norm_scale', 'elementwise_mul_63.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_64.tmp_0']} = square(inputs={X=['encoder_layer_2_ffn_fc_0.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_0.b_0', 'elementwise_mul_64.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_64.tmp_0']} = reduce_sum(inputs={X=['square_64.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_0.b_0', 'elementwise_mul_64.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_65.tmp_0']} = square(inputs={X=['encoder_layer_2_ffn_fc_0.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_0.w_0', 'elementwise_mul_65.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_65.tmp_0']} = reduce_sum(inputs={X=['square_65.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_0.w_0', 'elementwise_mul_65.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_66.tmp_0']} = square(inputs={X=['encoder_layer_2_ffn_fc_1.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_1.b_0', 'elementwise_mul_66.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_66.tmp_0']} = reduce_sum(inputs={X=['square_66.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_1.b_0', 'elementwise_mul_66.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_67.tmp_0']} = square(inputs={X=['encoder_layer_2_ffn_fc_1.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_1.w_0', 'elementwise_mul_67.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_67.tmp_0']} = reduce_sum(inputs={X=['square_67.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_1.w_0', 'elementwise_mul_67.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_68.tmp_0']} = square(inputs={X=['encoder_layer_2_multi_head_att_key_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.b_0', 'elementwise_mul_68.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_68.tmp_0']} = reduce_sum(inputs={X=['square_68.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.b_0', 'elementwise_mul_68.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_69.tmp_0']} = square(inputs={X=['encoder_layer_2_multi_head_att_key_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.w_0', 'elementwise_mul_69.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_69.tmp_0']} = reduce_sum(inputs={X=['square_69.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.w_0', 'elementwise_mul_69.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_70.tmp_0']} = square(inputs={X=['encoder_layer_2_multi_head_att_output_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.b_0', 'elementwise_mul_70.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_70.tmp_0']} = reduce_sum(inputs={X=['square_70.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.b_0', 'elementwise_mul_70.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_71.tmp_0']} = square(inputs={X=['encoder_layer_2_multi_head_att_output_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.w_0', 'elementwise_mul_71.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_71.tmp_0']} = reduce_sum(inputs={X=['square_71.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.w_0', 'elementwise_mul_71.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_72.tmp_0']} = square(inputs={X=['encoder_layer_2_multi_head_att_query_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.b_0', 'elementwise_mul_72.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_72.tmp_0']} = reduce_sum(inputs={X=['square_72.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.b_0', 'elementwise_mul_72.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_73.tmp_0']} = square(inputs={X=['encoder_layer_2_multi_head_att_query_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.w_0', 'elementwise_mul_73.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_73.tmp_0']} = reduce_sum(inputs={X=['square_73.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.w_0', 'elementwise_mul_73.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_74.tmp_0']} = square(inputs={X=['encoder_layer_2_multi_head_att_value_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.b_0', 'elementwise_mul_74.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_74.tmp_0']} = reduce_sum(inputs={X=['square_74.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.b_0', 'elementwise_mul_74.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_75.tmp_0']} = square(inputs={X=['encoder_layer_2_multi_head_att_value_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.w_0', 'elementwise_mul_75.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_75.tmp_0']} = reduce_sum(inputs={X=['square_75.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.w_0', 'elementwise_mul_75.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_76.tmp_0']} = square(inputs={X=['encoder_layer_2_post_att_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_post_att_layer_norm_bias', 'elementwise_mul_76.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_76.tmp_0']} = reduce_sum(inputs={X=['square_76.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_post_att_layer_norm_bias', 'elementwise_mul_76.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_77.tmp_0']} = square(inputs={X=['encoder_layer_2_post_att_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_post_att_layer_norm_scale', 'elementwise_mul_77.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_77.tmp_0']} = reduce_sum(inputs={X=['square_77.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_post_att_layer_norm_scale', 'elementwise_mul_77.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_78.tmp_0']} = square(inputs={X=['encoder_layer_2_post_ffn_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_post_ffn_layer_norm_bias', 'elementwise_mul_78.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_78.tmp_0']} = reduce_sum(inputs={X=['square_78.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_post_ffn_layer_norm_bias', 'elementwise_mul_78.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_79.tmp_0']} = square(inputs={X=['encoder_layer_2_post_ffn_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_post_ffn_layer_norm_scale', 'elementwise_mul_79.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_79.tmp_0']} = reduce_sum(inputs={X=['square_79.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_post_ffn_layer_norm_scale', 'elementwise_mul_79.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_80.tmp_0']} = square(inputs={X=['encoder_layer_3_ffn_fc_0.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_ffn_fc_0.b_0', 'elementwise_mul_80.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_80.tmp_0']} = reduce_sum(inputs={X=['square_80.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_ffn_fc_0.b_0', 'elementwise_mul_80.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_81.tmp_0']} = square(inputs={X=['encoder_layer_3_ffn_fc_0.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_ffn_fc_0.w_0', 'elementwise_mul_81.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_81.tmp_0']} = reduce_sum(inputs={X=['square_81.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_ffn_fc_0.w_0', 'elementwise_mul_81.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_82.tmp_0']} = square(inputs={X=['encoder_layer_3_ffn_fc_1.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_ffn_fc_1.b_0', 'elementwise_mul_82.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_82.tmp_0']} = reduce_sum(inputs={X=['square_82.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_ffn_fc_1.b_0', 'elementwise_mul_82.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_83.tmp_0']} = square(inputs={X=['encoder_layer_3_ffn_fc_1.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_ffn_fc_1.w_0', 'elementwise_mul_83.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_83.tmp_0']} = reduce_sum(inputs={X=['square_83.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_ffn_fc_1.w_0', 'elementwise_mul_83.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_84.tmp_0']} = square(inputs={X=['encoder_layer_3_multi_head_att_key_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_key_fc.b_0', 'elementwise_mul_84.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_84.tmp_0']} = reduce_sum(inputs={X=['square_84.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_key_fc.b_0', 'elementwise_mul_84.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_85.tmp_0']} = square(inputs={X=['encoder_layer_3_multi_head_att_key_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_key_fc.w_0', 'elementwise_mul_85.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_85.tmp_0']} = reduce_sum(inputs={X=['square_85.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_key_fc.w_0', 'elementwise_mul_85.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_86.tmp_0']} = square(inputs={X=['encoder_layer_3_multi_head_att_output_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_output_fc.b_0', 'elementwise_mul_86.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_86.tmp_0']} = reduce_sum(inputs={X=['square_86.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_output_fc.b_0', 'elementwise_mul_86.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_87.tmp_0']} = square(inputs={X=['encoder_layer_3_multi_head_att_output_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_output_fc.w_0', 'elementwise_mul_87.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_87.tmp_0']} = reduce_sum(inputs={X=['square_87.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_output_fc.w_0', 'elementwise_mul_87.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_88.tmp_0']} = square(inputs={X=['encoder_layer_3_multi_head_att_query_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_query_fc.b_0', 'elementwise_mul_88.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_88.tmp_0']} = reduce_sum(inputs={X=['square_88.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_query_fc.b_0', 'elementwise_mul_88.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_89.tmp_0']} = square(inputs={X=['encoder_layer_3_multi_head_att_query_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_query_fc.w_0', 'elementwise_mul_89.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_89.tmp_0']} = reduce_sum(inputs={X=['square_89.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_query_fc.w_0', 'elementwise_mul_89.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_90.tmp_0']} = square(inputs={X=['encoder_layer_3_multi_head_att_value_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_value_fc.b_0', 'elementwise_mul_90.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_90.tmp_0']} = reduce_sum(inputs={X=['square_90.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_value_fc.b_0', 'elementwise_mul_90.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_91.tmp_0']} = square(inputs={X=['encoder_layer_3_multi_head_att_value_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_value_fc.w_0', 'elementwise_mul_91.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_91.tmp_0']} = reduce_sum(inputs={X=['square_91.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_value_fc.w_0', 'elementwise_mul_91.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_92.tmp_0']} = square(inputs={X=['encoder_layer_3_post_att_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_post_att_layer_norm_bias', 'elementwise_mul_92.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_92.tmp_0']} = reduce_sum(inputs={X=['square_92.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_post_att_layer_norm_bias', 'elementwise_mul_92.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_93.tmp_0']} = square(inputs={X=['encoder_layer_3_post_att_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_post_att_layer_norm_scale', 'elementwise_mul_93.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_93.tmp_0']} = reduce_sum(inputs={X=['square_93.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_post_att_layer_norm_scale', 'elementwise_mul_93.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_94.tmp_0']} = square(inputs={X=['encoder_layer_3_post_ffn_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_post_ffn_layer_norm_bias', 'elementwise_mul_94.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_94.tmp_0']} = reduce_sum(inputs={X=['square_94.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_post_ffn_layer_norm_bias', 'elementwise_mul_94.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_95.tmp_0']} = square(inputs={X=['encoder_layer_3_post_ffn_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_post_ffn_layer_norm_scale', 'elementwise_mul_95.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_95.tmp_0']} = reduce_sum(inputs={X=['square_95.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_post_ffn_layer_norm_scale', 'elementwise_mul_95.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_96.tmp_0']} = square(inputs={X=['encoder_layer_4_ffn_fc_0.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_ffn_fc_0.b_0', 'elementwise_mul_96.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_96.tmp_0']} = reduce_sum(inputs={X=['square_96.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_ffn_fc_0.b_0', 'elementwise_mul_96.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_97.tmp_0']} = square(inputs={X=['encoder_layer_4_ffn_fc_0.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_ffn_fc_0.w_0', 'elementwise_mul_97.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_97.tmp_0']} = reduce_sum(inputs={X=['square_97.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_ffn_fc_0.w_0', 'elementwise_mul_97.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_98.tmp_0']} = square(inputs={X=['encoder_layer_4_ffn_fc_1.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_ffn_fc_1.b_0', 'elementwise_mul_98.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_98.tmp_0']} = reduce_sum(inputs={X=['square_98.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_ffn_fc_1.b_0', 'elementwise_mul_98.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_99.tmp_0']} = square(inputs={X=['encoder_layer_4_ffn_fc_1.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_ffn_fc_1.w_0', 'elementwise_mul_99.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_99.tmp_0']} = reduce_sum(inputs={X=['square_99.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_ffn_fc_1.w_0', 'elementwise_mul_99.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_100.tmp_0']} = square(inputs={X=['encoder_layer_4_multi_head_att_key_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_key_fc.b_0', 'elementwise_mul_100.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_100.tmp_0']} = reduce_sum(inputs={X=['square_100.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_key_fc.b_0', 'elementwise_mul_100.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_101.tmp_0']} = square(inputs={X=['encoder_layer_4_multi_head_att_key_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_key_fc.w_0', 'elementwise_mul_101.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_101.tmp_0']} = reduce_sum(inputs={X=['square_101.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_key_fc.w_0', 'elementwise_mul_101.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_102.tmp_0']} = square(inputs={X=['encoder_layer_4_multi_head_att_output_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_output_fc.b_0', 'elementwise_mul_102.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_102.tmp_0']} = reduce_sum(inputs={X=['square_102.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_output_fc.b_0', 'elementwise_mul_102.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_103.tmp_0']} = square(inputs={X=['encoder_layer_4_multi_head_att_output_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_output_fc.w_0', 'elementwise_mul_103.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_103.tmp_0']} = reduce_sum(inputs={X=['square_103.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_output_fc.w_0', 'elementwise_mul_103.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_104.tmp_0']} = square(inputs={X=['encoder_layer_4_multi_head_att_query_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_query_fc.b_0', 'elementwise_mul_104.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_104.tmp_0']} = reduce_sum(inputs={X=['square_104.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_query_fc.b_0', 'elementwise_mul_104.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_105.tmp_0']} = square(inputs={X=['encoder_layer_4_multi_head_att_query_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_query_fc.w_0', 'elementwise_mul_105.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_105.tmp_0']} = reduce_sum(inputs={X=['square_105.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_query_fc.w_0', 'elementwise_mul_105.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_106.tmp_0']} = square(inputs={X=['encoder_layer_4_multi_head_att_value_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_value_fc.b_0', 'elementwise_mul_106.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_106.tmp_0']} = reduce_sum(inputs={X=['square_106.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_value_fc.b_0', 'elementwise_mul_106.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_107.tmp_0']} = square(inputs={X=['encoder_layer_4_multi_head_att_value_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_value_fc.w_0', 'elementwise_mul_107.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_107.tmp_0']} = reduce_sum(inputs={X=['square_107.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_value_fc.w_0', 'elementwise_mul_107.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_108.tmp_0']} = square(inputs={X=['encoder_layer_4_post_att_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_post_att_layer_norm_bias', 'elementwise_mul_108.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_108.tmp_0']} = reduce_sum(inputs={X=['square_108.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_post_att_layer_norm_bias', 'elementwise_mul_108.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_109.tmp_0']} = square(inputs={X=['encoder_layer_4_post_att_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_post_att_layer_norm_scale', 'elementwise_mul_109.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_109.tmp_0']} = reduce_sum(inputs={X=['square_109.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_post_att_layer_norm_scale', 'elementwise_mul_109.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_110.tmp_0']} = square(inputs={X=['encoder_layer_4_post_ffn_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_post_ffn_layer_norm_bias', 'elementwise_mul_110.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_110.tmp_0']} = reduce_sum(inputs={X=['square_110.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_post_ffn_layer_norm_bias', 'elementwise_mul_110.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_111.tmp_0']} = square(inputs={X=['encoder_layer_4_post_ffn_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_post_ffn_layer_norm_scale', 'elementwise_mul_111.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_111.tmp_0']} = reduce_sum(inputs={X=['square_111.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_post_ffn_layer_norm_scale', 'elementwise_mul_111.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_112.tmp_0']} = square(inputs={X=['encoder_layer_5_ffn_fc_0.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_ffn_fc_0.b_0', 'elementwise_mul_112.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_112.tmp_0']} = reduce_sum(inputs={X=['square_112.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_ffn_fc_0.b_0', 'elementwise_mul_112.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_113.tmp_0']} = square(inputs={X=['encoder_layer_5_ffn_fc_0.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_ffn_fc_0.w_0', 'elementwise_mul_113.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_113.tmp_0']} = reduce_sum(inputs={X=['square_113.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_ffn_fc_0.w_0', 'elementwise_mul_113.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_114.tmp_0']} = square(inputs={X=['encoder_layer_5_ffn_fc_1.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_ffn_fc_1.b_0', 'elementwise_mul_114.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_114.tmp_0']} = reduce_sum(inputs={X=['square_114.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_ffn_fc_1.b_0', 'elementwise_mul_114.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_115.tmp_0']} = square(inputs={X=['encoder_layer_5_ffn_fc_1.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_ffn_fc_1.w_0', 'elementwise_mul_115.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_115.tmp_0']} = reduce_sum(inputs={X=['square_115.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_ffn_fc_1.w_0', 'elementwise_mul_115.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_116.tmp_0']} = square(inputs={X=['encoder_layer_5_multi_head_att_key_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_key_fc.b_0', 'elementwise_mul_116.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_116.tmp_0']} = reduce_sum(inputs={X=['square_116.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_key_fc.b_0', 'elementwise_mul_116.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_117.tmp_0']} = square(inputs={X=['encoder_layer_5_multi_head_att_key_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_key_fc.w_0', 'elementwise_mul_117.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_117.tmp_0']} = reduce_sum(inputs={X=['square_117.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_key_fc.w_0', 'elementwise_mul_117.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_118.tmp_0']} = square(inputs={X=['encoder_layer_5_multi_head_att_output_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_output_fc.b_0', 'elementwise_mul_118.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_118.tmp_0']} = reduce_sum(inputs={X=['square_118.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_output_fc.b_0', 'elementwise_mul_118.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_119.tmp_0']} = square(inputs={X=['encoder_layer_5_multi_head_att_output_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_output_fc.w_0', 'elementwise_mul_119.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_119.tmp_0']} = reduce_sum(inputs={X=['square_119.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_output_fc.w_0', 'elementwise_mul_119.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_120.tmp_0']} = square(inputs={X=['encoder_layer_5_multi_head_att_query_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_query_fc.b_0', 'elementwise_mul_120.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_120.tmp_0']} = reduce_sum(inputs={X=['square_120.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_query_fc.b_0', 'elementwise_mul_120.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_121.tmp_0']} = square(inputs={X=['encoder_layer_5_multi_head_att_query_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_query_fc.w_0', 'elementwise_mul_121.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_121.tmp_0']} = reduce_sum(inputs={X=['square_121.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_query_fc.w_0', 'elementwise_mul_121.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_122.tmp_0']} = square(inputs={X=['encoder_layer_5_multi_head_att_value_fc.b_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_value_fc.b_0', 'elementwise_mul_122.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_122.tmp_0']} = reduce_sum(inputs={X=['square_122.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_value_fc.b_0', 'elementwise_mul_122.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_123.tmp_0']} = square(inputs={X=['encoder_layer_5_multi_head_att_value_fc.w_0@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_value_fc.w_0', 'elementwise_mul_123.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_123.tmp_0']} = reduce_sum(inputs={X=['square_123.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_value_fc.w_0', 'elementwise_mul_123.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_124.tmp_0']} = square(inputs={X=['encoder_layer_5_post_att_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_post_att_layer_norm_bias', 'elementwise_mul_124.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_124.tmp_0']} = reduce_sum(inputs={X=['square_124.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_post_att_layer_norm_bias', 'elementwise_mul_124.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_125.tmp_0']} = square(inputs={X=['encoder_layer_5_post_att_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_post_att_layer_norm_scale', 'elementwise_mul_125.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_125.tmp_0']} = reduce_sum(inputs={X=['square_125.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_post_att_layer_norm_scale', 'elementwise_mul_125.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_126.tmp_0']} = square(inputs={X=['encoder_layer_5_post_ffn_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_post_ffn_layer_norm_bias', 'elementwise_mul_126.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_126.tmp_0']} = reduce_sum(inputs={X=['square_126.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_post_ffn_layer_norm_bias', 'elementwise_mul_126.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_127.tmp_0']} = square(inputs={X=['encoder_layer_5_post_ffn_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_post_ffn_layer_norm_scale', 'elementwise_mul_127.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_127.tmp_0']} = reduce_sum(inputs={X=['square_127.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_post_ffn_layer_norm_scale', 'elementwise_mul_127.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_198.tmp_0']} = square(inputs={X=['pos_embedding@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['pos_embedding', 'elementwise_mul_198.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_198.tmp_0']} = reduce_sum(inputs={X=['square_198.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['pos_embedding', 'elementwise_mul_198.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_199.tmp_0']} = square(inputs={X=['pre_encoder_layer_norm_bias@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['pre_encoder_layer_norm_bias', 'elementwise_mul_199.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_199.tmp_0']} = reduce_sum(inputs={X=['square_199.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['pre_encoder_layer_norm_bias', 'elementwise_mul_199.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_200.tmp_0']} = square(inputs={X=['pre_encoder_layer_norm_scale@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['pre_encoder_layer_norm_scale', 'elementwise_mul_200.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_200.tmp_0']} = reduce_sum(inputs={X=['square_200.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['pre_encoder_layer_norm_scale', 'elementwise_mul_200.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_201.tmp_0']} = square(inputs={X=['sent_embedding@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['sent_embedding', 'elementwise_mul_201.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_201.tmp_0']} = reduce_sum(inputs={X=['square_201.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['sent_embedding', 'elementwise_mul_201.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['square_202.tmp_0']} = square(inputs={X=['word_embedding@GRAD@MERGED']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['word_embedding', 'elementwise_mul_202.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['reduce_sum_202.tmp_0']} = reduce_sum(inputs={X=['square_202.tmp_0']}, dim = [0], in_dtype = -1, keep_dim = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['word_embedding', 'elementwise_mul_202.tmp_0'], out_dtype = -1, reduce_all = True, use_mkldnn = False)
    {Out=['sum_0.tmp_0']} = sum(inputs={X=['reduce_sum_0.tmp_0', 'reduce_sum_1.tmp_0', 'reduce_sum_2.tmp_0', 'reduce_sum_3.tmp_0', 'reduce_sum_4.tmp_0', 'reduce_sum_5.tmp_0', 'reduce_sum_6.tmp_0', 'reduce_sum_7.tmp_0', 'reduce_sum_8.tmp_0', 'reduce_sum_9.tmp_0', 'reduce_sum_10.tmp_0', 'reduce_sum_11.tmp_0', 'reduce_sum_12.tmp_0', 'reduce_sum_13.tmp_0', 'reduce_sum_14.tmp_0', 'reduce_sum_15.tmp_0', 'reduce_sum_48.tmp_0', 'reduce_sum_49.tmp_0', 'reduce_sum_50.tmp_0', 'reduce_sum_51.tmp_0', 'reduce_sum_52.tmp_0', 'reduce_sum_53.tmp_0', 'reduce_sum_54.tmp_0', 'reduce_sum_55.tmp_0', 'reduce_sum_56.tmp_0', 'reduce_sum_57.tmp_0', 'reduce_sum_58.tmp_0', 'reduce_sum_59.tmp_0', 'reduce_sum_60.tmp_0', 'reduce_sum_61.tmp_0', 'reduce_sum_62.tmp_0', 'reduce_sum_63.tmp_0', 'reduce_sum_64.tmp_0', 'reduce_sum_65.tmp_0', 'reduce_sum_66.tmp_0', 'reduce_sum_67.tmp_0', 'reduce_sum_68.tmp_0', 'reduce_sum_69.tmp_0', 'reduce_sum_70.tmp_0', 'reduce_sum_71.tmp_0', 'reduce_sum_72.tmp_0', 'reduce_sum_73.tmp_0', 'reduce_sum_74.tmp_0', 'reduce_sum_75.tmp_0', 'reduce_sum_76.tmp_0', 'reduce_sum_77.tmp_0', 'reduce_sum_78.tmp_0', 'reduce_sum_79.tmp_0', 'reduce_sum_80.tmp_0', 'reduce_sum_81.tmp_0', 'reduce_sum_82.tmp_0', 'reduce_sum_83.tmp_0', 'reduce_sum_84.tmp_0', 'reduce_sum_85.tmp_0', 'reduce_sum_86.tmp_0', 'reduce_sum_87.tmp_0', 'reduce_sum_88.tmp_0', 'reduce_sum_89.tmp_0', 'reduce_sum_90.tmp_0', 'reduce_sum_91.tmp_0', 'reduce_sum_92.tmp_0', 'reduce_sum_93.tmp_0', 'reduce_sum_94.tmp_0', 'reduce_sum_95.tmp_0', 'reduce_sum_96.tmp_0', 'reduce_sum_97.tmp_0', 'reduce_sum_98.tmp_0', 'reduce_sum_99.tmp_0', 'reduce_sum_100.tmp_0', 'reduce_sum_101.tmp_0', 'reduce_sum_102.tmp_0', 'reduce_sum_103.tmp_0', 'reduce_sum_104.tmp_0', 'reduce_sum_105.tmp_0', 'reduce_sum_106.tmp_0', 'reduce_sum_107.tmp_0', 'reduce_sum_108.tmp_0', 'reduce_sum_109.tmp_0', 'reduce_sum_110.tmp_0', 'reduce_sum_111.tmp_0', 'reduce_sum_112.tmp_0', 'reduce_sum_113.tmp_0', 'reduce_sum_114.tmp_0', 'reduce_sum_115.tmp_0', 'reduce_sum_116.tmp_0', 'reduce_sum_117.tmp_0', 'reduce_sum_118.tmp_0', 'reduce_sum_119.tmp_0', 'reduce_sum_120.tmp_0', 'reduce_sum_121.tmp_0', 'reduce_sum_122.tmp_0', 'reduce_sum_123.tmp_0', 'reduce_sum_124.tmp_0', 'reduce_sum_125.tmp_0', 'reduce_sum_126.tmp_0', 'reduce_sum_127.tmp_0', 'reduce_sum_198.tmp_0', 'reduce_sum_199.tmp_0', 'reduce_sum_200.tmp_0', 'reduce_sum_201.tmp_0', 'reduce_sum_202.tmp_0']}, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['word_embedding', 'elementwise_mul_202.tmp_0'], use_mkldnn = False)
    {Out=['sum_0.tmp_0']} = c_allreduce_sum(inputs={X=['sum_0.tmp_0']}, op_device = , op_namescope = /, op_role = 2, op_role_var = [], ring_id = 3, tag = tag, use_calc_stream = True, use_model_parallel = False)
    {Out=['sqrt_0.tmp_0']} = sqrt(inputs={X=['sum_0.tmp_0']}, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['word_embedding', 'elementwise_mul_202.tmp_0'], use_cudnn = False, use_mkldnn = False)
    {Out=['fill_constant_7.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['word_embedding', 'elementwise_mul_202.tmp_0'], place_type = -1, shape = [1], str_value = 1.0, value = 1.0)
    {Out=['elementwise_max_0.tmp_0']} = elementwise_max(inputs={X=['fill_constant_7.tmp_0'], Y=['sqrt_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['word_embedding', 'elementwise_mul_202.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_div_0.tmp_0']} = elementwise_div(inputs={X=['fill_constant_7.tmp_0'], Y=['elementwise_max_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['word_embedding', 'elementwise_mul_202.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_0.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_ffn_fc_0.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.b_0', 'elementwise_mul_0.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_1.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_ffn_fc_0.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.w_0', 'elementwise_mul_1.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_2.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_ffn_fc_1.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_1.b_0', 'elementwise_mul_2.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_3.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_ffn_fc_1.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_1.w_0', 'elementwise_mul_3.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_4.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_multi_head_att_key_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.b_0', 'elementwise_mul_4.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_5.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_multi_head_att_key_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.w_0', 'elementwise_mul_5.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_6.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_multi_head_att_output_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.b_0', 'elementwise_mul_6.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_7.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_multi_head_att_output_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.w_0', 'elementwise_mul_7.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_8.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_multi_head_att_query_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.b_0', 'elementwise_mul_8.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_9.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_multi_head_att_query_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.w_0', 'elementwise_mul_9.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_10.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_multi_head_att_value_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.b_0', 'elementwise_mul_10.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_11.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_multi_head_att_value_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.w_0', 'elementwise_mul_11.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_12.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_post_att_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_post_att_layer_norm_bias', 'elementwise_mul_12.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_13.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_post_att_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_post_att_layer_norm_scale', 'elementwise_mul_13.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_14.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_post_ffn_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_post_ffn_layer_norm_bias', 'elementwise_mul_14.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_15.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_0_post_ffn_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_0_post_ffn_layer_norm_scale', 'elementwise_mul_15.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_48.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_ffn_fc_0.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_0.b_0', 'elementwise_mul_48.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_49.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_ffn_fc_0.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_0.w_0', 'elementwise_mul_49.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_50.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_ffn_fc_1.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_1.b_0', 'elementwise_mul_50.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_51.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_ffn_fc_1.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_1.w_0', 'elementwise_mul_51.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_52.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_multi_head_att_key_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.b_0', 'elementwise_mul_52.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_53.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_multi_head_att_key_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.w_0', 'elementwise_mul_53.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_54.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_multi_head_att_output_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.b_0', 'elementwise_mul_54.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_55.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_multi_head_att_output_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.w_0', 'elementwise_mul_55.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_56.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_multi_head_att_query_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.b_0', 'elementwise_mul_56.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_57.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_multi_head_att_query_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.w_0', 'elementwise_mul_57.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_58.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_multi_head_att_value_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.b_0', 'elementwise_mul_58.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_59.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_multi_head_att_value_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.w_0', 'elementwise_mul_59.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_60.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_post_att_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_post_att_layer_norm_bias', 'elementwise_mul_60.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_61.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_post_att_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_post_att_layer_norm_scale', 'elementwise_mul_61.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_62.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_post_ffn_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_post_ffn_layer_norm_bias', 'elementwise_mul_62.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_63.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_1_post_ffn_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_1_post_ffn_layer_norm_scale', 'elementwise_mul_63.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_64.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_ffn_fc_0.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_0.b_0', 'elementwise_mul_64.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_65.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_ffn_fc_0.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_0.w_0', 'elementwise_mul_65.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_66.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_ffn_fc_1.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_1.b_0', 'elementwise_mul_66.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_67.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_ffn_fc_1.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_1.w_0', 'elementwise_mul_67.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_68.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_multi_head_att_key_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.b_0', 'elementwise_mul_68.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_69.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_multi_head_att_key_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.w_0', 'elementwise_mul_69.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_70.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_multi_head_att_output_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.b_0', 'elementwise_mul_70.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_71.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_multi_head_att_output_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.w_0', 'elementwise_mul_71.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_72.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_multi_head_att_query_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.b_0', 'elementwise_mul_72.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_73.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_multi_head_att_query_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.w_0', 'elementwise_mul_73.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_74.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_multi_head_att_value_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.b_0', 'elementwise_mul_74.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_75.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_multi_head_att_value_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.w_0', 'elementwise_mul_75.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_76.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_post_att_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_post_att_layer_norm_bias', 'elementwise_mul_76.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_77.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_post_att_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_post_att_layer_norm_scale', 'elementwise_mul_77.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_78.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_post_ffn_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_post_ffn_layer_norm_bias', 'elementwise_mul_78.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_79.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_2_post_ffn_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_2_post_ffn_layer_norm_scale', 'elementwise_mul_79.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_80.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_3_ffn_fc_0.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_ffn_fc_0.b_0', 'elementwise_mul_80.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_81.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_3_ffn_fc_0.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_ffn_fc_0.w_0', 'elementwise_mul_81.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_82.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_3_ffn_fc_1.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_ffn_fc_1.b_0', 'elementwise_mul_82.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_83.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_3_ffn_fc_1.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_ffn_fc_1.w_0', 'elementwise_mul_83.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_84.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_3_multi_head_att_key_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_key_fc.b_0', 'elementwise_mul_84.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_85.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_3_multi_head_att_key_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_key_fc.w_0', 'elementwise_mul_85.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_86.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_3_multi_head_att_output_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_output_fc.b_0', 'elementwise_mul_86.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_87.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_3_multi_head_att_output_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_output_fc.w_0', 'elementwise_mul_87.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_88.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_3_multi_head_att_query_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_query_fc.b_0', 'elementwise_mul_88.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_89.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_3_multi_head_att_query_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_query_fc.w_0', 'elementwise_mul_89.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_90.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_3_multi_head_att_value_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_value_fc.b_0', 'elementwise_mul_90.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_91.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_3_multi_head_att_value_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_value_fc.w_0', 'elementwise_mul_91.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_92.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_3_post_att_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_post_att_layer_norm_bias', 'elementwise_mul_92.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_93.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_3_post_att_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_post_att_layer_norm_scale', 'elementwise_mul_93.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_94.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_3_post_ffn_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_post_ffn_layer_norm_bias', 'elementwise_mul_94.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_95.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_3_post_ffn_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_3_post_ffn_layer_norm_scale', 'elementwise_mul_95.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_96.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_4_ffn_fc_0.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_ffn_fc_0.b_0', 'elementwise_mul_96.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_97.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_4_ffn_fc_0.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_ffn_fc_0.w_0', 'elementwise_mul_97.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_98.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_4_ffn_fc_1.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_ffn_fc_1.b_0', 'elementwise_mul_98.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_99.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_4_ffn_fc_1.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_ffn_fc_1.w_0', 'elementwise_mul_99.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_100.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_4_multi_head_att_key_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_key_fc.b_0', 'elementwise_mul_100.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_101.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_4_multi_head_att_key_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_key_fc.w_0', 'elementwise_mul_101.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_102.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_4_multi_head_att_output_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_output_fc.b_0', 'elementwise_mul_102.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_103.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_4_multi_head_att_output_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_output_fc.w_0', 'elementwise_mul_103.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_104.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_4_multi_head_att_query_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_query_fc.b_0', 'elementwise_mul_104.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_105.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_4_multi_head_att_query_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_query_fc.w_0', 'elementwise_mul_105.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_106.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_4_multi_head_att_value_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_value_fc.b_0', 'elementwise_mul_106.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_107.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_4_multi_head_att_value_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_value_fc.w_0', 'elementwise_mul_107.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_108.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_4_post_att_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_post_att_layer_norm_bias', 'elementwise_mul_108.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_109.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_4_post_att_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_post_att_layer_norm_scale', 'elementwise_mul_109.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_110.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_4_post_ffn_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_post_ffn_layer_norm_bias', 'elementwise_mul_110.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_111.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_4_post_ffn_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_4_post_ffn_layer_norm_scale', 'elementwise_mul_111.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_112.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_5_ffn_fc_0.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_ffn_fc_0.b_0', 'elementwise_mul_112.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_113.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_5_ffn_fc_0.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_ffn_fc_0.w_0', 'elementwise_mul_113.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_114.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_5_ffn_fc_1.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_ffn_fc_1.b_0', 'elementwise_mul_114.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_115.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_5_ffn_fc_1.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_ffn_fc_1.w_0', 'elementwise_mul_115.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_116.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_5_multi_head_att_key_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_key_fc.b_0', 'elementwise_mul_116.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_117.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_5_multi_head_att_key_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_key_fc.w_0', 'elementwise_mul_117.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_118.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_5_multi_head_att_output_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_output_fc.b_0', 'elementwise_mul_118.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_119.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_5_multi_head_att_output_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_output_fc.w_0', 'elementwise_mul_119.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_120.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_5_multi_head_att_query_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_query_fc.b_0', 'elementwise_mul_120.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_121.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_5_multi_head_att_query_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_query_fc.w_0', 'elementwise_mul_121.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_122.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_5_multi_head_att_value_fc.b_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_value_fc.b_0', 'elementwise_mul_122.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_123.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_5_multi_head_att_value_fc.w_0@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_value_fc.w_0', 'elementwise_mul_123.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_124.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_5_post_att_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_post_att_layer_norm_bias', 'elementwise_mul_124.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_125.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_5_post_att_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_post_att_layer_norm_scale', 'elementwise_mul_125.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_126.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_5_post_ffn_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_post_ffn_layer_norm_bias', 'elementwise_mul_126.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_127.tmp_0']} = elementwise_mul(inputs={X=['encoder_layer_5_post_ffn_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['encoder_layer_5_post_ffn_layer_norm_scale', 'elementwise_mul_127.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_198.tmp_0']} = elementwise_mul(inputs={X=['pos_embedding@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['pos_embedding', 'elementwise_mul_198.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_199.tmp_0']} = elementwise_mul(inputs={X=['pre_encoder_layer_norm_bias@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['pre_encoder_layer_norm_bias', 'elementwise_mul_199.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_200.tmp_0']} = elementwise_mul(inputs={X=['pre_encoder_layer_norm_scale@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['pre_encoder_layer_norm_scale', 'elementwise_mul_200.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_201.tmp_0']} = elementwise_mul(inputs={X=['sent_embedding@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['sent_embedding', 'elementwise_mul_201.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['elementwise_mul_202.tmp_0']} = elementwise_mul(inputs={X=['word_embedding@GRAD@MERGED'], Y=['elementwise_div_0.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /gradient_clip/, op_role = 2, op_role_var = ['word_embedding', 'elementwise_mul_202.tmp_0'], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Beta1PowOut=['encoder_layer_0_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_0_ffn_fc_0.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_0_ffn_fc_0.b_0_moment1_0'], Moment2Out=['encoder_layer_0_ffn_fc_0.b_0_moment2_0'], ParamOut=['encoder_layer_0_ffn_fc_0.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_0_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_0_ffn_fc_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_0.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_0_ffn_fc_0.b_0_moment1_0'], Moment2=['encoder_layer_0_ffn_fc_0.b_0_moment2_0'], Param=['encoder_layer_0_ffn_fc_0.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.b_0', 'elementwise_mul_0.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_0_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_0_ffn_fc_0.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_0_ffn_fc_0.w_0_moment1_0'], Moment2Out=['encoder_layer_0_ffn_fc_0.w_0_moment2_0'], ParamOut=['encoder_layer_0_ffn_fc_0.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_0_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_0_ffn_fc_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_1.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_0_ffn_fc_0.w_0_moment1_0'], Moment2=['encoder_layer_0_ffn_fc_0.w_0_moment2_0'], Param=['encoder_layer_0_ffn_fc_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_1/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_0.w_0', 'elementwise_mul_1.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_0_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_0_ffn_fc_1.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_0_ffn_fc_1.b_0_moment1_0'], Moment2Out=['encoder_layer_0_ffn_fc_1.b_0_moment2_0'], ParamOut=['encoder_layer_0_ffn_fc_1.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_0_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_0_ffn_fc_1.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_2.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_0_ffn_fc_1.b_0_moment1_0'], Moment2=['encoder_layer_0_ffn_fc_1.b_0_moment2_0'], Param=['encoder_layer_0_ffn_fc_1.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_2/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_1.b_0', 'elementwise_mul_2.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_0_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_0_ffn_fc_1.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_0_ffn_fc_1.w_0_moment1_0'], Moment2Out=['encoder_layer_0_ffn_fc_1.w_0_moment2_0'], ParamOut=['encoder_layer_0_ffn_fc_1.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_0_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_0_ffn_fc_1.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_3.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_0_ffn_fc_1.w_0_moment1_0'], Moment2=['encoder_layer_0_ffn_fc_1.w_0_moment2_0'], Param=['encoder_layer_0_ffn_fc_1.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_3/, op_role = 2, op_role_var = ['encoder_layer_0_ffn_fc_1.w_0', 'elementwise_mul_3.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_0_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_0_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_0_multi_head_att_key_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_0_multi_head_att_key_fc.b_0_moment2_0'], ParamOut=['encoder_layer_0_multi_head_att_key_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_0_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_0_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_4.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_0_multi_head_att_key_fc.b_0_moment1_0'], Moment2=['encoder_layer_0_multi_head_att_key_fc.b_0_moment2_0'], Param=['encoder_layer_0_multi_head_att_key_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_4/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.b_0', 'elementwise_mul_4.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_0_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_0_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_0_multi_head_att_key_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_0_multi_head_att_key_fc.w_0_moment2_0'], ParamOut=['encoder_layer_0_multi_head_att_key_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_0_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_0_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_5.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_0_multi_head_att_key_fc.w_0_moment1_0'], Moment2=['encoder_layer_0_multi_head_att_key_fc.w_0_moment2_0'], Param=['encoder_layer_0_multi_head_att_key_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_5/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_key_fc.w_0', 'elementwise_mul_5.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_0_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_0_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_0_multi_head_att_output_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_0_multi_head_att_output_fc.b_0_moment2_0'], ParamOut=['encoder_layer_0_multi_head_att_output_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_0_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_0_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_6.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_0_multi_head_att_output_fc.b_0_moment1_0'], Moment2=['encoder_layer_0_multi_head_att_output_fc.b_0_moment2_0'], Param=['encoder_layer_0_multi_head_att_output_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_6/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.b_0', 'elementwise_mul_6.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_0_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_0_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_0_multi_head_att_output_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_0_multi_head_att_output_fc.w_0_moment2_0'], ParamOut=['encoder_layer_0_multi_head_att_output_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_0_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_0_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_7.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_0_multi_head_att_output_fc.w_0_moment1_0'], Moment2=['encoder_layer_0_multi_head_att_output_fc.w_0_moment2_0'], Param=['encoder_layer_0_multi_head_att_output_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_7/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_output_fc.w_0', 'elementwise_mul_7.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_0_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_0_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_0_multi_head_att_query_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_0_multi_head_att_query_fc.b_0_moment2_0'], ParamOut=['encoder_layer_0_multi_head_att_query_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_0_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_0_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_8.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_0_multi_head_att_query_fc.b_0_moment1_0'], Moment2=['encoder_layer_0_multi_head_att_query_fc.b_0_moment2_0'], Param=['encoder_layer_0_multi_head_att_query_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_8/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.b_0', 'elementwise_mul_8.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_0_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_0_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_0_multi_head_att_query_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_0_multi_head_att_query_fc.w_0_moment2_0'], ParamOut=['encoder_layer_0_multi_head_att_query_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_0_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_0_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_9.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_0_multi_head_att_query_fc.w_0_moment1_0'], Moment2=['encoder_layer_0_multi_head_att_query_fc.w_0_moment2_0'], Param=['encoder_layer_0_multi_head_att_query_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_9/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_query_fc.w_0', 'elementwise_mul_9.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_0_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_0_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_0_multi_head_att_value_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_0_multi_head_att_value_fc.b_0_moment2_0'], ParamOut=['encoder_layer_0_multi_head_att_value_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_0_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_0_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_10.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_0_multi_head_att_value_fc.b_0_moment1_0'], Moment2=['encoder_layer_0_multi_head_att_value_fc.b_0_moment2_0'], Param=['encoder_layer_0_multi_head_att_value_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_10/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.b_0', 'elementwise_mul_10.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_0_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_0_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_0_multi_head_att_value_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_0_multi_head_att_value_fc.w_0_moment2_0'], ParamOut=['encoder_layer_0_multi_head_att_value_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_0_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_0_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_11.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_0_multi_head_att_value_fc.w_0_moment1_0'], Moment2=['encoder_layer_0_multi_head_att_value_fc.w_0_moment2_0'], Param=['encoder_layer_0_multi_head_att_value_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_11/, op_role = 2, op_role_var = ['encoder_layer_0_multi_head_att_value_fc.w_0', 'elementwise_mul_11.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_0_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_0_post_att_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_0_post_att_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_0_post_att_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_0_post_att_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_0_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_0_post_att_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_12.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_0_post_att_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_0_post_att_layer_norm_bias_moment2_0'], Param=['encoder_layer_0_post_att_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_12/, op_role = 2, op_role_var = ['encoder_layer_0_post_att_layer_norm_bias', 'elementwise_mul_12.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_0_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_0_post_att_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_0_post_att_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_0_post_att_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_0_post_att_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_0_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_0_post_att_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_13.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_0_post_att_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_0_post_att_layer_norm_scale_moment2_0'], Param=['encoder_layer_0_post_att_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_13/, op_role = 2, op_role_var = ['encoder_layer_0_post_att_layer_norm_scale', 'elementwise_mul_13.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_0_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_0_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_0_post_ffn_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_0_post_ffn_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_0_post_ffn_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_0_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_0_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_14.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_0_post_ffn_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_0_post_ffn_layer_norm_bias_moment2_0'], Param=['encoder_layer_0_post_ffn_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_14/, op_role = 2, op_role_var = ['encoder_layer_0_post_ffn_layer_norm_bias', 'elementwise_mul_14.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_0_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_0_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_0_post_ffn_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_0_post_ffn_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_0_post_ffn_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_0_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_0_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_15.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_0_post_ffn_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_0_post_ffn_layer_norm_scale_moment2_0'], Param=['encoder_layer_0_post_ffn_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_15/, op_role = 2, op_role_var = ['encoder_layer_0_post_ffn_layer_norm_scale', 'elementwise_mul_15.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_1_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_1_ffn_fc_0.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_1_ffn_fc_0.b_0_moment1_0'], Moment2Out=['encoder_layer_1_ffn_fc_0.b_0_moment2_0'], ParamOut=['encoder_layer_1_ffn_fc_0.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_1_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_1_ffn_fc_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_48.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_1_ffn_fc_0.b_0_moment1_0'], Moment2=['encoder_layer_1_ffn_fc_0.b_0_moment2_0'], Param=['encoder_layer_1_ffn_fc_0.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_48/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_0.b_0', 'elementwise_mul_48.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_1_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_1_ffn_fc_0.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_1_ffn_fc_0.w_0_moment1_0'], Moment2Out=['encoder_layer_1_ffn_fc_0.w_0_moment2_0'], ParamOut=['encoder_layer_1_ffn_fc_0.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_1_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_1_ffn_fc_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_49.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_1_ffn_fc_0.w_0_moment1_0'], Moment2=['encoder_layer_1_ffn_fc_0.w_0_moment2_0'], Param=['encoder_layer_1_ffn_fc_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_49/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_0.w_0', 'elementwise_mul_49.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_1_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_1_ffn_fc_1.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_1_ffn_fc_1.b_0_moment1_0'], Moment2Out=['encoder_layer_1_ffn_fc_1.b_0_moment2_0'], ParamOut=['encoder_layer_1_ffn_fc_1.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_1_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_1_ffn_fc_1.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_50.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_1_ffn_fc_1.b_0_moment1_0'], Moment2=['encoder_layer_1_ffn_fc_1.b_0_moment2_0'], Param=['encoder_layer_1_ffn_fc_1.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_50/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_1.b_0', 'elementwise_mul_50.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_1_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_1_ffn_fc_1.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_1_ffn_fc_1.w_0_moment1_0'], Moment2Out=['encoder_layer_1_ffn_fc_1.w_0_moment2_0'], ParamOut=['encoder_layer_1_ffn_fc_1.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_1_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_1_ffn_fc_1.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_51.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_1_ffn_fc_1.w_0_moment1_0'], Moment2=['encoder_layer_1_ffn_fc_1.w_0_moment2_0'], Param=['encoder_layer_1_ffn_fc_1.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_51/, op_role = 2, op_role_var = ['encoder_layer_1_ffn_fc_1.w_0', 'elementwise_mul_51.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_1_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_1_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_1_multi_head_att_key_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_1_multi_head_att_key_fc.b_0_moment2_0'], ParamOut=['encoder_layer_1_multi_head_att_key_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_1_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_1_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_52.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_1_multi_head_att_key_fc.b_0_moment1_0'], Moment2=['encoder_layer_1_multi_head_att_key_fc.b_0_moment2_0'], Param=['encoder_layer_1_multi_head_att_key_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_52/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.b_0', 'elementwise_mul_52.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_1_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_1_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_1_multi_head_att_key_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_1_multi_head_att_key_fc.w_0_moment2_0'], ParamOut=['encoder_layer_1_multi_head_att_key_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_1_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_1_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_53.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_1_multi_head_att_key_fc.w_0_moment1_0'], Moment2=['encoder_layer_1_multi_head_att_key_fc.w_0_moment2_0'], Param=['encoder_layer_1_multi_head_att_key_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_53/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_key_fc.w_0', 'elementwise_mul_53.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_1_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_1_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_1_multi_head_att_output_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_1_multi_head_att_output_fc.b_0_moment2_0'], ParamOut=['encoder_layer_1_multi_head_att_output_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_1_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_1_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_54.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_1_multi_head_att_output_fc.b_0_moment1_0'], Moment2=['encoder_layer_1_multi_head_att_output_fc.b_0_moment2_0'], Param=['encoder_layer_1_multi_head_att_output_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_54/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.b_0', 'elementwise_mul_54.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_1_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_1_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_1_multi_head_att_output_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_1_multi_head_att_output_fc.w_0_moment2_0'], ParamOut=['encoder_layer_1_multi_head_att_output_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_1_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_1_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_55.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_1_multi_head_att_output_fc.w_0_moment1_0'], Moment2=['encoder_layer_1_multi_head_att_output_fc.w_0_moment2_0'], Param=['encoder_layer_1_multi_head_att_output_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_55/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_output_fc.w_0', 'elementwise_mul_55.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_1_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_1_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_1_multi_head_att_query_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_1_multi_head_att_query_fc.b_0_moment2_0'], ParamOut=['encoder_layer_1_multi_head_att_query_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_1_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_1_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_56.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_1_multi_head_att_query_fc.b_0_moment1_0'], Moment2=['encoder_layer_1_multi_head_att_query_fc.b_0_moment2_0'], Param=['encoder_layer_1_multi_head_att_query_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_56/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.b_0', 'elementwise_mul_56.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_1_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_1_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_1_multi_head_att_query_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_1_multi_head_att_query_fc.w_0_moment2_0'], ParamOut=['encoder_layer_1_multi_head_att_query_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_1_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_1_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_57.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_1_multi_head_att_query_fc.w_0_moment1_0'], Moment2=['encoder_layer_1_multi_head_att_query_fc.w_0_moment2_0'], Param=['encoder_layer_1_multi_head_att_query_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_57/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_query_fc.w_0', 'elementwise_mul_57.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_1_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_1_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_1_multi_head_att_value_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_1_multi_head_att_value_fc.b_0_moment2_0'], ParamOut=['encoder_layer_1_multi_head_att_value_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_1_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_1_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_58.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_1_multi_head_att_value_fc.b_0_moment1_0'], Moment2=['encoder_layer_1_multi_head_att_value_fc.b_0_moment2_0'], Param=['encoder_layer_1_multi_head_att_value_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_58/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.b_0', 'elementwise_mul_58.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_1_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_1_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_1_multi_head_att_value_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_1_multi_head_att_value_fc.w_0_moment2_0'], ParamOut=['encoder_layer_1_multi_head_att_value_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_1_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_1_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_59.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_1_multi_head_att_value_fc.w_0_moment1_0'], Moment2=['encoder_layer_1_multi_head_att_value_fc.w_0_moment2_0'], Param=['encoder_layer_1_multi_head_att_value_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_59/, op_role = 2, op_role_var = ['encoder_layer_1_multi_head_att_value_fc.w_0', 'elementwise_mul_59.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_1_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_1_post_att_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_1_post_att_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_1_post_att_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_1_post_att_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_1_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_1_post_att_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_60.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_1_post_att_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_1_post_att_layer_norm_bias_moment2_0'], Param=['encoder_layer_1_post_att_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_60/, op_role = 2, op_role_var = ['encoder_layer_1_post_att_layer_norm_bias', 'elementwise_mul_60.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_1_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_1_post_att_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_1_post_att_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_1_post_att_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_1_post_att_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_1_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_1_post_att_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_61.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_1_post_att_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_1_post_att_layer_norm_scale_moment2_0'], Param=['encoder_layer_1_post_att_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_61/, op_role = 2, op_role_var = ['encoder_layer_1_post_att_layer_norm_scale', 'elementwise_mul_61.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_1_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_1_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_1_post_ffn_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_1_post_ffn_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_1_post_ffn_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_1_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_1_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_62.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_1_post_ffn_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_1_post_ffn_layer_norm_bias_moment2_0'], Param=['encoder_layer_1_post_ffn_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_62/, op_role = 2, op_role_var = ['encoder_layer_1_post_ffn_layer_norm_bias', 'elementwise_mul_62.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_1_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_1_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_1_post_ffn_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_1_post_ffn_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_1_post_ffn_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_1_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_1_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_63.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_1_post_ffn_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_1_post_ffn_layer_norm_scale_moment2_0'], Param=['encoder_layer_1_post_ffn_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_63/, op_role = 2, op_role_var = ['encoder_layer_1_post_ffn_layer_norm_scale', 'elementwise_mul_63.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_2_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_2_ffn_fc_0.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_2_ffn_fc_0.b_0_moment1_0'], Moment2Out=['encoder_layer_2_ffn_fc_0.b_0_moment2_0'], ParamOut=['encoder_layer_2_ffn_fc_0.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_2_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_2_ffn_fc_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_64.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_2_ffn_fc_0.b_0_moment1_0'], Moment2=['encoder_layer_2_ffn_fc_0.b_0_moment2_0'], Param=['encoder_layer_2_ffn_fc_0.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_64/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_0.b_0', 'elementwise_mul_64.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_2_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_2_ffn_fc_0.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_2_ffn_fc_0.w_0_moment1_0'], Moment2Out=['encoder_layer_2_ffn_fc_0.w_0_moment2_0'], ParamOut=['encoder_layer_2_ffn_fc_0.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_2_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_2_ffn_fc_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_65.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_2_ffn_fc_0.w_0_moment1_0'], Moment2=['encoder_layer_2_ffn_fc_0.w_0_moment2_0'], Param=['encoder_layer_2_ffn_fc_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_65/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_0.w_0', 'elementwise_mul_65.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_2_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_2_ffn_fc_1.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_2_ffn_fc_1.b_0_moment1_0'], Moment2Out=['encoder_layer_2_ffn_fc_1.b_0_moment2_0'], ParamOut=['encoder_layer_2_ffn_fc_1.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_2_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_2_ffn_fc_1.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_66.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_2_ffn_fc_1.b_0_moment1_0'], Moment2=['encoder_layer_2_ffn_fc_1.b_0_moment2_0'], Param=['encoder_layer_2_ffn_fc_1.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_66/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_1.b_0', 'elementwise_mul_66.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_2_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_2_ffn_fc_1.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_2_ffn_fc_1.w_0_moment1_0'], Moment2Out=['encoder_layer_2_ffn_fc_1.w_0_moment2_0'], ParamOut=['encoder_layer_2_ffn_fc_1.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_2_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_2_ffn_fc_1.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_67.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_2_ffn_fc_1.w_0_moment1_0'], Moment2=['encoder_layer_2_ffn_fc_1.w_0_moment2_0'], Param=['encoder_layer_2_ffn_fc_1.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_67/, op_role = 2, op_role_var = ['encoder_layer_2_ffn_fc_1.w_0', 'elementwise_mul_67.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_2_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_2_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_2_multi_head_att_key_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_2_multi_head_att_key_fc.b_0_moment2_0'], ParamOut=['encoder_layer_2_multi_head_att_key_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_2_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_2_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_68.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_2_multi_head_att_key_fc.b_0_moment1_0'], Moment2=['encoder_layer_2_multi_head_att_key_fc.b_0_moment2_0'], Param=['encoder_layer_2_multi_head_att_key_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_68/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.b_0', 'elementwise_mul_68.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_2_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_2_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_2_multi_head_att_key_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_2_multi_head_att_key_fc.w_0_moment2_0'], ParamOut=['encoder_layer_2_multi_head_att_key_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_2_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_2_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_69.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_2_multi_head_att_key_fc.w_0_moment1_0'], Moment2=['encoder_layer_2_multi_head_att_key_fc.w_0_moment2_0'], Param=['encoder_layer_2_multi_head_att_key_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_69/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_key_fc.w_0', 'elementwise_mul_69.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_2_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_2_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_2_multi_head_att_output_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_2_multi_head_att_output_fc.b_0_moment2_0'], ParamOut=['encoder_layer_2_multi_head_att_output_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_2_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_2_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_70.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_2_multi_head_att_output_fc.b_0_moment1_0'], Moment2=['encoder_layer_2_multi_head_att_output_fc.b_0_moment2_0'], Param=['encoder_layer_2_multi_head_att_output_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_70/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.b_0', 'elementwise_mul_70.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_2_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_2_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_2_multi_head_att_output_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_2_multi_head_att_output_fc.w_0_moment2_0'], ParamOut=['encoder_layer_2_multi_head_att_output_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_2_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_2_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_71.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_2_multi_head_att_output_fc.w_0_moment1_0'], Moment2=['encoder_layer_2_multi_head_att_output_fc.w_0_moment2_0'], Param=['encoder_layer_2_multi_head_att_output_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_71/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_output_fc.w_0', 'elementwise_mul_71.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_2_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_2_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_2_multi_head_att_query_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_2_multi_head_att_query_fc.b_0_moment2_0'], ParamOut=['encoder_layer_2_multi_head_att_query_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_2_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_2_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_72.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_2_multi_head_att_query_fc.b_0_moment1_0'], Moment2=['encoder_layer_2_multi_head_att_query_fc.b_0_moment2_0'], Param=['encoder_layer_2_multi_head_att_query_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_72/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.b_0', 'elementwise_mul_72.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_2_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_2_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_2_multi_head_att_query_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_2_multi_head_att_query_fc.w_0_moment2_0'], ParamOut=['encoder_layer_2_multi_head_att_query_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_2_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_2_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_73.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_2_multi_head_att_query_fc.w_0_moment1_0'], Moment2=['encoder_layer_2_multi_head_att_query_fc.w_0_moment2_0'], Param=['encoder_layer_2_multi_head_att_query_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_73/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_query_fc.w_0', 'elementwise_mul_73.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_2_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_2_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_2_multi_head_att_value_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_2_multi_head_att_value_fc.b_0_moment2_0'], ParamOut=['encoder_layer_2_multi_head_att_value_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_2_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_2_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_74.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_2_multi_head_att_value_fc.b_0_moment1_0'], Moment2=['encoder_layer_2_multi_head_att_value_fc.b_0_moment2_0'], Param=['encoder_layer_2_multi_head_att_value_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_74/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.b_0', 'elementwise_mul_74.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_2_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_2_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_2_multi_head_att_value_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_2_multi_head_att_value_fc.w_0_moment2_0'], ParamOut=['encoder_layer_2_multi_head_att_value_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_2_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_2_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_75.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_2_multi_head_att_value_fc.w_0_moment1_0'], Moment2=['encoder_layer_2_multi_head_att_value_fc.w_0_moment2_0'], Param=['encoder_layer_2_multi_head_att_value_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_75/, op_role = 2, op_role_var = ['encoder_layer_2_multi_head_att_value_fc.w_0', 'elementwise_mul_75.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_2_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_2_post_att_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_2_post_att_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_2_post_att_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_2_post_att_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_2_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_2_post_att_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_76.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_2_post_att_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_2_post_att_layer_norm_bias_moment2_0'], Param=['encoder_layer_2_post_att_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_76/, op_role = 2, op_role_var = ['encoder_layer_2_post_att_layer_norm_bias', 'elementwise_mul_76.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_2_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_2_post_att_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_2_post_att_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_2_post_att_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_2_post_att_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_2_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_2_post_att_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_77.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_2_post_att_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_2_post_att_layer_norm_scale_moment2_0'], Param=['encoder_layer_2_post_att_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_77/, op_role = 2, op_role_var = ['encoder_layer_2_post_att_layer_norm_scale', 'elementwise_mul_77.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_2_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_2_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_2_post_ffn_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_2_post_ffn_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_2_post_ffn_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_2_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_2_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_78.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_2_post_ffn_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_2_post_ffn_layer_norm_bias_moment2_0'], Param=['encoder_layer_2_post_ffn_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_78/, op_role = 2, op_role_var = ['encoder_layer_2_post_ffn_layer_norm_bias', 'elementwise_mul_78.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_2_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_2_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_2_post_ffn_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_2_post_ffn_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_2_post_ffn_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_2_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_2_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_79.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_2_post_ffn_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_2_post_ffn_layer_norm_scale_moment2_0'], Param=['encoder_layer_2_post_ffn_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_79/, op_role = 2, op_role_var = ['encoder_layer_2_post_ffn_layer_norm_scale', 'elementwise_mul_79.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_3_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_3_ffn_fc_0.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_3_ffn_fc_0.b_0_moment1_0'], Moment2Out=['encoder_layer_3_ffn_fc_0.b_0_moment2_0'], ParamOut=['encoder_layer_3_ffn_fc_0.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_3_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_3_ffn_fc_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_80.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_3_ffn_fc_0.b_0_moment1_0'], Moment2=['encoder_layer_3_ffn_fc_0.b_0_moment2_0'], Param=['encoder_layer_3_ffn_fc_0.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_80/, op_role = 2, op_role_var = ['encoder_layer_3_ffn_fc_0.b_0', 'elementwise_mul_80.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_3_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_3_ffn_fc_0.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_3_ffn_fc_0.w_0_moment1_0'], Moment2Out=['encoder_layer_3_ffn_fc_0.w_0_moment2_0'], ParamOut=['encoder_layer_3_ffn_fc_0.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_3_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_3_ffn_fc_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_81.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_3_ffn_fc_0.w_0_moment1_0'], Moment2=['encoder_layer_3_ffn_fc_0.w_0_moment2_0'], Param=['encoder_layer_3_ffn_fc_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_81/, op_role = 2, op_role_var = ['encoder_layer_3_ffn_fc_0.w_0', 'elementwise_mul_81.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_3_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_3_ffn_fc_1.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_3_ffn_fc_1.b_0_moment1_0'], Moment2Out=['encoder_layer_3_ffn_fc_1.b_0_moment2_0'], ParamOut=['encoder_layer_3_ffn_fc_1.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_3_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_3_ffn_fc_1.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_82.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_3_ffn_fc_1.b_0_moment1_0'], Moment2=['encoder_layer_3_ffn_fc_1.b_0_moment2_0'], Param=['encoder_layer_3_ffn_fc_1.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_82/, op_role = 2, op_role_var = ['encoder_layer_3_ffn_fc_1.b_0', 'elementwise_mul_82.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_3_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_3_ffn_fc_1.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_3_ffn_fc_1.w_0_moment1_0'], Moment2Out=['encoder_layer_3_ffn_fc_1.w_0_moment2_0'], ParamOut=['encoder_layer_3_ffn_fc_1.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_3_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_3_ffn_fc_1.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_83.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_3_ffn_fc_1.w_0_moment1_0'], Moment2=['encoder_layer_3_ffn_fc_1.w_0_moment2_0'], Param=['encoder_layer_3_ffn_fc_1.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_83/, op_role = 2, op_role_var = ['encoder_layer_3_ffn_fc_1.w_0', 'elementwise_mul_83.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_3_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_3_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_3_multi_head_att_key_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_3_multi_head_att_key_fc.b_0_moment2_0'], ParamOut=['encoder_layer_3_multi_head_att_key_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_3_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_3_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_84.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_3_multi_head_att_key_fc.b_0_moment1_0'], Moment2=['encoder_layer_3_multi_head_att_key_fc.b_0_moment2_0'], Param=['encoder_layer_3_multi_head_att_key_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_84/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_key_fc.b_0', 'elementwise_mul_84.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_3_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_3_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_3_multi_head_att_key_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_3_multi_head_att_key_fc.w_0_moment2_0'], ParamOut=['encoder_layer_3_multi_head_att_key_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_3_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_3_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_85.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_3_multi_head_att_key_fc.w_0_moment1_0'], Moment2=['encoder_layer_3_multi_head_att_key_fc.w_0_moment2_0'], Param=['encoder_layer_3_multi_head_att_key_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_85/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_key_fc.w_0', 'elementwise_mul_85.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_3_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_3_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_3_multi_head_att_output_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_3_multi_head_att_output_fc.b_0_moment2_0'], ParamOut=['encoder_layer_3_multi_head_att_output_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_3_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_3_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_86.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_3_multi_head_att_output_fc.b_0_moment1_0'], Moment2=['encoder_layer_3_multi_head_att_output_fc.b_0_moment2_0'], Param=['encoder_layer_3_multi_head_att_output_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_86/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_output_fc.b_0', 'elementwise_mul_86.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_3_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_3_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_3_multi_head_att_output_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_3_multi_head_att_output_fc.w_0_moment2_0'], ParamOut=['encoder_layer_3_multi_head_att_output_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_3_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_3_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_87.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_3_multi_head_att_output_fc.w_0_moment1_0'], Moment2=['encoder_layer_3_multi_head_att_output_fc.w_0_moment2_0'], Param=['encoder_layer_3_multi_head_att_output_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_87/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_output_fc.w_0', 'elementwise_mul_87.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_3_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_3_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_3_multi_head_att_query_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_3_multi_head_att_query_fc.b_0_moment2_0'], ParamOut=['encoder_layer_3_multi_head_att_query_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_3_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_3_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_88.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_3_multi_head_att_query_fc.b_0_moment1_0'], Moment2=['encoder_layer_3_multi_head_att_query_fc.b_0_moment2_0'], Param=['encoder_layer_3_multi_head_att_query_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_88/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_query_fc.b_0', 'elementwise_mul_88.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_3_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_3_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_3_multi_head_att_query_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_3_multi_head_att_query_fc.w_0_moment2_0'], ParamOut=['encoder_layer_3_multi_head_att_query_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_3_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_3_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_89.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_3_multi_head_att_query_fc.w_0_moment1_0'], Moment2=['encoder_layer_3_multi_head_att_query_fc.w_0_moment2_0'], Param=['encoder_layer_3_multi_head_att_query_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_89/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_query_fc.w_0', 'elementwise_mul_89.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_3_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_3_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_3_multi_head_att_value_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_3_multi_head_att_value_fc.b_0_moment2_0'], ParamOut=['encoder_layer_3_multi_head_att_value_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_3_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_3_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_90.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_3_multi_head_att_value_fc.b_0_moment1_0'], Moment2=['encoder_layer_3_multi_head_att_value_fc.b_0_moment2_0'], Param=['encoder_layer_3_multi_head_att_value_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_90/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_value_fc.b_0', 'elementwise_mul_90.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_3_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_3_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_3_multi_head_att_value_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_3_multi_head_att_value_fc.w_0_moment2_0'], ParamOut=['encoder_layer_3_multi_head_att_value_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_3_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_3_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_91.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_3_multi_head_att_value_fc.w_0_moment1_0'], Moment2=['encoder_layer_3_multi_head_att_value_fc.w_0_moment2_0'], Param=['encoder_layer_3_multi_head_att_value_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_91/, op_role = 2, op_role_var = ['encoder_layer_3_multi_head_att_value_fc.w_0', 'elementwise_mul_91.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_3_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_3_post_att_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_3_post_att_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_3_post_att_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_3_post_att_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_3_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_3_post_att_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_92.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_3_post_att_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_3_post_att_layer_norm_bias_moment2_0'], Param=['encoder_layer_3_post_att_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_92/, op_role = 2, op_role_var = ['encoder_layer_3_post_att_layer_norm_bias', 'elementwise_mul_92.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_3_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_3_post_att_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_3_post_att_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_3_post_att_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_3_post_att_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_3_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_3_post_att_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_93.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_3_post_att_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_3_post_att_layer_norm_scale_moment2_0'], Param=['encoder_layer_3_post_att_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_93/, op_role = 2, op_role_var = ['encoder_layer_3_post_att_layer_norm_scale', 'elementwise_mul_93.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_3_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_3_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_3_post_ffn_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_3_post_ffn_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_3_post_ffn_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_3_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_3_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_94.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_3_post_ffn_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_3_post_ffn_layer_norm_bias_moment2_0'], Param=['encoder_layer_3_post_ffn_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_94/, op_role = 2, op_role_var = ['encoder_layer_3_post_ffn_layer_norm_bias', 'elementwise_mul_94.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_3_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_3_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_3_post_ffn_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_3_post_ffn_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_3_post_ffn_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_3_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_3_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_95.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_3_post_ffn_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_3_post_ffn_layer_norm_scale_moment2_0'], Param=['encoder_layer_3_post_ffn_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_95/, op_role = 2, op_role_var = ['encoder_layer_3_post_ffn_layer_norm_scale', 'elementwise_mul_95.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_4_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_4_ffn_fc_0.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_4_ffn_fc_0.b_0_moment1_0'], Moment2Out=['encoder_layer_4_ffn_fc_0.b_0_moment2_0'], ParamOut=['encoder_layer_4_ffn_fc_0.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_4_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_4_ffn_fc_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_96.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_4_ffn_fc_0.b_0_moment1_0'], Moment2=['encoder_layer_4_ffn_fc_0.b_0_moment2_0'], Param=['encoder_layer_4_ffn_fc_0.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_96/, op_role = 2, op_role_var = ['encoder_layer_4_ffn_fc_0.b_0', 'elementwise_mul_96.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_4_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_4_ffn_fc_0.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_4_ffn_fc_0.w_0_moment1_0'], Moment2Out=['encoder_layer_4_ffn_fc_0.w_0_moment2_0'], ParamOut=['encoder_layer_4_ffn_fc_0.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_4_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_4_ffn_fc_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_97.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_4_ffn_fc_0.w_0_moment1_0'], Moment2=['encoder_layer_4_ffn_fc_0.w_0_moment2_0'], Param=['encoder_layer_4_ffn_fc_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_97/, op_role = 2, op_role_var = ['encoder_layer_4_ffn_fc_0.w_0', 'elementwise_mul_97.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_4_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_4_ffn_fc_1.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_4_ffn_fc_1.b_0_moment1_0'], Moment2Out=['encoder_layer_4_ffn_fc_1.b_0_moment2_0'], ParamOut=['encoder_layer_4_ffn_fc_1.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_4_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_4_ffn_fc_1.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_98.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_4_ffn_fc_1.b_0_moment1_0'], Moment2=['encoder_layer_4_ffn_fc_1.b_0_moment2_0'], Param=['encoder_layer_4_ffn_fc_1.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_98/, op_role = 2, op_role_var = ['encoder_layer_4_ffn_fc_1.b_0', 'elementwise_mul_98.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_4_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_4_ffn_fc_1.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_4_ffn_fc_1.w_0_moment1_0'], Moment2Out=['encoder_layer_4_ffn_fc_1.w_0_moment2_0'], ParamOut=['encoder_layer_4_ffn_fc_1.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_4_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_4_ffn_fc_1.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_99.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_4_ffn_fc_1.w_0_moment1_0'], Moment2=['encoder_layer_4_ffn_fc_1.w_0_moment2_0'], Param=['encoder_layer_4_ffn_fc_1.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_99/, op_role = 2, op_role_var = ['encoder_layer_4_ffn_fc_1.w_0', 'elementwise_mul_99.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_4_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_4_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_4_multi_head_att_key_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_4_multi_head_att_key_fc.b_0_moment2_0'], ParamOut=['encoder_layer_4_multi_head_att_key_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_4_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_4_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_100.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_4_multi_head_att_key_fc.b_0_moment1_0'], Moment2=['encoder_layer_4_multi_head_att_key_fc.b_0_moment2_0'], Param=['encoder_layer_4_multi_head_att_key_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_100/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_key_fc.b_0', 'elementwise_mul_100.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_4_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_4_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_4_multi_head_att_key_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_4_multi_head_att_key_fc.w_0_moment2_0'], ParamOut=['encoder_layer_4_multi_head_att_key_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_4_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_4_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_101.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_4_multi_head_att_key_fc.w_0_moment1_0'], Moment2=['encoder_layer_4_multi_head_att_key_fc.w_0_moment2_0'], Param=['encoder_layer_4_multi_head_att_key_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_101/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_key_fc.w_0', 'elementwise_mul_101.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_4_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_4_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_4_multi_head_att_output_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_4_multi_head_att_output_fc.b_0_moment2_0'], ParamOut=['encoder_layer_4_multi_head_att_output_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_4_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_4_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_102.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_4_multi_head_att_output_fc.b_0_moment1_0'], Moment2=['encoder_layer_4_multi_head_att_output_fc.b_0_moment2_0'], Param=['encoder_layer_4_multi_head_att_output_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_102/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_output_fc.b_0', 'elementwise_mul_102.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_4_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_4_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_4_multi_head_att_output_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_4_multi_head_att_output_fc.w_0_moment2_0'], ParamOut=['encoder_layer_4_multi_head_att_output_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_4_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_4_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_103.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_4_multi_head_att_output_fc.w_0_moment1_0'], Moment2=['encoder_layer_4_multi_head_att_output_fc.w_0_moment2_0'], Param=['encoder_layer_4_multi_head_att_output_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_103/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_output_fc.w_0', 'elementwise_mul_103.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_4_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_4_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_4_multi_head_att_query_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_4_multi_head_att_query_fc.b_0_moment2_0'], ParamOut=['encoder_layer_4_multi_head_att_query_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_4_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_4_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_104.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_4_multi_head_att_query_fc.b_0_moment1_0'], Moment2=['encoder_layer_4_multi_head_att_query_fc.b_0_moment2_0'], Param=['encoder_layer_4_multi_head_att_query_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_104/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_query_fc.b_0', 'elementwise_mul_104.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_4_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_4_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_4_multi_head_att_query_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_4_multi_head_att_query_fc.w_0_moment2_0'], ParamOut=['encoder_layer_4_multi_head_att_query_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_4_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_4_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_105.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_4_multi_head_att_query_fc.w_0_moment1_0'], Moment2=['encoder_layer_4_multi_head_att_query_fc.w_0_moment2_0'], Param=['encoder_layer_4_multi_head_att_query_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_105/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_query_fc.w_0', 'elementwise_mul_105.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_4_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_4_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_4_multi_head_att_value_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_4_multi_head_att_value_fc.b_0_moment2_0'], ParamOut=['encoder_layer_4_multi_head_att_value_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_4_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_4_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_106.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_4_multi_head_att_value_fc.b_0_moment1_0'], Moment2=['encoder_layer_4_multi_head_att_value_fc.b_0_moment2_0'], Param=['encoder_layer_4_multi_head_att_value_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_106/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_value_fc.b_0', 'elementwise_mul_106.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_4_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_4_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_4_multi_head_att_value_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_4_multi_head_att_value_fc.w_0_moment2_0'], ParamOut=['encoder_layer_4_multi_head_att_value_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_4_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_4_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_107.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_4_multi_head_att_value_fc.w_0_moment1_0'], Moment2=['encoder_layer_4_multi_head_att_value_fc.w_0_moment2_0'], Param=['encoder_layer_4_multi_head_att_value_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_107/, op_role = 2, op_role_var = ['encoder_layer_4_multi_head_att_value_fc.w_0', 'elementwise_mul_107.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_4_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_4_post_att_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_4_post_att_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_4_post_att_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_4_post_att_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_4_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_4_post_att_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_108.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_4_post_att_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_4_post_att_layer_norm_bias_moment2_0'], Param=['encoder_layer_4_post_att_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_108/, op_role = 2, op_role_var = ['encoder_layer_4_post_att_layer_norm_bias', 'elementwise_mul_108.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_4_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_4_post_att_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_4_post_att_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_4_post_att_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_4_post_att_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_4_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_4_post_att_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_109.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_4_post_att_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_4_post_att_layer_norm_scale_moment2_0'], Param=['encoder_layer_4_post_att_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_109/, op_role = 2, op_role_var = ['encoder_layer_4_post_att_layer_norm_scale', 'elementwise_mul_109.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_4_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_4_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_4_post_ffn_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_4_post_ffn_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_4_post_ffn_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_4_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_4_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_110.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_4_post_ffn_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_4_post_ffn_layer_norm_bias_moment2_0'], Param=['encoder_layer_4_post_ffn_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_110/, op_role = 2, op_role_var = ['encoder_layer_4_post_ffn_layer_norm_bias', 'elementwise_mul_110.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_4_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_4_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_4_post_ffn_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_4_post_ffn_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_4_post_ffn_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_4_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_4_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_111.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_4_post_ffn_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_4_post_ffn_layer_norm_scale_moment2_0'], Param=['encoder_layer_4_post_ffn_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_111/, op_role = 2, op_role_var = ['encoder_layer_4_post_ffn_layer_norm_scale', 'elementwise_mul_111.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_5_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_5_ffn_fc_0.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_5_ffn_fc_0.b_0_moment1_0'], Moment2Out=['encoder_layer_5_ffn_fc_0.b_0_moment2_0'], ParamOut=['encoder_layer_5_ffn_fc_0.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_5_ffn_fc_0.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_5_ffn_fc_0.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_112.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_5_ffn_fc_0.b_0_moment1_0'], Moment2=['encoder_layer_5_ffn_fc_0.b_0_moment2_0'], Param=['encoder_layer_5_ffn_fc_0.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_112/, op_role = 2, op_role_var = ['encoder_layer_5_ffn_fc_0.b_0', 'elementwise_mul_112.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_5_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_5_ffn_fc_0.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_5_ffn_fc_0.w_0_moment1_0'], Moment2Out=['encoder_layer_5_ffn_fc_0.w_0_moment2_0'], ParamOut=['encoder_layer_5_ffn_fc_0.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_5_ffn_fc_0.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_5_ffn_fc_0.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_113.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_5_ffn_fc_0.w_0_moment1_0'], Moment2=['encoder_layer_5_ffn_fc_0.w_0_moment2_0'], Param=['encoder_layer_5_ffn_fc_0.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_113/, op_role = 2, op_role_var = ['encoder_layer_5_ffn_fc_0.w_0', 'elementwise_mul_113.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_5_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_5_ffn_fc_1.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_5_ffn_fc_1.b_0_moment1_0'], Moment2Out=['encoder_layer_5_ffn_fc_1.b_0_moment2_0'], ParamOut=['encoder_layer_5_ffn_fc_1.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_5_ffn_fc_1.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_5_ffn_fc_1.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_114.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_5_ffn_fc_1.b_0_moment1_0'], Moment2=['encoder_layer_5_ffn_fc_1.b_0_moment2_0'], Param=['encoder_layer_5_ffn_fc_1.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_114/, op_role = 2, op_role_var = ['encoder_layer_5_ffn_fc_1.b_0', 'elementwise_mul_114.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_5_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_5_ffn_fc_1.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_5_ffn_fc_1.w_0_moment1_0'], Moment2Out=['encoder_layer_5_ffn_fc_1.w_0_moment2_0'], ParamOut=['encoder_layer_5_ffn_fc_1.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_5_ffn_fc_1.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_5_ffn_fc_1.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_115.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_5_ffn_fc_1.w_0_moment1_0'], Moment2=['encoder_layer_5_ffn_fc_1.w_0_moment2_0'], Param=['encoder_layer_5_ffn_fc_1.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_115/, op_role = 2, op_role_var = ['encoder_layer_5_ffn_fc_1.w_0', 'elementwise_mul_115.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_5_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_5_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_5_multi_head_att_key_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_5_multi_head_att_key_fc.b_0_moment2_0'], ParamOut=['encoder_layer_5_multi_head_att_key_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_5_multi_head_att_key_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_5_multi_head_att_key_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_116.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_5_multi_head_att_key_fc.b_0_moment1_0'], Moment2=['encoder_layer_5_multi_head_att_key_fc.b_0_moment2_0'], Param=['encoder_layer_5_multi_head_att_key_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_116/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_key_fc.b_0', 'elementwise_mul_116.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_5_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_5_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_5_multi_head_att_key_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_5_multi_head_att_key_fc.w_0_moment2_0'], ParamOut=['encoder_layer_5_multi_head_att_key_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_5_multi_head_att_key_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_5_multi_head_att_key_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_117.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_5_multi_head_att_key_fc.w_0_moment1_0'], Moment2=['encoder_layer_5_multi_head_att_key_fc.w_0_moment2_0'], Param=['encoder_layer_5_multi_head_att_key_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_117/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_key_fc.w_0', 'elementwise_mul_117.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_5_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_5_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_5_multi_head_att_output_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_5_multi_head_att_output_fc.b_0_moment2_0'], ParamOut=['encoder_layer_5_multi_head_att_output_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_5_multi_head_att_output_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_5_multi_head_att_output_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_118.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_5_multi_head_att_output_fc.b_0_moment1_0'], Moment2=['encoder_layer_5_multi_head_att_output_fc.b_0_moment2_0'], Param=['encoder_layer_5_multi_head_att_output_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_118/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_output_fc.b_0', 'elementwise_mul_118.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_5_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_5_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_5_multi_head_att_output_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_5_multi_head_att_output_fc.w_0_moment2_0'], ParamOut=['encoder_layer_5_multi_head_att_output_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_5_multi_head_att_output_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_5_multi_head_att_output_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_119.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_5_multi_head_att_output_fc.w_0_moment1_0'], Moment2=['encoder_layer_5_multi_head_att_output_fc.w_0_moment2_0'], Param=['encoder_layer_5_multi_head_att_output_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_119/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_output_fc.w_0', 'elementwise_mul_119.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_5_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_5_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_5_multi_head_att_query_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_5_multi_head_att_query_fc.b_0_moment2_0'], ParamOut=['encoder_layer_5_multi_head_att_query_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_5_multi_head_att_query_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_5_multi_head_att_query_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_120.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_5_multi_head_att_query_fc.b_0_moment1_0'], Moment2=['encoder_layer_5_multi_head_att_query_fc.b_0_moment2_0'], Param=['encoder_layer_5_multi_head_att_query_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_120/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_query_fc.b_0', 'elementwise_mul_120.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_5_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_5_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_5_multi_head_att_query_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_5_multi_head_att_query_fc.w_0_moment2_0'], ParamOut=['encoder_layer_5_multi_head_att_query_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_5_multi_head_att_query_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_5_multi_head_att_query_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_121.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_5_multi_head_att_query_fc.w_0_moment1_0'], Moment2=['encoder_layer_5_multi_head_att_query_fc.w_0_moment2_0'], Param=['encoder_layer_5_multi_head_att_query_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_121/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_query_fc.w_0', 'elementwise_mul_121.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_5_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_5_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_5_multi_head_att_value_fc.b_0_moment1_0'], Moment2Out=['encoder_layer_5_multi_head_att_value_fc.b_0_moment2_0'], ParamOut=['encoder_layer_5_multi_head_att_value_fc.b_0']} = adam(inputs={Beta1Pow=['encoder_layer_5_multi_head_att_value_fc.b_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_5_multi_head_att_value_fc.b_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_122.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_5_multi_head_att_value_fc.b_0_moment1_0'], Moment2=['encoder_layer_5_multi_head_att_value_fc.b_0_moment2_0'], Param=['encoder_layer_5_multi_head_att_value_fc.b_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_122/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_value_fc.b_0', 'elementwise_mul_122.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_5_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_5_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Moment1Out=['encoder_layer_5_multi_head_att_value_fc.w_0_moment1_0'], Moment2Out=['encoder_layer_5_multi_head_att_value_fc.w_0_moment2_0'], ParamOut=['encoder_layer_5_multi_head_att_value_fc.w_0']} = adam(inputs={Beta1Pow=['encoder_layer_5_multi_head_att_value_fc.w_0_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_5_multi_head_att_value_fc.w_0_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_123.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_5_multi_head_att_value_fc.w_0_moment1_0'], Moment2=['encoder_layer_5_multi_head_att_value_fc.w_0_moment2_0'], Param=['encoder_layer_5_multi_head_att_value_fc.w_0']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_123/, op_role = 2, op_role_var = ['encoder_layer_5_multi_head_att_value_fc.w_0', 'elementwise_mul_123.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_5_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_5_post_att_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_5_post_att_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_5_post_att_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_5_post_att_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_5_post_att_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_5_post_att_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_124.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_5_post_att_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_5_post_att_layer_norm_bias_moment2_0'], Param=['encoder_layer_5_post_att_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_124/, op_role = 2, op_role_var = ['encoder_layer_5_post_att_layer_norm_bias', 'elementwise_mul_124.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_5_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_5_post_att_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_5_post_att_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_5_post_att_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_5_post_att_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_5_post_att_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_5_post_att_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_125.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_5_post_att_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_5_post_att_layer_norm_scale_moment2_0'], Param=['encoder_layer_5_post_att_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_125/, op_role = 2, op_role_var = ['encoder_layer_5_post_att_layer_norm_scale', 'elementwise_mul_125.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_5_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_5_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['encoder_layer_5_post_ffn_layer_norm_bias_moment1_0'], Moment2Out=['encoder_layer_5_post_ffn_layer_norm_bias_moment2_0'], ParamOut=['encoder_layer_5_post_ffn_layer_norm_bias']} = adam(inputs={Beta1Pow=['encoder_layer_5_post_ffn_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_5_post_ffn_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_126.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_5_post_ffn_layer_norm_bias_moment1_0'], Moment2=['encoder_layer_5_post_ffn_layer_norm_bias_moment2_0'], Param=['encoder_layer_5_post_ffn_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_126/, op_role = 2, op_role_var = ['encoder_layer_5_post_ffn_layer_norm_bias', 'elementwise_mul_126.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['encoder_layer_5_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['encoder_layer_5_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['encoder_layer_5_post_ffn_layer_norm_scale_moment1_0'], Moment2Out=['encoder_layer_5_post_ffn_layer_norm_scale_moment2_0'], ParamOut=['encoder_layer_5_post_ffn_layer_norm_scale']} = adam(inputs={Beta1Pow=['encoder_layer_5_post_ffn_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['encoder_layer_5_post_ffn_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_127.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['encoder_layer_5_post_ffn_layer_norm_scale_moment1_0'], Moment2=['encoder_layer_5_post_ffn_layer_norm_scale_moment2_0'], Param=['encoder_layer_5_post_ffn_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_127/, op_role = 2, op_role_var = ['encoder_layer_5_post_ffn_layer_norm_scale', 'elementwise_mul_127.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['pos_embedding_beta1_pow_acc_0'], Beta2PowOut=['pos_embedding_beta2_pow_acc_0'], Moment1Out=['pos_embedding_moment1_0'], Moment2Out=['pos_embedding_moment2_0'], ParamOut=['pos_embedding']} = adam(inputs={Beta1Pow=['pos_embedding_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['pos_embedding_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_198.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['pos_embedding_moment1_0'], Moment2=['pos_embedding_moment2_0'], Param=['pos_embedding']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_198/, op_role = 2, op_role_var = ['pos_embedding', 'elementwise_mul_198.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['pre_encoder_layer_norm_bias_beta1_pow_acc_0'], Beta2PowOut=['pre_encoder_layer_norm_bias_beta2_pow_acc_0'], Moment1Out=['pre_encoder_layer_norm_bias_moment1_0'], Moment2Out=['pre_encoder_layer_norm_bias_moment2_0'], ParamOut=['pre_encoder_layer_norm_bias']} = adam(inputs={Beta1Pow=['pre_encoder_layer_norm_bias_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['pre_encoder_layer_norm_bias_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_199.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['pre_encoder_layer_norm_bias_moment1_0'], Moment2=['pre_encoder_layer_norm_bias_moment2_0'], Param=['pre_encoder_layer_norm_bias']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_199/, op_role = 2, op_role_var = ['pre_encoder_layer_norm_bias', 'elementwise_mul_199.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['pre_encoder_layer_norm_scale_beta1_pow_acc_0'], Beta2PowOut=['pre_encoder_layer_norm_scale_beta2_pow_acc_0'], Moment1Out=['pre_encoder_layer_norm_scale_moment1_0'], Moment2Out=['pre_encoder_layer_norm_scale_moment2_0'], ParamOut=['pre_encoder_layer_norm_scale']} = adam(inputs={Beta1Pow=['pre_encoder_layer_norm_scale_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['pre_encoder_layer_norm_scale_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_200.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['pre_encoder_layer_norm_scale_moment1_0'], Moment2=['pre_encoder_layer_norm_scale_moment2_0'], Param=['pre_encoder_layer_norm_scale']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_200/, op_role = 2, op_role_var = ['pre_encoder_layer_norm_scale', 'elementwise_mul_200.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['sent_embedding_beta1_pow_acc_0'], Beta2PowOut=['sent_embedding_beta2_pow_acc_0'], Moment1Out=['sent_embedding_moment1_0'], Moment2Out=['sent_embedding_moment2_0'], ParamOut=['sent_embedding']} = adam(inputs={Beta1Pow=['sent_embedding_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['sent_embedding_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_201.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['sent_embedding_moment1_0'], Moment2=['sent_embedding_moment2_0'], Param=['sent_embedding']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_201/, op_role = 2, op_role_var = ['sent_embedding', 'elementwise_mul_201.tmp_0'], use_global_beta_pow = False)
    {Beta1PowOut=['word_embedding_beta1_pow_acc_0'], Beta2PowOut=['word_embedding_beta2_pow_acc_0'], Moment1Out=['word_embedding_moment1_0'], Moment2Out=['word_embedding_moment2_0'], ParamOut=['word_embedding']} = adam(inputs={Beta1Pow=['word_embedding_beta1_pow_acc_0'], Beta1Tensor=[], Beta2Pow=['word_embedding_beta2_pow_acc_0'], Beta2Tensor=[], EpsilonTensor=[], Grad=['elementwise_mul_202.tmp_0'], LearningRate=['scheduled_learning_rate'], MasterParam=[], Moment1=['word_embedding_moment1_0'], Moment2=['word_embedding_moment2_0'], Param=['word_embedding']}, beta1 = 0.8999999761581421, beta2 = 0.9990000128746033, epsilon = 9.99999993922529e-09, lazy_mode = False, min_row_size_to_use_multithread = 1000, multi_precision = False, op_device = , op_namescope = /optimizer_202/, op_role = 2, op_role_var = ['word_embedding', 'elementwise_mul_202.tmp_0'], use_global_beta_pow = False)
}
{ // block 1
    var tmp_120 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var tmp_121 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var tmp_122 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)

    {Out=['tmp_120']} = fill_constant(inputs={}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [1], str_value = , value = 10000.0)
    {Out=['tmp_121']} = elementwise_div(inputs={X=['cast_2.tmp_0'], Y=['tmp_120']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 16, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_122']} = scale(inputs={ScaleTensor=[], X=['tmp_121']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 16, op_role_var = [], scale = 9.999999747378752e-05, use_mkldnn = False)
    {Out=['scheduled_learning_rate']} = assign(inputs={X=['tmp_122']}, op_device = , op_namescope = /, op_role = 16, op_role_var = [])
}
{ // block 2
    var cast_3.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var fill_constant_5.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var elementwise_min_0.tmp_0 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var tmp_123 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var tmp_124 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var tmp_125 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var tmp_126 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(True)
    var tmp_127 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var tmp_128 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)
    var tmp_129 : LOD_TENSOR.shape(1,).dtype(float32).stop_gradient(False)

    {Out=['cast_3.tmp_0']} = cast(inputs={X=['@LR_DECAY_COUNTER@']}, in_dtype = 3, op_device = , op_namescope = /, op_role = 16, op_role_var = [], out_dtype = 5, use_mkldnn = False)
    {Out=['fill_constant_5.tmp_0']} = fill_constant(inputs={ShapeTensor=[], ShapeTensorList=[], ValueTensor=[]}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [1], str_value = 2000.0, value = 2000.0)
    {Out=['elementwise_min_0.tmp_0']} = elementwise_min(inputs={X=['cast_3.tmp_0'], Y=['fill_constant_5.tmp_0']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 16, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_123']} = fill_constant(inputs={}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [1], str_value = , value = 2000.0)
    {Out=['tmp_124']} = elementwise_div(inputs={X=['elementwise_min_0.tmp_0'], Y=['tmp_123']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 16, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_125']} = scale(inputs={ScaleTensor=[], X=['tmp_124']}, bias = 1.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 16, op_role_var = [], scale = -1.0, use_mkldnn = False)
    {Out=['tmp_126']} = fill_constant(inputs={}, dtype = 5, force_cpu = False, op_device = , op_namescope = /, op_role = 16, op_role_var = [], place_type = -1, shape = [1], str_value = , value = 1.0)
    {Out=['tmp_127']} = elementwise_pow(inputs={X=['tmp_125'], Y=['tmp_126']}, Scale_out = 1.0, Scale_x = 1.0, Scale_y = 1.0, axis = -1, mkldnn_data_type = float32, op_device = , op_namescope = /, op_role = 16, op_role_var = [], use_mkldnn = False, use_quantizer = False, x_data_format = , y_data_format = )
    {Out=['tmp_128']} = scale(inputs={ScaleTensor=[], X=['tmp_127']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 16, op_role_var = [], scale = 9.999999747378752e-05, use_mkldnn = False)
    {Out=['tmp_129']} = scale(inputs={ScaleTensor=[], X=['tmp_128']}, bias = 0.0, bias_after_scale = True, op_device = , op_namescope = /, op_role = 16, op_role_var = [], scale = 1.0, use_mkldnn = False)
    {Out=['scheduled_learning_rate']} = assign(inputs={X=['tmp_129']}, op_device = , op_namescope = /, op_role = 16, op_role_var = [])
}
